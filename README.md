
# Edge-Cloud-AI: Inference Optimization Prototype

This repository contains my ongoing prototype work on optimizing inference performance for large-scale AI systems across edge and cloud computing environments. The goal is to reduce latency, memory consumption, and energy usage in real-time AI deployments, particularly in constrained or distributed settings.

The work explores:
- Model partitioning and adaptive deployment
- Temporal attention and long-context modeling
- Key-value (KV) cache compression strategies
- Benchmarking inference performance across edge-cloud settings

---
## üåê Broader Impact and Motivation

This project addresses key challenges in deploying large-scale AI systems in resource-constrained or latency-sensitive environments. The techniques explored‚Äîsuch as adaptive inference, efficient memory access, and edge-cloud coordination‚Äîaim to enhance accessibility, cost-efficiency, and energy sustainability in AI deployment.

Potential applications include:
- Public-sector services (e.g., education, healthcare, emergency response)
- Secure and low-latency systems in distributed settings
- Educational platforms where compute resources are limited

This is an early-stage prototype under active research and development.

---

## üì¢ Note

This is an active research prototype. Components are under development and subject to change.

---

## üìß Contact

Guiran l
gliu@sfsu.edu
