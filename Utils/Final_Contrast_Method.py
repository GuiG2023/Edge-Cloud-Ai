"""
é¢„æµ‹å¼ vs éªŒè¯å¼è·¯ç”±å¯¹æ¯”å®éªŒ
"""
import torch
import time
from typing import Dict, List, Tuple


class PredictiveRouter:
    """é¢„æµ‹å¼è·¯ç”±å™¨ - ä½ çš„æ–¹æ³•"""

    def __init__(self, slm_model, slm_tokenizer, attention_analyzer):
        self.slm = slm_model
        self.tokenizer = slm_tokenizer
        self.attention_analyzer = attention_analyzer

    def route_decision(self, task_text):
        """åœ¨ç”Ÿæˆå‰é¢„æµ‹æ˜¯å¦éœ€è¦è·¯ç”±"""
        start_time = time.time()

        # æå–æ³¨æ„åŠ›ç‰¹å¾
        features = self.attention_analyzer.extract_multi_dimensional_features(
            task_text, self.slm, self.tokenizer
        )

        # é¢„æµ‹å¤æ‚åº¦
        result = self.attention_analyzer.predict_complexity_enhanced(features)

        decision_time = time.time() - start_time

        return {
            'should_route': result['is_complex'],
            'complexity_score': result['complexity_score'],
            'decision_time': decision_time,
            'method': 'predictive'
        }


class VerificationRouter:
    """éªŒè¯å¼è·¯ç”±å™¨ - è®ºæ–‡æ–¹æ³•"""

    def __init__(self, slm_model, slm_tokenizer, llm_model, llm_tokenizer, prob_threshold=0.1):
        self.slm = slm_model
        self.slm_tokenizer = slm_tokenizer
        self.llm = llm_model
        self.llm_tokenizer = llm_tokenizer
        self.prob_threshold = prob_threshold

    def route_decision(self, task_text, num_preview_tokens=10):
        """ç”Ÿæˆéƒ¨åˆ†tokenåéªŒè¯æ˜¯å¦éœ€è¦è·¯ç”±"""
        start_time = time.time()

        # 1. SLMå…ˆç”Ÿæˆå‡ ä¸ªtoken
        inputs = self.slm_tokenizer(task_text, return_tensors="pt")

        with torch.no_grad():
            # ç”Ÿæˆé¢„è§ˆtoken
            preview_outputs = self.slm.generate(
                **inputs,
                max_new_tokens=num_preview_tokens,
                output_scores=True,
                return_dict_in_generate=True,
                do_sample=True,
                temperature=0.7
            )

        preview_tokens = preview_outputs.sequences[0][inputs['input_ids'].shape[1]:]

        # 2. LLMè¯„ä¼°è¿™äº›tokençš„æ¦‚ç‡
        full_text = self.slm_tokenizer.decode(preview_outputs.sequences[0])
        llm_inputs = self.llm_tokenizer(full_text, return_tensors="pt")

        with torch.no_grad():
            llm_outputs = self.llm(**llm_inputs)
            llm_probs = torch.softmax(llm_outputs.logits[0], dim=-1)

        # 3. æ£€æŸ¥æ˜¯å¦æœ‰ä½æ¦‚ç‡token
        should_route = False
        min_prob = float('inf')

        for i, token_id in enumerate(preview_tokens):
            if i < llm_probs.shape[0] - 1:  # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                token_prob = llm_probs[-(len(preview_tokens) - i), token_id].item()
                min_prob = min(min_prob, token_prob)

                if token_prob < self.prob_threshold:
                    should_route = True
                    break

        decision_time = time.time() - start_time

        return {
            'should_route': should_route,
            'min_token_prob': min_prob,
            'decision_time': decision_time,
            'method': 'verification',
            'preview_tokens': len(preview_tokens)
        }


class RouterComparison:
    """è·¯ç”±æ–¹æ³•å¯¹æ¯”å®éªŒ"""

    def __init__(self):
        # åŠ è½½æ¨¡å‹
        self.slm_name = "microsoft/DialoGPT-small"
        self.llm_name = "microsoft/DialoGPT-medium"

        # SLM
        self.slm_tokenizer = AutoTokenizer.from_pretrained(self.slm_name)
        self.slm_model = AutoModelForCausalLM.from_pretrained(self.slm_name, output_attentions=True)

        # LLM
        self.llm_tokenizer = AutoTokenizer.from_pretrained(self.llm_name)
        self.llm_model = AutoModelForCausalLM.from_pretrained(self.llm_name)

        # è®¾ç½®pad_token
        if self.slm_tokenizer.pad_token is None:
            self.slm_tokenizer.pad_token = self.slm_tokenizer.eos_token
        if self.llm_tokenizer.pad_token is None:
            self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token

        # æ³¨æ„åŠ›åˆ†æå™¨
        from your_previous_code import EnhancedAttentionAnalyzer  # éœ€è¦å¯¼å…¥ä½ ä¹‹å‰çš„ä»£ç 
        attention_analyzer = EnhancedAttentionAnalyzer()

        # åˆå§‹åŒ–è·¯ç”±å™¨
        self.predictive_router = PredictiveRouter(
            self.slm_model, self.slm_tokenizer, attention_analyzer
        )

        self.verification_router = VerificationRouter(
            self.slm_model, self.slm_tokenizer,
            self.llm_model, self.llm_tokenizer,
            prob_threshold=0.1
        )

    def generate_full_answer(self, model, tokenizer, question):
        """ç”Ÿæˆå®Œæ•´ç­”æ¡ˆ"""
        inputs = tokenizer(question, return_tensors="pt", truncation=True)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.pad_token_id
            )

        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = generated_text[len(question):].strip()
        return answer

    def extract_answer(self, text):
        """ä»æ–‡æœ¬ä¸­æå–æ•°å€¼ç­”æ¡ˆ"""
        import re
        numbers = re.findall(r'\b\d+\.?\d*\b', text)
        return numbers[-1] if numbers else None

    def evaluate_answer(self, predicted, ground_truth):
        """è¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§"""
        pred_num = self.extract_answer(str(predicted))
        truth_num = self.extract_answer(str(ground_truth))

        if pred_num is None or truth_num is None:
            return False

        try:
            return float(pred_num) == float(truth_num)
        except:
            return pred_num.strip().lower() == truth_num.strip().lower()

    def run_comparison_experiment(self, dataset):
        """è¿è¡Œå¯¹æ¯”å®éªŒ"""
        results = []

        print("ğŸ”¬ å¼€å§‹é¢„æµ‹å¼ vs éªŒè¯å¼è·¯ç”±å¯¹æ¯”å®éªŒ")
        print("=" * 60)

        for i, sample in enumerate(dataset):
            question = sample['question']
            ground_truth = sample['answer']

            print(f"\nğŸ“ æ ·æœ¬ {i + 1}: {question[:50]}...")

            # 1. é¢„æµ‹å¼è·¯ç”±å†³ç­–
            pred_decision = self.predictive_router.route_decision(question)

            # 2. éªŒè¯å¼è·¯ç”±å†³ç­–
            verif_decision = self.verification_router.route_decision(question)

            # 3. ç”Ÿæˆç­”æ¡ˆ
            slm_answer = self.generate_full_answer(self.slm_model, self.slm_tokenizer, question)
            llm_answer = self.generate_full_answer(self.llm_model, self.llm_tokenizer, question)

            # 4. æ ¹æ®è·¯ç”±å†³ç­–é€‰æ‹©æœ€ç»ˆç­”æ¡ˆ
            if pred_decision['should_route']:
                pred_final_answer = llm_answer
                pred_used_model = "LLM"
            else:
                pred_final_answer = slm_answer
                pred_used_model = "SLM"

            if verif_decision['should_route']:
                verif_final_answer = llm_answer
                verif_used_model = "LLM"
            else:
                verif_final_answer = slm_answer
                verif_used_model = "SLM"

            # 5. è¯„ä¼°å‡†ç¡®æ€§
            slm_correct = self.evaluate_answer(slm_answer, ground_truth)
            llm_correct = self.evaluate_answer(llm_answer, ground_truth)
            pred_correct = self.evaluate_answer(pred_final_answer, ground_truth)
            verif_correct = self.evaluate_answer(verif_final_answer, ground_truth)

            # 6. è®°å½•ç»“æœ
            result = {
                'question': question,
                'ground_truth': ground_truth,
                'slm_answer': slm_answer,
                'llm_answer': llm_answer,
                'slm_correct': slm_correct,
                'llm_correct': llm_correct,

                # é¢„æµ‹å¼ç»“æœ
                'pred_should_route': pred_decision['should_route'],
                'pred_complexity_score': pred_decision['complexity_score'],
                'pred_decision_time': pred_decision['decision_time'],
                'pred_final_answer': pred_final_answer,
                'pred_used_model': pred_used_model,
                'pred_correct': pred_correct,

                # éªŒè¯å¼ç»“æœ
                'verif_should_route': verif_decision['should_route'],
                'verif_min_prob': verif_decision['min_token_prob'],
                'verif_decision_time': verif_decision['decision_time'],
                'verif_final_answer': verif_final_answer,
                'verif_used_model': verif_used_model,
                'verif_correct': verif_correct,
            }

            results.append(result)

            # å®æ—¶æ˜¾ç¤ºç»“æœ
            print(f"   é¢„æµ‹å¼: {'âœ…' if pred_correct else 'âŒ'} | è·¯ç”±: {pred_decision['should_route']} | "
                  f"ç”¨æ—¶: {pred_decision['decision_time']:.3f}s | ä½¿ç”¨: {pred_used_model}")
            print(f"   éªŒè¯å¼: {'âœ…' if verif_correct else 'âŒ'} | è·¯ç”±: {verif_decision['should_route']} | "
                  f"ç”¨æ—¶: {verif_decision['decision_time']:.3f}s | ä½¿ç”¨: {verif_used_model}")

        return self.analyze_comparison_results(results)

    def analyze_comparison_results(self, results):
        """åˆ†æå¯¹æ¯”ç»“æœ"""
        total = len(results)

        # åŸºç¡€å‡†ç¡®ç‡
        slm_accuracy = sum(r['slm_correct'] for r in results) / total
        llm_accuracy = sum(r['llm_correct'] for r in results) / total
        pred_accuracy = sum(r['pred_correct'] for r in results) / total
        verif_accuracy = sum(r['verif_correct'] for r in results) / total

        # è·¯ç”±ç»Ÿè®¡
        pred_routing_rate = sum(r['pred_should_route'] for r in results) / total
        verif_routing_rate = sum(r['verif_should_route'] for r in results) / total

        # å†³ç­–æ—¶é—´
        pred_avg_time = sum(r['pred_decision_time'] for r in results) / total
        verif_avg_time = sum(r['verif_decision_time'] for r in results) / total

        # è·¯ç”±ä¸€è‡´æ€§
        routing_agreement = sum(
            r['pred_should_route'] == r['verif_should_route'] for r in results
        ) / total

        # æˆæœ¬è®¡ç®—
        pred_cost = self.calculate_routing_cost(results, 'pred')
        verif_cost = self.calculate_routing_cost(results, 'verif')
        slm_cost = total * 1.0
        llm_cost = total * 10.0

        # æ•ˆç‡åˆ†æ
        pred_efficiency = pred_accuracy / (pred_cost / slm_cost)
        verif_efficiency = verif_accuracy / (verif_cost / slm_cost)

        report = {
            'accuracy': {
                'slm_only': slm_accuracy,
                'llm_only': llm_accuracy,
                'predictive': pred_accuracy,
                'verification': verif_accuracy
            },
            'routing': {
                'predictive_rate': pred_routing_rate,
                'verification_rate': verif_routing_rate,
                'agreement': routing_agreement
            },
            'timing': {
                'predictive_avg': pred_avg_time,
                'verification_avg': verif_avg_time,
                'speedup_ratio': verif_avg_time / pred_avg_time
            },
            'cost': {
                'slm_cost': slm_cost,
                'llm_cost': llm_cost,
                'predictive_cost': pred_cost,
                'verification_cost': verif_cost
            },
            'efficiency': {
                'predictive': pred_efficiency,
                'verification': verif_efficiency
            },
            'detailed_results': results
        }

        self.print_comparison_report(report)
        return report

    def calculate_routing_cost(self, results, method_prefix):
        """è®¡ç®—è·¯ç”±æˆæœ¬"""
        slm_calls = 0
        llm_calls = 0

        for r in results:
            if method_prefix == 'pred':
                if r['pred_should_route']:
                    llm_calls += 1
                else:
                    slm_calls += 1
                # é¢„æµ‹å¼è¿˜éœ€è¦è®¡ç®—é¢„æµ‹æˆæœ¬
                slm_calls += 0.1  # é¢„æµ‹çš„é¢å¤–æˆæœ¬å¾ˆå°
            else:  # verification
                slm_calls += 1  # æ€»æ˜¯éœ€è¦SLMå…ˆç”Ÿæˆ
                if r['verif_should_route']:
                    llm_calls += 1
                llm_calls += 0.1  # LLMéªŒè¯çš„é¢å¤–æˆæœ¬

        return slm_calls * 1.0 + llm_calls * 10.0

    def print_comparison_report(self, report):
        """æ‰“å°å¯¹æ¯”æŠ¥å‘Š"""
        print("\n" + "=" * 70)
        print("ğŸ“Š é¢„æµ‹å¼ vs éªŒè¯å¼è·¯ç”±å¯¹æ¯”æŠ¥å‘Š")
        print("=" * 70)

        acc = report['accuracy']
        print(f"\nğŸ“ˆ å‡†ç¡®ç‡å¯¹æ¯”:")
        print(f"  SLM Only:      {acc['slm_only']:.3f} ({acc['slm_only'] * 100:.1f}%)")
        print(f"  LLM Only:      {acc['llm_only']:.3f} ({acc['llm_only'] * 100:.1f}%)")
        print(f"  é¢„æµ‹å¼è·¯ç”±:     {acc['predictive']:.3f} ({acc['predictive'] * 100:.1f}%)")
        print(f"  éªŒè¯å¼è·¯ç”±:     {acc['verification']:.3f} ({acc['verification'] * 100:.1f}%)")

        routing = report['routing']
        print(f"\nğŸ¯ è·¯ç”±è¡Œä¸ºå¯¹æ¯”:")
        print(f"  é¢„æµ‹å¼è·¯ç”±ç‡:   {routing['predictive_rate']:.3f} ({routing['predictive_rate'] * 100:.1f}%)")
        print(f"  éªŒè¯å¼è·¯ç”±ç‡:   {routing['verification_rate']:.3f} ({routing['verification_rate'] * 100:.1f}%)")
        print(f"  è·¯ç”±å†³ç­–ä¸€è‡´æ€§: {routing['agreement']:.3f} ({routing['agreement'] * 100:.1f}%)")

        timing = report['timing']
        print(f"\nâ±ï¸ æ•ˆç‡å¯¹æ¯”:")
        print(f"  é¢„æµ‹å¼å¹³å‡å†³ç­–æ—¶é—´: {timing['predictive_avg']:.3f}s")
        print(f"  éªŒè¯å¼å¹³å‡å†³ç­–æ—¶é—´: {timing['verification_avg']:.3f}s")
        print(f"  é¢„æµ‹å¼é€Ÿåº¦ä¼˜åŠ¿:     {timing['speedup_ratio']:.1f}x")

        cost = report['cost']
        print(f"\nğŸ’° æˆæœ¬å¯¹æ¯”:")
        print(f"  SLMæˆæœ¬:      {cost['slm_cost']:.1f}")
        print(f"  LLMæˆæœ¬:      {cost['llm_cost']:.1f}")
        print(f"  é¢„æµ‹å¼æˆæœ¬:    {cost['predictive_cost']:.1f}")
        print(f"  éªŒè¯å¼æˆæœ¬:    {cost['verification_cost']:.1f}")

        eff = report['efficiency']
        print(f"\nâš¡ æ•ˆç‡æŒ‡æ ‡ (å‡†ç¡®ç‡/ç›¸å¯¹æˆæœ¬):")
        print(f"  é¢„æµ‹å¼æ•ˆç‡:    {eff['predictive']:.3f}")
        print(f"  éªŒè¯å¼æ•ˆç‡:    {eff['verification']:.3f}")

        # ç»¼åˆè¯„ä»·
        print(f"\nğŸ† ç»¼åˆè¯„ä»·:")
        if acc['predictive'] >= acc['verification'] and timing['predictive_avg'] < timing['verification_avg']:
            print("âœ… é¢„æµ‹å¼æ–¹æ³•åœ¨å‡†ç¡®ç‡å’Œæ•ˆç‡ä¸Šéƒ½ä¼˜äºéªŒè¯å¼æ–¹æ³•")
        elif acc['predictive'] > acc['verification']:
            print("âœ… é¢„æµ‹å¼æ–¹æ³•å‡†ç¡®ç‡æ›´é«˜")
        elif timing['predictive_avg'] < timing['verification_avg']:
            print("âœ… é¢„æµ‹å¼æ–¹æ³•æ•ˆç‡æ›´é«˜")
        else:
            print("âš ï¸ ä¸¤ç§æ–¹æ³•å„æœ‰ä¼˜åŠ£ï¼Œéœ€è¦æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©")


def run_router_comparison():
    """è¿è¡Œè·¯ç”±å™¨å¯¹æ¯”å®éªŒ"""
    comparison = RouterComparison()

    # åŠ è½½æµ‹è¯•æ•°æ®é›†
    evaluator = RoutingEffectivenessEvaluator()
    dataset = evaluator.load_gsm8k_dataset(sample_size=20)  # å°æ ·æœ¬æµ‹è¯•

    # è¿è¡Œå¯¹æ¯”
    results = comparison.run_comparison_experiment(dataset)

    return results


# ä½ çš„å®æ–½æ­¥éª¤
def your_implementation_roadmap():
    """ä½ çš„é€æ­¥å®æ–½è·¯çº¿å›¾"""
    print("ğŸ—ºï¸ åŸºäºå½“å‰å®éªŒçš„å®æ–½è·¯çº¿å›¾")
    print("=" * 50)

    steps = [
        {
            "step": 1,
            "title": "å®Œå–„å½“å‰Baseline",
            "tasks": [
                "ä½¿ç”¨GSM8Kæ•°æ®é›†æ›¿æ¢å½“å‰çš„è®¤çŸ¥å¤æ‚åº¦ä»»åŠ¡",
                "å®ç°å‡†ç¡®ç‡è¯„ä¼°æœºåˆ¶",
                "å»ºç«‹æˆæœ¬è®¡ç®—æ¨¡å‹"
            ],
            "time": "3-5å¤©"
        },
        {
            "step": 2,
            "title": "å®ç°è·¯ç”±æ•ˆæœè¯„ä¼°",
            "tasks": [
                "æ„å»ºRoutingEffectivenessEvaluator",
                "æµ‹è¯•ä¸åŒé˜ˆå€¼ä¸‹çš„æ€§èƒ½",
                "ç”Ÿæˆæƒè¡¡åˆ†æå›¾è¡¨"
            ],
            "time": "2-3å¤©"
        },
        {
            "step": 3,
            "title": "å®ç°éªŒè¯å¼è·¯ç”±å¯¹æ¯”",
            "tasks": [
                "å®ç°VerificationRouter",
                "è¿›è¡Œé¢„æµ‹å¼vséªŒè¯å¼å¯¹æ¯”",
                "åˆ†æä¸¤ç§æ–¹æ³•çš„ä¼˜åŠ£"
            ],
            "time": "3-4å¤©"
        },
        {
            "step": 4,
            "title": "æ„å»ºAttention Predictor",
            "tasks": [
                "åŸºäºBaselineè®­ç»ƒä¸“é—¨çš„é¢„æµ‹å™¨",
                "å®ç°åœ¨çº¿å­¦ä¹ æœºåˆ¶",
                "ä¼˜åŒ–é¢„æµ‹ç²¾åº¦"
            ],
            "time": "1-2å‘¨"
        },
        {
            "step": 5,
            "title": "ç»¼åˆè¯„ä¼°å’Œè®ºæ–‡æ’°å†™",
            "tasks": [
                "å¤šæ•°æ®é›†éªŒè¯",
                "ä¸å…¶ä»–æ–¹æ³•å¯¹æ¯”",
                "æ’°å†™å®éªŒæŠ¥å‘Š"
            ],
            "time": "1-2å‘¨"
        }
    ]

    for step in steps:
        print(f"\nğŸ¯ æ­¥éª¤{step['step']}: {step['title']} ({step['time']})")
        for task in step['tasks']:
            print(f"   â€¢ {task}")

    print(f"\nâ° æ€»é¢„è®¡æ—¶é—´: 4-6å‘¨")
    print(f"ğŸ“ å®Œæˆåçš„æˆæœ: å®Œæ•´çš„SLM-LLMè·¯ç”±ç³»ç»Ÿ + å­¦æœ¯è®ºæ–‡")


if __name__ == "__main__":
    # æ˜¾ç¤ºå®æ–½è·¯çº¿å›¾
    your_implementation_roadmap()

    print("\n" + "=" * 50)
    print("ğŸš€ å¼€å§‹è¿è¡Œå¯¹æ¯”å®éªŒ...")

    # è¿è¡Œå®é™…å¯¹æ¯”å®éªŒ
    results = run_router_comparison()