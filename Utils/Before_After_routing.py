"""
è·¯ç”±å‰åå‡†ç¡®ç‡è¯„ä¼°æ¡†æ¶
"""
import torch
import json
from typing import Dict, List, Tuple
from transformers import AutoTokenizer, AutoModelForCausalLM


class RoutingEffectivenessEvaluator:
    def __init__(self, slm_name="microsoft/DialoGPT-small", llm_name="microsoft/DialoGPT-medium"):
        # æœ¬åœ°SLM
        self.slm_tokenizer = AutoTokenizer.from_pretrained(slm_name)
        self.slm_model = AutoModelForCausalLM.from_pretrained(slm_name, output_attentions=True)

        # äº‘ç«¯LLM (æ¨¡æ‹Ÿ)
        self.llm_tokenizer = AutoTokenizer.from_pretrained(llm_name)
        self.llm_model = AutoModelForCausalLM.from_pretrained(llm_name)

        # è®¾ç½®pad_token
        if self.slm_tokenizer.pad_token is None:
            self.slm_tokenizer.pad_token = self.slm_tokenizer.eos_token
        if self.llm_tokenizer.pad_token is None:
            self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token

        # æ³¨æ„åŠ›ç†µåˆ†æå™¨
        from your_previous_code import EnhancedAttentionAnalyzer
        self.attention_analyzer = EnhancedAttentionAnalyzer()

    def load_gsm8k_dataset(self, sample_size=100):
        """åŠ è½½GSM8Kæ•°å­¦æ¨ç†æ•°æ®é›†"""
        # è¿™é‡Œæ¨¡æ‹ŸGSM8Kæ•°æ®é›†ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦ä¸‹è½½çœŸå®æ•°æ®
        gsm8k_samples = [
            {
                "question": "James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many meters does he run a week?",
                "answer": "540",
                "solution_steps": "3 sprints * 3 times a week = 9 sprints per week. 9 sprints * 60 meters = 540 meters."
            },
            {
                "question": "Every day, Wendi feeds each of her chickens three cups of mixed chicken feed. If Wendi has 20 chickens, how many cups of feed does she need a day?",
                "answer": "60",
                "solution_steps": "20 chickens * 3 cups per chicken = 60 cups"
            },
            {
                "question": "Kylar went to the store to buy glasses for his new apartment. One glass costs $5. If Kylar bought 16 glasses, how much did he spend?",
                "answer": "80",
                "solution_steps": "16 glasses * $5 per glass = $80"
            }
            # å®é™…åº”ç”¨ä¸­éœ€è¦åŠ è½½å®Œæ•´çš„GSM8Kæ•°æ®é›†
        ]
        return gsm8k_samples[:sample_size]

    def extract_numerical_answer(self, text):
        """ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–æ•°å€¼ç­”æ¡ˆ"""
        import re
        # å¯»æ‰¾æ•°å­—æ¨¡å¼
        numbers = re.findall(r'\b\d+\.?\d*\b', text)
        if numbers:
            return numbers[-1]  # è¿”å›æœ€åä¸€ä¸ªæ•°å­—ä½œä¸ºç­”æ¡ˆ
        return None

    def evaluate_single_answer(self, predicted, ground_truth):
        """è¯„ä¼°å•ä¸ªç­”æ¡ˆçš„æ­£ç¡®æ€§"""
        pred_num = self.extract_numerical_answer(str(predicted))
        truth_num = self.extract_numerical_answer(str(ground_truth))

        if pred_num is None or truth_num is None:
            return False

        try:
            return float(pred_num) == float(truth_num)
        except:
            return pred_num.strip().lower() == truth_num.strip().lower()

    def generate_answer(self, model, tokenizer, question, max_length=100):
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”Ÿæˆç­”æ¡ˆ"""
        inputs = tokenizer(question, return_tensors="pt", truncation=True, max_length=512)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_length,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.pad_token_id
            )

        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer_part = generated_text[len(question):].strip()
        return answer_part

    def predict_routing_decision(self, question):
        """åŸºäºæ³¨æ„åŠ›ç†µé¢„æµ‹æ˜¯å¦éœ€è¦è·¯ç”±"""
        features = self.attention_analyzer.extract_multi_dimensional_features(
            question, self.slm_model, self.slm_tokenizer
        )
        result = self.attention_analyzer.predict_complexity_enhanced(features)
        return result['is_complex'], result['complexity_score']

    def run_routing_evaluation(self, dataset):
        """è¿è¡Œå®Œæ•´çš„è·¯ç”±è¯„ä¼°å®éªŒ"""
        results = []

        print(f"ğŸ§ª å¼€å§‹è·¯ç”±æ•ˆæœè¯„ä¼°ï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬...")

        for i, sample in enumerate(dataset):
            question = sample['question']
            ground_truth = sample['answer']

            print(f"\nğŸ“ æ ·æœ¬ {i + 1}: {question[:50]}...")

            # 1. SLMå•ç‹¬å¤„ç†
            slm_answer = self.generate_answer(self.slm_model, self.slm_tokenizer, question)
            slm_correct = self.evaluate_single_answer(slm_answer, ground_truth)

            # 2. LLMå•ç‹¬å¤„ç†
            llm_answer = self.generate_answer(self.llm_model, self.llm_tokenizer, question)
            llm_correct = self.evaluate_single_answer(llm_answer, ground_truth)

            # 3. é¢„æµ‹è·¯ç”±å†³ç­–
            should_route, complexity_score = self.predict_routing_decision(question)

            # 4. æ··åˆç­–ç•¥ç»“æœ
            if should_route:
                hybrid_answer = llm_answer  # è·¯ç”±åˆ°äº‘ç«¯
                hybrid_correct = llm_correct
                used_model = "LLM"
            else:
                hybrid_answer = slm_answer  # æœ¬åœ°å¤„ç†
                hybrid_correct = slm_correct
                used_model = "SLM"

            # 5. è®°å½•ç»“æœ
            result = {
                'question': question,
                'ground_truth': ground_truth,
                'slm_answer': slm_answer,
                'llm_answer': llm_answer,
                'hybrid_answer': hybrid_answer,
                'slm_correct': slm_correct,
                'llm_correct': llm_correct,
                'hybrid_correct': hybrid_correct,
                'should_route': should_route,
                'complexity_score': complexity_score,
                'used_model': used_model
            }

            results.append(result)

            print(f"   SLM: {'âœ…' if slm_correct else 'âŒ'} | LLM: {'âœ…' if llm_correct else 'âŒ'} | "
                  f"Hybrid: {'âœ…' if hybrid_correct else 'âŒ'} | Route: {should_route} | Used: {used_model}")

        return self.analyze_routing_results(results)

    def analyze_routing_results(self, results):
        """åˆ†æè·¯ç”±ç»“æœ"""
        total = len(results)

        # åŸºç¡€å‡†ç¡®ç‡
        slm_accuracy = sum(r['slm_correct'] for r in results) / total
        llm_accuracy = sum(r['llm_correct'] for r in results) / total
        hybrid_accuracy = sum(r['hybrid_correct'] for r in results) / total

        # è·¯ç”±ç»Ÿè®¡
        routed_count = sum(r['should_route'] for r in results)
        routing_rate = routed_count / total

        # è·¯ç”±å†³ç­–è´¨é‡åˆ†æ
        correct_routing_decisions = 0
        for r in results:
            if r['should_route'] and r['llm_correct'] and not r['slm_correct']:
                correct_routing_decisions += 1  # æ­£ç¡®è·¯ç”±åˆ°LLM
            elif not r['should_route'] and (r['slm_correct'] or not r['llm_correct']):
                correct_routing_decisions += 1  # æ­£ç¡®ä¿ç•™åœ¨SLM

        routing_decision_accuracy = correct_routing_decisions / total

        # æˆæœ¬åˆ†æï¼ˆæ¨¡æ‹Ÿï¼‰
        slm_cost = total * 1.0  # SLMå•ä½æˆæœ¬
        llm_cost = total * 10.0  # LLMå•ä½æˆæœ¬
        hybrid_cost = (total - routed_count) * 1.0 + routed_count * 10.0

        cost_savings = (llm_cost - hybrid_cost) / llm_cost

        report = {
            'accuracy': {
                'slm_only': slm_accuracy,
                'llm_only': llm_accuracy,
                'hybrid': hybrid_accuracy
            },
            'routing': {
                'routing_rate': routing_rate,
                'routing_decision_accuracy': routing_decision_accuracy,
                'routed_samples': routed_count,
                'total_samples': total
            },
            'cost': {
                'slm_cost': slm_cost,
                'llm_cost': llm_cost,
                'hybrid_cost': hybrid_cost,
                'cost_savings': cost_savings
            },
            'detailed_results': results
        }

        self.print_evaluation_report(report)
        return report

    def print_evaluation_report(self, report):
        """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š è·¯ç”±æ•ˆæœè¯„ä¼°æŠ¥å‘Š")
        print("=" * 60)

        acc = report['accuracy']
        print(f"\nğŸ“ˆ å‡†ç¡®ç‡å¯¹æ¯”:")
        print(f"  SLM Only:  {acc['slm_only']:.3f} ({acc['slm_only'] * 100:.1f}%)")
        print(f"  LLM Only:  {acc['llm_only']:.3f} ({acc['llm_only'] * 100:.1f}%)")
        print(f"  Hybrid:    {acc['hybrid']:.3f} ({acc['hybrid'] * 100:.1f}%)")

        routing = report['routing']
        print(f"\nğŸ¯ è·¯ç”±å†³ç­–åˆ†æ:")
        print(f"  è·¯ç”±ç‡: {routing['routing_rate']:.3f} ({routing['routing_rate'] * 100:.1f}%)")
        print(f"  è·¯ç”±å†³ç­–å‡†ç¡®æ€§: {routing['routing_decision_accuracy']:.3f}")
        print(f"  è·¯ç”±æ ·æœ¬æ•°: {routing['routed_samples']}/{routing['total_samples']}")

        cost = report['cost']
        print(f"\nğŸ’° æˆæœ¬æ•ˆç›Šåˆ†æ:")
        print(f"  SLMæ€»æˆæœ¬: {cost['slm_cost']:.1f}")
        print(f"  LLMæ€»æˆæœ¬: {cost['llm_cost']:.1f}")
        print(f"  æ··åˆæˆæœ¬: {cost['hybrid_cost']:.1f}")
        print(f"  æˆæœ¬èŠ‚çœ: {cost['cost_savings']:.3f} ({cost['cost_savings'] * 100:.1f}%)")

        # ç»¼åˆè¯„ä»·
        if acc['hybrid'] >= acc['slm_only'] and cost['cost_savings'] > 0:
            print(
                f"\nâœ… è·¯ç”±ç­–ç•¥æœ‰æ•ˆ: å‡†ç¡®ç‡æå‡ {(acc['hybrid'] - acc['slm_only']) * 100:.1f}%, æˆæœ¬èŠ‚çœ {cost['cost_savings'] * 100:.1f}%")
        else:
            print(f"\nâš ï¸ è·¯ç”±ç­–ç•¥éœ€è¦ä¼˜åŒ–")


# ä½¿ç”¨ç¤ºä¾‹
def run_routing_effectiveness_experiment():
    """è¿è¡Œè·¯ç”±æ•ˆæœå®éªŒ"""
    evaluator = RoutingEffectivenessEvaluator()

    # åŠ è½½æ•°æ®é›†
    dataset = evaluator.load_gsm8k_dataset(sample_size=50)

    # è¿è¡Œè¯„ä¼°
    results = evaluator.run_routing_evaluation(dataset)

    return results


if __name__ == "__main__":
    results = run_routing_effectiveness_experiment()