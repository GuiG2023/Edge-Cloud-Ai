# coding=utf-8
"""
å®Œæ•´çš„GSM8Kæ™ºèƒ½è·¯ç”±å‡†ç¡®ç‡è¯„ä¼°ç³»ç»Ÿ
æ•´åˆäº†çœŸå®GSM8Kæ•°æ®é›†ã€ä¸€ä¸ªå¯å­¦ä¹ çš„å¤æ‚åº¦åˆ†æå™¨å’Œæ¨¡å‹æ¥å£ã€‚
æœ¬æ–‡ä»¶åŒ…å«ä¸¤ç§æ¨¡å¼:
1. 'train' æ¨¡å¼: ç”Ÿæˆè®­ç»ƒæ•°æ®å¹¶è®­ç»ƒè·¯ç”±å™¨æ¨¡å‹ã€‚
2. 'eval' æ¨¡å¼: åŠ è½½è®­ç»ƒå¥½çš„è·¯ç”±å™¨æ¨¡å‹ï¼Œå¹¶è¿›è¡Œç«¯åˆ°ç«¯çš„æ€§èƒ½è¯„ä¼°ã€‚
"""

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from torch import nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login
import re
import json
import os
import warnings
import getpass
from typing import Dict, List, Tuple, Optional
import random
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim

warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# GPUè®¾ç½®
if torch.cuda.is_available():
    device = torch.device('cuda')
    print(f"ğŸš€ Using GPU: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device('cpu')
    print("ğŸ’» Using CPU")


# ========================= åŸºç¡€é…ç½®ç±» =========================
class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»"""

    def __init__(self, name: str, model_path: str, cost_per_token: float, avg_latency_ms: int):
        self.name = name
        self.model_path = model_path
        self.cost_per_token = cost_per_token
        self.avg_latency_ms = avg_latency_ms


class ModelInterface:
    """æ¨¡å‹æ¥å£åŸºç±»"""

    def __init__(self, config: ModelConfig):
        self.config = config
        self.model = None
        self.tokenizer = None

    def predict(self, question: str) -> str:
        raise NotImplementedError


# ========================= GSM8Kæ•°æ®å¤„ç†å™¨ =========================
class FixedGSM8KProcessor:
    """ä¿®å¤ç‰ˆGSM8Kæ•°æ®å¤„ç†å™¨"""

    def __init__(self, data_path="gsm8k_data/train.jsonl", max_samples=1000):
        print(f"ğŸ“š Loading GSM8K dataset...")
        self.data_path = data_path
        self.max_samples = max_samples
        self.samples = []

        # å°è¯•å¤šç§åŠ è½½æ–¹å¼
        if self._load_from_datasets():
            print(f"âœ… Loaded {len(self.samples)} samples from datasets library")
        elif self._load_from_local():
            print(f"âœ… Loaded {len(self.samples)} samples from local file")
        else:
            print("ğŸ”„ Using enhanced fallback data...")
            self.samples = self._create_balanced_fallback()

    def _load_from_datasets(self):
        """ä»datasetsåº“åŠ è½½GSM8K"""
        try:
            from datasets import load_dataset
            print("ğŸ”„ Loading from HuggingFace datasets...")

            # ä½¿ç”¨GSM8Kçš„testé›†ï¼Œå› ä¸ºå®ƒæœ‰æ ‡å‡†ç­”æ¡ˆ
            dataset = load_dataset("gsm8k", "main")
            test_data = dataset['test']

            for i in range(min(self.max_samples, len(test_data))):
                self.samples.append({
                    'question': test_data[i]['question'],
                    'answer': test_data[i]['answer']
                })
            return len(self.samples) > 0
        except Exception as e:
            print(f"âš ï¸ Failed to load from datasets: {e}")
            return False

    def _load_from_local(self):
        """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½"""
        try:
            if not os.path.exists(self.data_path):
                return False

            with open(self.data_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if line.strip():
                        try:
                            data = json.loads(line)
                            self.samples.append({
                                'question': data['question'],
                                'answer': data['answer']
                            })
                            if len(self.samples) >= self.max_samples:
                                break
                        except:
                            continue
            return len(self.samples) > 0
        except:
            return False

    # def _create_balanced_fallback(self):
    #     """åˆ›å»ºå¹³è¡¡çš„åå¤‡æ•°æ®é›†"""
    #     fallback_data = [
    #         {'question': "Janet has 3 apples. She eats 1 apple. How many apples does she have left?",
    #          'answer': "Janet starts with 3 apples.\nShe eats 1 apple.\n3 - 1 = 2\n#### 2"},
    #         {'question': "Tom bought 5 books for $2 each. How much did he spend in total?",
    #          'answer': "Tom bought 5 books.\nEach book costs $2.\n5 Ã— 2 = 10\n#### 10"},
    #         {
    #             'question': "A store sells apples for $3 per kg. If John buys 2.5 kg and pays with a $10 bill, how much change does he get?",
    #             'answer': "Cost per kg: $3\nAmount bought: 2.5 kg\nTotal cost: 3 Ã— 2.5 = $7.50\nPaid: $10\nChange: 10 - 7.50 = $2.50\n#### 2.5"},
    #         {
    #             'question': "A company has 120 employees. 25% work in sales, 30% in engineering, and the rest in administration. If the sales team gets a 15% increase and engineering gets a 10% increase, how many total employees will there be after the increases?",
    #             'answer': "Initial employees: 120\nSales: 25% of 120 = 0.25 Ã— 120 = 30 employees\nEngineering: 30% of 120 = 0.30 Ã— 120 = 36 employees\nAdministration: 120 - 30 - 36 = 54 employees\n\nAfter increases:\nSales increase: 30 Ã— 0.15 = 4.5 â‰ˆ 5 new employees\nEngineering increase: 36 Ã— 0.10 = 3.6 â‰ˆ 4 new employees\n\nTotal after increases: 120 + 5 + 4 = 129 employees\n#### 129"}
    #     ]
    #     print(f"âœ… Created {len(fallback_data)} fallback samples")
    #     return fallback_data

    def extract_answer(self, answer_text: str) -> str:
        """ä»GSM8Kçš„ç­”æ¡ˆæ–‡æœ¬ä¸­æå–æ•°å€¼"""
        match = re.search(r'####\s*([+-]?\d+(?:\.\d+)?)', answer_text)
        if match: return match.group(1)
        numbers = re.findall(r'([+-]?\d+(?:\.\d+)?)', answer_text)
        return numbers[-1] if numbers else "No answer found"

    def count_solution_steps(self, answer: str) -> int:
        """å¤šç»´åº¦æ­¥éª¤è¯†åˆ«ç»¼åˆåˆ¤æ–­æ¨ç†å¤æ‚åº¦"""
        lines = [line.strip() for line in answer.split('\n') if line.strip()]
        meaningful_lines = [line for line in lines if not line.startswith('####')]
        math_operations = len(re.findall(r'\d+\s*[+\-Ã—Ã·*/]\s*\d+', answer))
        equals_count = answer.count('=')
        step_words = len(re.findall(r'\b(then|next|so|therefore|thus|after|now|finally)\b', answer.lower()))
        steps = max(len(meaningful_lines) - 1, math_operations, equals_count, step_words, 1)
        return min(steps, 12)

    def classify_difficulty(self, steps: int) -> str:
        """ä¿®å¤çš„éš¾åº¦åˆ†çº§ - é€‚åº”çœŸå®GSM8Kåˆ†å¸ƒ"""
        if steps <= 4:
            return "simple"
        elif steps <= 8:
            return "medium"
        else:
            return "complex"

    def get_balanced_sample(self, n_total: int = 200, simple_ratio: float = 0.5) -> Tuple[List, List]:
        """è·å–å¹³è¡¡çš„æ ·æœ¬æ•°æ®ï¼ŒæŒ‰æ­¥éª¤æ•°åˆ†ç±»"""
        print(f"ğŸ¯ å‡†å¤‡é‡‡æ · {n_total} é“é¢˜ç›® (ç®€å•é¢˜æ¯”ä¾‹: {simple_ratio:.1%})")
        simple_problems, complex_problems = [], []
        print("ğŸ“‹ æ­£åœ¨åˆ†æé—®é¢˜å¤æ‚åº¦...")
        for i, item in enumerate(self.samples):
            if i % 100 == 0 and i > 0: print(f"   å¤„ç†è¿›åº¦: {i}/{len(self.samples)}")
            problem_data = {'question': item['question'], 'answer': self.extract_answer(item['answer']),
                            'original_answer': item['answer'], 'steps': self.count_solution_steps(item['answer']),
                            'difficulty': self.classify_difficulty(self.count_solution_steps(item['answer']))}
            if problem_data['difficulty'] == "simple":
                simple_problems.append(problem_data)
            else:
                complex_problems.append(problem_data)
        print(f"âœ… åˆ†ç±»å®Œæˆ: {len(simple_problems)} ç®€å•é¢˜, {len(complex_problems)} å¤æ‚é¢˜")
        n_simple, n_complex = int(n_total * simple_ratio), n_total - int(n_total * simple_ratio)
        if len(simple_problems) < n_simple: n_simple = len(simple_problems)
        if len(complex_problems) < n_complex: n_complex = len(complex_problems)
        sampled_simple = random.sample(simple_problems, n_simple) if n_simple > 0 else []
        sampled_complex = random.sample(complex_problems, n_complex) if n_complex > 0 else []
        print(f"ğŸ² æœ€ç»ˆé‡‡æ ·: {len(sampled_simple)} ç®€å•é¢˜, {len(sampled_complex)} å¤æ‚é¢˜")
        return sampled_simple, sampled_complex


# ========================= æ–°å¢: å¯è®­ç»ƒçš„å¤æ‚åº¦é¢„æµ‹ç½‘ç»œ =========================
class ComplexityPredictorNet(nn.Module):
    """
    ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œï¼Œç”¨äºä»æ³¨æ„åŠ›ç‰¹å¾ä¸­å­¦ä¹ å¹¶é¢„æµ‹ä»»åŠ¡å¤æ‚åº¦ã€‚
    å–ä»£äº†åŸæ¥ç¡¬ç¼–ç çš„è§„åˆ™å’Œæƒé‡ã€‚
    """

    def __init__(self, input_features: int = 8):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_features, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)  # è¾“å‡ºä¸€ä¸ªåŸå§‹çš„logitå€¼ï¼Œç”¨äºäºŒåˆ†ç±»
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        è¾“å…¥: ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ³¨æ„åŠ›ç‰¹å¾çš„å‘é‡
        è¾“å‡º: ä¸€ä¸ªlogitå€¼ï¼Œ>0å€¾å‘äºè®¤ä¸ºæ˜¯å¤æ‚é—®é¢˜
        """
        return self.network(x)


# ========================= å‡çº§: å¯å­¦ä¹ çš„æ™ºèƒ½è·¯ç”±å™¨ =========================
class LearnedAttentionRouter:
    """
    ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œæ¥ä»£æ›¿å›ºå®šè§„åˆ™ï¼Œè¿›è¡Œè·¯ç”±å†³ç­–ã€‚
    """

    def __init__(self, model_path: str, device, threshold=0.5):
        self.device = device
        self.threshold = threshold  # è¿™é‡Œçš„é˜ˆå€¼ä½œç”¨äºæ¨¡å‹çš„sigmoidè¾“å‡ºæ¦‚ç‡

        # åŠ è½½æˆ‘ä»¬è®­ç»ƒå¥½çš„å¤æ‚åº¦é¢„æµ‹ç½‘ç»œ
        print(f"ğŸ§  Loading learned complexity predictor from: {model_path}")
        self.predictor_net = ComplexityPredictorNet().to(self.device)
        try:
            self.predictor_net.load_state_dict(torch.load(model_path, map_location=self.device))
            self.predictor_net.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
            print("âœ… Learned predictor loaded successfully.")
        except FileNotFoundError:
            print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ {model_path} æœªæ‰¾åˆ°! è·¯ç”±å™¨å°†ä½¿ç”¨æœªè®­ç»ƒçš„ç½‘ç»œï¼Œç»“æœå°†æ˜¯éšæœºçš„ã€‚")
            print("ğŸ’¡ è¯·å…ˆè¿è¡Œ 'train' æ¨¡å¼æ¥ç”Ÿæˆæ¨¡å‹æ–‡ä»¶ã€‚")
        except Exception as e:
            print(f"âŒ åŠ è½½é¢„æµ‹ç½‘ç»œå¤±è´¥: {e}")
            raise

    def route(self, question: str, slm_model, slm_tokenizer) -> Tuple[str, float]:
        """è·¯ç”±å†³ç­–: è¿”å› ('SLM'/'LLM', å¤æ‚åº¦åˆ†æ•°)"""
        try:
            features = self.extract_core_features(question, slm_model, slm_tokenizer)
            prediction = self.predict_complexity(features)
            route_decision = "LLM" if prediction['is_complex'] else "SLM"
            return route_decision, prediction['complexity_score']
        except Exception as e:
            print(f"âš ï¸ Route decision failed: {e}, defaulting to LLM")
            return "LLM", 1.0

    def predict_complexity(self, features: dict) -> dict:
        """ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œé¢„æµ‹"""
        feature_vector = torch.tensor([
            features['avg_entropy'], features['entropy_std'], features['max_entropy'],
            features['avg_variance'], features['variance_std'], features['max_variance'],
            features['avg_max_attention'], features['concentration_std']
        ], dtype=torch.float32).to(self.device)

        with torch.no_grad():
            logit = self.predictor_net(feature_vector.unsqueeze(0))
            probability = torch.sigmoid(logit).item()

        return {'complexity_score': probability, 'is_complex': probability > self.threshold}

    def extract_core_features(self, text: str, model, tokenizer) -> dict:
        """ç¨³å¥çš„ç‰¹å¾æå–"""
        model_device = next(model.parameters()).device
        inputs = tokenizer(text, return_tensors="pt", max_length=200, truncation=True, padding=True)
        inputs = {k: v.to(model_device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs, output_attentions=True)
        attentions = outputs.attentions[-1][0]
        seq_len = inputs['attention_mask'].sum().item()
        attentions = attentions.cpu()
        entropy_features = self._compute_entropy(attentions, seq_len)
        variance_features = self._compute_variance(attentions, seq_len)
        concentration_features = self._compute_concentration(attentions, seq_len)
        return {**entropy_features, **variance_features, **concentration_features}

    def _compute_entropy(self, attentions, seq_len):
        all_entropies = [-torch.sum(
            (attentions[head, pos, :seq_len] + 1e-9) * torch.log(attentions[head, pos, :seq_len] + 1e-9)).item() for
                         head in range(attentions.shape[0]) for pos in range(seq_len)]
        return {'avg_entropy': np.mean(all_entropies), 'entropy_std': np.std(all_entropies),
                'max_entropy': np.max(all_entropies)}

    def _compute_variance(self, attentions, seq_len):
        variances = [torch.var(attentions[head, pos, :seq_len]).item() for head in range(attentions.shape[0]) for pos in
                     range(seq_len)]
        return {'avg_variance': np.mean(variances), 'variance_std': np.std(variances),
                'max_variance': np.max(variances)}

    def _compute_concentration(self, attentions, seq_len):
        max_attentions = [torch.max(attentions[head, pos, :seq_len]).item() for head in range(attentions.shape[0]) for
                          pos in range(seq_len)]
        return {'avg_max_attention': np.mean(max_attentions), 'concentration_std': np.std(max_attentions)}


# ========================= SLM/LLMæ¥å£ å’Œ å‡†ç¡®ç‡éªŒè¯å™¨ (ä¿æŒä¸å˜) =========================
class SLMInterface(ModelInterface):
    """å°æ¨¡å‹æ¥å£"""

    def load_model(self):
        print(f"ğŸ”„ Loading SLM: {self.config.name}")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(self.config.model_path, torch_dtype=torch.float16,
                                                              device_map="auto" if torch.cuda.is_available() else None,
                                                              trust_remote_code=True, output_attentions=True)
            if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token
            print(f"âœ… SLM loaded successfully")
        except Exception as e:
            print(f"âŒ Failed to load SLM: {e}");
            raise

    def predict(self, question: str, num_tokens_to_generate=0) -> tuple:
        """
        ã€æœ€ç»ˆå‡çº§ç‰ˆ predict æ–¹æ³•ã€‘
        - ä½¿ç”¨æ€ç»´é“¾æç¤ºï¼Œå¹¶è§„å®šäº†è¾“å‡ºæ ¼å¼ã€‚
        - ä½¿ç”¨è´ªå©ªæœç´¢ (do_sample=False) ä¿è¯ç»“æœå¯å¤ç°ã€‚
        - åœ¨éœ€è¦æ—¶ï¼Œå¯ä»¥åªç”Ÿæˆå°‘é‡tokenå¹¶è¿”å›æ³¨æ„åŠ›åºåˆ—ã€‚
        """
        if self.model is None: self.load_model()

        # --- 1. ä½¿ç”¨æ–°çš„ã€æ›´ä¼˜åŒ–çš„â€œæ€ç»´é“¾â€æç¤º ---
        prompt = f"""Solve the following math problem. Think step by step and then write the final answer in the format #### <answer>.

        Question: {question}
        Answer: Let's think step by step.
        """

        inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.model.device)

        max_len = num_tokens_to_generate if num_tokens_to_generate > 0 else 200
        should_return_attentions = num_tokens_to_generate > 0

        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_new_tokens=max_len,
                do_sample=False,  # <--- 2. æ”¹ä¸ºè´ªå©ªæœç´¢ï¼Œä¿è¯ç»“æœç¡®å®šæ€§
                pad_token_id=self.tokenizer.eos_token_id,
                output_attentions=should_return_attentions,
                return_dict_in_generate=should_return_attentions
            )

        sequence = outputs.sequences[0] if hasattr(outputs, 'sequences') else outputs[0]
        full_response = self.tokenizer.decode(sequence, skip_special_tokens=True)

        # 3. è°ƒæ•´ç­”æ¡ˆåˆ‡åˆ†é€»è¾‘ä»¥åŒ¹é…æ–°æç¤º
        assistant_response_start = "Answer: Let's think step by step."
        start_index = full_response.rfind(assistant_response_start)
        answer_text = full_response[
                      start_index + len(assistant_response_start):].strip() if start_index != -1 else full_response

        attentions = outputs.attentions if should_return_attentions and hasattr(outputs, 'attentsions') else None

        return answer_text, attentions


class EnhancedLLMInterface(ModelInterface):
    """å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥å£"""

    def __init__(self, config: ModelConfig, hf_token: str = None):
        super().__init__(config)
        self.hf_token = hf_token
        self.max_memory_per_gpu = "35GB"

    def setup_authentication(self):
        if self.hf_token:
            login(token=self.hf_token)
        elif os.getenv('HUGGINGFACE_TOKEN'):
            login(token=os.getenv('HUGGINGFACE_TOKEN'))
        else:
            print("âš ï¸ No HuggingFace token provided. Trying without authentication...")

    def load_model(self):
        print(f"ğŸ”„ Loading LLM: {self.config.name}");
        self.setup_authentication()
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path, trust_remote_code=True)
            model_kwargs = {"torch_dtype": torch.float16, "trust_remote_code": True, "low_cpu_mem_usage": True}
            if "70B" in self.config.model_path:
                model_kwargs.update(
                    {"device_map": "auto", "max_memory": {0: self.max_memory_per_gpu}, "load_in_8bit": True})
            elif "34B" in self.config.model_path or "30B" in self.config.model_path:
                model_kwargs.update({"device_map": "auto", "max_memory": {0: self.max_memory_per_gpu}})
            else:
                model_kwargs.update({"device_map": "auto" if torch.cuda.is_available() else None})
            self.model = AutoModelForCausalLM.from_pretrained(self.config.model_path, **model_kwargs)
            if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token
            print(f"âœ… LLM loaded successfully on {next(self.model.parameters()).device}")
        except Exception as e:
            print(f"âŒ Failed to load LLM: {e}"); raise

    def predict(self, question: str) -> str:
        if self.model is None: self.load_model()
        prompt = self._build_math_prompt(question)
        inputs = self.tokenizer.encode(prompt, return_tensors="pt", max_length=2048, truncation=True)
        inputs = inputs.to(next(self.model.parameters()).device)
        with torch.no_grad():
            outputs = self.model.generate(inputs, max_new_tokens=300, do_sample=True, temperature=0.7, top_p=0.9,
                                          pad_token_id=self.tokenizer.eos_token_id, repetition_penalty=1.1)
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return full_response[len(prompt):].strip()

    def _build_math_prompt(self, question: str) -> str:
        return f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful math tutor. Solve the following math problem step by step and provide the final numerical answer at the end.\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{question}\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nI'll solve this step by step.\n\n"""


class AccuracyValidator:
    """å‡†ç¡®ç‡éªŒè¯å™¨"""

    @staticmethod
    def extract_final_answer(response: str) -> str:
        patterns = [r'[Tt]he answer is\s*([+-]?\d+(?:\.\d+)?)', r'[Ff]inal answer:\s*([+-]?\d+(?:\.\d+)?)',
                    r'[Aa]nswer:\s*([+-]?\d+(?:\.\d+)?)', r'=\s*([+-]?\d+(?:\.\d+)?)\s*$',
                    r'####\s*([+-]?\d+(?:\.\d+)?)', r'([+-]?\d+(?:\.\d+)?)\s*$']
        for pattern in patterns:
            match = re.search(pattern, response)
            if match: return match.group(1).strip()
        numbers = re.findall(r'([+-]?\d+(?:\.\d+)?)', response)
        return numbers[-1] if numbers else "No answer found"

    @staticmethod
    def is_correct(predicted: str, ground_truth: str, tolerance: float = 0.01) -> bool:
        try:
            pred_num, true_num = float(predicted), float(ground_truth)
            if abs(pred_num - true_num) <= tolerance: return True
            if true_num != 0: return abs(pred_num - true_num) / abs(true_num) <= 0.01
            return False
        except (ValueError, TypeError):
            return str(predicted).strip().lower() == str(ground_truth).strip().lower()


# ========================= ä¸»è¯„ä¼°å™¨ (ä¿®æ”¹å) =========================
class GSM8KAccuracyEvaluator:
    """åŸºäºGSM8Kçš„å‡†ç¡®ç‡è¯„ä¼°å™¨"""

    def __init__(self, hf_token=None, max_samples=1000):
        self.hf_token = hf_token
        self.validator = AccuracyValidator()
        self.data_processor = FixedGSM8KProcessor(max_samples=max_samples)
        self.slm_config = ModelConfig(name="Llama-3.2-3B", model_path="meta-llama/Llama-3.2-3B", cost_per_token=0.001,
                                      avg_latency_ms=100)
        self.llm_config = ModelConfig(name="Llama-3.1-8B-Instruct", model_path="meta-llama/Llama-3.1-8B-Instruct",
                                      cost_per_token=0.003, avg_latency_ms=800)
        self.slm = SLMInterface(self.slm_config)
        self.llm = EnhancedLLMInterface(self.llm_config, hf_token)
        self.router = None  # ç­‰å¾…SLMåŠ è½½ååˆå§‹åŒ–
        print(
            f"ğŸ¯ SLM: {self.slm_config.name}\nğŸ¯ LLM: {self.llm_config.name}\nğŸ“Š æˆæœ¬æ¯”ä¾‹: 1:{self.llm_config.cost_per_token / self.slm_config.cost_per_token:.1f}")

    def _ensure_slm_loaded(self):
        """ç¡®ä¿SLMå·²åŠ è½½ï¼ˆè·¯ç”±å™¨éœ€è¦ç”¨åˆ°ï¼‰"""
        if self.slm.model is None: self.slm.load_model()
        if self.router is None:
            # <--- ä¿®æ”¹: ä½¿ç”¨æ–°çš„LearnedAttentionRouter
            self.router = LearnedAttentionRouter(model_path="router_model.pth", device=device, threshold=0.5)
            print("âœ… å¯å­¦ä¹ çš„æ³¨æ„åŠ›è·¯ç”±å™¨å·²åˆå§‹åŒ–")

    def evaluate_model_on_problems(self, model_interface, problems: List[Dict], model_name: str,
                                   max_problems: Optional[int] = None) -> Dict:
        """åœ¨æŒ‡å®šé—®é¢˜é›†åˆä¸Šè¯„ä¼°æ¨¡å‹"""
        print(f"\nğŸ” è¯„ä¼° {model_name}...")
        if max_problems and len(problems) > max_problems:
            problems = random.sample(problems, max_problems)
            print(f"   éšæœºé€‰æ‹© {max_problems} é“é¢˜è¿›è¡Œæµ‹è¯•")
        correct_count, total_count, detailed_results, error_cases = 0, len(problems), [], []
        for i, problem in enumerate(problems):
            question, ground_truth = problem['question'], problem['answer']
            print(f"   å¤„ç†é—®é¢˜ {i + 1}/{total_count}...", end="\r")
            try:
                response = model_interface.predict(question)
                predicted_answer = self.validator.extract_final_answer(response)
                is_correct = self.validator.is_correct(predicted_answer, ground_truth)
                if is_correct:
                    correct_count += 1
                else:
                    error_cases.append(
                        {'question': question[:100], 'ground_truth': ground_truth, 'predicted': predicted_answer})
                detailed_results.append({'question': question, 'ground_truth': ground_truth, 'model_response': response,
                                         'extracted_answer': predicted_answer, 'is_correct': is_correct})
            except Exception as e:
                print(f"   âš ï¸ å¤„ç†é”™è¯¯: {e}")
                detailed_results.append(
                    {'question': question, 'ground_truth': ground_truth, 'model_response': f"ERROR: {e}",
                     'extracted_answer': "ERROR", 'is_correct': False})
        print()  # æ¢è¡Œ
        accuracy = correct_count / total_count if total_count > 0 else 0
        result = {'model_name': model_name, 'accuracy': accuracy, 'correct_count': correct_count,
                  'total_count': total_count, 'error_rate': 1 - accuracy, 'error_cases': error_cases[:10],
                  'detailed_results': detailed_results}
        print(f"âœ… {model_name} å‡†ç¡®ç‡: {accuracy:.2%} ({correct_count}/{total_count})")
        return result

    def evaluate_routing_accuracy(self, simple_problems: List, complex_problems: List) -> Dict:
        """è¯„ä¼°è·¯ç”±å‡†ç¡®æ€§"""
        print(f"\nğŸ§­ è¯„ä¼°è·¯ç”±å‡†ç¡®æ€§...")
        self._ensure_slm_loaded()
        correct_routes, total_routes, routing_details = 0, 0, []
        test_simple = simple_problems[:min(50, len(simple_problems))]
        for problem in test_simple:
            predicted_route, _ = self.router.route(problem['question'], self.slm.model, self.slm.tokenizer)
            is_correct = (predicted_route == "SLM")
            if is_correct: correct_routes += 1
            routing_details.append(
                {'question': problem['question'][:100], 'true_complexity': 'simple', 'expected': "SLM",
                 'predicted': predicted_route, 'correct': is_correct})
            total_routes += 1
        test_complex = complex_problems[:min(50, len(complex_problems))]
        for problem in test_complex:
            predicted_route, _ = self.router.route(problem['question'], self.slm.model, self.slm.tokenizer)
            is_correct = (predicted_route == "LLM")
            if is_correct: correct_routes += 1
            routing_details.append(
                {'question': problem['question'][:100], 'true_complexity': 'complex', 'expected': "LLM",
                 'predicted': predicted_route, 'correct': is_correct})
            total_routes += 1
        routing_accuracy = correct_routes / total_routes if total_routes > 0 else 0
        print(f"âœ… è·¯ç”±å‡†ç¡®ç‡: {routing_accuracy:.2%} ({correct_routes}/{total_routes})")
        return {'routing_accuracy': routing_accuracy, 'correct_routes': correct_routes, 'total_routes': total_routes,
                'routing_details': routing_details}

    def run_gsm8k_evaluation(self, n_samples: int = 200, simple_ratio: float = 0.5):
        """è¿è¡ŒGSM8Kè¯„ä¼°"""
        print("=" * 60 + "\nğŸš€ å¼€å§‹åŸºäºGSM8Kçš„å‡†ç¡®ç‡è¯„ä¼°\n" + "=" * 60)
        print(f"ğŸ“Š å‡†å¤‡GSM8Kæ•°æ® (æ ·æœ¬æ•°: {n_samples})...")
        simple_problems, complex_problems = self.data_processor.get_balanced_sample(n_total=n_samples,
                                                                                    simple_ratio=simple_ratio)
        if not simple_problems and not complex_problems: print("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®"); return None

        slm_simple_results = self.evaluate_model_on_problems(self.slm, simple_problems, "SLM on Simple")
        llm_complex_results = self.evaluate_model_on_problems(self.llm, complex_problems, "LLM on Complex")
        slm_complex_results = self.evaluate_model_on_problems(self.slm, complex_problems, "SLM on Complex (X-Eval)")
        llm_simple_results = self.evaluate_model_on_problems(self.llm, simple_problems, "LLM on Simple (X-Eval)")
        routing_results = self.evaluate_routing_accuracy(simple_problems, complex_problems)

        smart_routing_results = self._calculate_smart_routing_performance(slm_simple_results, llm_complex_results,
                                                                          routing_results)
        self._generate_final_report(slm_simple_results, llm_complex_results, slm_complex_results, llm_simple_results,
                                    routing_results, smart_routing_results, n_samples)

    def _calculate_smart_routing_performance(self, slm_res, llm_res, rout_res):
        """è®¡ç®—æ™ºèƒ½è·¯ç”±ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½"""
        rout_acc, slm_acc, llm_acc = rout_res['routing_accuracy'], slm_res['accuracy'], llm_res['accuracy']
        simple_ratio = slm_res['total_count'] / max(1, slm_res['total_count'] + llm_res['total_count'])
        # é¢„ä¼°å‡†ç¡®ç‡ = (æ­£ç¡®è·¯ç”±çš„éƒ¨åˆ† * å…¶å¯¹åº”çš„æ¨¡å‹å‡†ç¡®ç‡) + (é”™è¯¯è·¯ç”±çš„éƒ¨åˆ† * å…¶å¯¹åº”çš„æ¨¡å‹å‡†ç¡®ç‡)
        # ç®€å•é—®é¢˜è¢«æ­£ç¡®è·¯ç”±(SLM)çš„å‡†ç¡®ç‡ + ç®€å•é—®é¢˜è¢«é”™è¯¯è·¯ç”±(LLM)çš„å‡†ç¡®ç‡ + å¤æ‚é—®é¢˜è¢«æ­£ç¡®è·¯ç”±(LLM)çš„å‡†ç¡®ç‡ + å¤æ‚é—®é¢˜è¢«é”™è¯¯è·¯ç”±(SLM)çš„å‡†ç¡®ç‡
        # è¿™é‡Œç®€åŒ–ä¸ºï¼š(ç†æƒ³æƒ…å†µä¸‹çš„å‡†ç¡®ç‡ * è·¯ç”±å‡†ç¡®ç‡) + (éšæœºæƒ…å†µä¸‹çš„å‡†ç¡®ç‡ * è·¯ç”±é”™è¯¯ç‡)
        est_acc = ((slm_acc * simple_ratio) + (llm_acc * (1 - simple_ratio))) * rout_acc + 0.5 * (1 - rout_acc)

        smart_cost = simple_ratio * self.slm_config.cost_per_token + (1 - simple_ratio) * self.llm_config.cost_per_token
        llm_cost, slm_cost = self.llm_config.cost_per_token, self.slm_config.cost_per_token
        return {'estimated_accuracy': est_acc, 'cost_per_problem': smart_cost,
                'cost_savings_vs_llm': (llm_cost - smart_cost) / llm_cost,
                'cost_increase_vs_slm': (smart_cost - slm_cost) / slm_cost}

    def _generate_final_report(self, s_s, l_c, s_c, l_s, r, s_r, n):
        print("\n" + "=" * 70 + "\nğŸ“‹ GSM8Kæ•°æ®é›†æ™ºèƒ½è·¯ç”±å‡†ç¡®ç‡è¯„ä¼°æŠ¥å‘Š\n" + "=" * 70)
        print(f"ğŸ“Š è¯„ä¼°è§„æ¨¡: {n} é“GSM8KçœŸé¢˜")
        print(f"\nğŸ¯ æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡:")
        print(
            f"â”œâ”€â”€ SLMåœ¨ç®€å•é—®é¢˜å‡†ç¡®ç‡: {s_s['accuracy']:.2%}\nâ”œâ”€â”€ LLMåœ¨å¤æ‚é—®é¢˜å‡†ç¡®ç‡: {l_c['accuracy']:.2%}\nâ”œâ”€â”€ è·¯ç”±åˆ¤æ–­å‡†ç¡®ç‡: {r['routing_accuracy']:.2%}\nâ””â”€â”€ æ™ºèƒ½è·¯ç”±é¢„ä¼°å‡†ç¡®ç‡: {s_r['estimated_accuracy']:.2%}")
        print(
            f"\nğŸ“Š äº¤å‰éªŒè¯ç»“æœ:\nâ”œâ”€â”€ SLMåœ¨å¤æ‚é—®é¢˜å‡†ç¡®ç‡: {s_c['accuracy']:.2%} (è¯æ˜å±€é™æ€§)\nâ”œâ”€â”€ LLMåœ¨ç®€å•é—®é¢˜å‡†ç¡®ç‡: {l_s['accuracy']:.2%} (è¯æ˜è¿‡åº¦é…ç½®)")
        print(
            f"\nğŸ’° æˆæœ¬æ•ˆç›Šåˆ†æ:\nâ”œâ”€â”€ çº¯SLMæˆæœ¬: ${self.slm_config.cost_per_token:.4f}/é¢˜\nâ”œâ”€â”€ çº¯LLMæˆæœ¬: ${self.llm_config.cost_per_token:.4f}/é¢˜\nâ”œâ”€â”€ æ™ºèƒ½è·¯ç”±æˆæœ¬: ${s_r['cost_per_problem']:.4f}/é¢˜\nâ”œâ”€â”€ vs LLMèŠ‚çœ: {s_r['cost_savings_vs_llm']:.1%}")
        slm_ok, rout_ok, cost_ok = s_s['accuracy'] >= 0.8, r['routing_accuracy'] >= 0.85, s_r[
            'cost_savings_vs_llm'] >= 0.3
        print(
            f"\nğŸ’¡ å…³é”®åˆ¤æ–­:\nâ”œâ”€â”€ SLMå¯é æ€§: {'âœ…' if slm_ok else 'âŒ'} ({s_s['accuracy']:.1%})\nâ”œâ”€â”€ è·¯ç”±å¯é æ€§: {'âœ…' if rout_ok else 'âŒ'} ({r['routing_accuracy']:.1%})\nâ””â”€â”€ æˆæœ¬æ•ˆç›Š: {'âœ…' if cost_ok else 'âŒ'} ({s_r['cost_savings_vs_llm']:.1%})")
        if slm_ok and rout_ok and cost_ok:
            rec = "âœ… å¼ºçƒˆæ¨èä½¿ç”¨æ™ºèƒ½è·¯ç”±"
        elif not slm_ok:
            rec = "âŒ ä¸æ¨è - SLMä¸å¯é "
        elif not rout_ok:
            rec = "âŒ ä¸æ¨è - è·¯ç”±ä¸å‡†ç¡®"
        else:
            rec = "âš ï¸ è°¨æ…è€ƒè™‘ - æˆæœ¬æ•ˆç›Šæœ‰é™"
        print(f"\nğŸ¯ æœ€ç»ˆå»ºè®®: {rec}")


# ========================= æ–°å¢: è®­ç»ƒæ¨¡å— =========================
def generate_router_training_data(evaluator: GSM8KAccuracyEvaluator, output_file="router_training_data.jsonl"):
    """ç”Ÿæˆç”¨äºè®­ç»ƒè·¯ç”±å™¨çš„æ•°æ®é›†ã€‚"""
    print("\n" + "=" * 50 + "\nğŸ§  å¼€å§‹ç”Ÿæˆè·¯ç”±å™¨è®­ç»ƒæ•°æ®...\n" + "=" * 50)
    evaluator._ensure_slm_loaded()
    slm_interface = evaluator.slm
    all_problems = evaluator.data_processor.samples
    print(f"ğŸ“Š å°†å¤„ç† {len(all_problems)} ä¸ªé—®é¢˜æ¥ç”Ÿæˆç‰¹å¾å’Œæ ‡ç­¾ã€‚")
    training_samples = []
    # ä½¿ç”¨ä¸´æ—¶çš„ã€åŸºäºè§„åˆ™çš„åˆ†æå™¨æ¥æå–ç‰¹å¾
    temp_feature_extractor = LearnedAttentionRouter("dummy_path.pth", device)  # åˆ›å»ºå®ä¾‹ä»¥ä½¿ç”¨å…¶ç‰¹å¾æå–æ–¹æ³•

    for i, problem in enumerate(all_problems):
        question = problem['question']
        ground_truth_answer = evaluator.data_processor.extract_answer(problem['answer'])
        if i % 20 == 0: print(f"   è¿›åº¦: {i}/{len(all_problems)}")
        try:
            slm_response = slm_interface.predict(question)
            slm_extracted_answer = evaluator.validator.extract_final_answer(slm_response)
            is_slm_correct = evaluator.validator.is_correct(slm_extracted_answer, ground_truth_answer)
            is_complex_label = 1.0 if not is_slm_correct else 0.0
            attention_features = temp_feature_extractor.extract_core_features(question, slm_interface.model,
                                                                              slm_interface.tokenizer)
            training_samples.append({"features": attention_features, "label": is_complex_label})
        except Exception as e:
            print(f"   âš ï¸ è·³è¿‡é—®é¢˜ {i}ï¼Œå¤„ç†é”™è¯¯: {e}");
            continue
    with open(output_file, 'w', encoding='utf-8') as f:
        for sample in training_samples: f.write(json.dumps(sample) + '\n')
    print(f"\nâœ… è®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæ¯•! å…± {len(training_samples)} æ¡æ ·æœ¬å·²ä¿å­˜è‡³ {output_file}")


class RouterDataset(Dataset):
    def __init__(self, training_data):
        self.samples = []
        for sample in training_data:
            feature_vector = [sample['features']['avg_entropy'], sample['features']['entropy_std'],
                              sample['features']['max_entropy'], sample['features']['avg_variance'],
                              sample['features']['variance_std'], sample['features']['max_variance'],
                              sample['features']['avg_max_attention'], sample['features']['concentration_std']]
            self.samples.append({"features": torch.tensor(feature_vector, dtype=torch.float32),
                                 "label": torch.tensor([sample['label']], dtype=torch.float32)})

    def __len__(self): return len(self.samples)

    def __getitem__(self, idx): return self.samples[idx]


def train_router(training_data_path="router_training_data.jsonl", epochs=20, lr=1e-4, batch_size=32):
    """è®­ç»ƒå¤æ‚åº¦é¢„æµ‹ç½‘ç»œ"""
    print("\n" + "=" * 50 + "\nğŸš€ å¼€å§‹è®­ç»ƒæ™ºèƒ½è·¯ç”±å™¨...\n" + "=" * 50)
    training_data = []
    with open(training_data_path, 'r', encoding='utf-8') as f:
        for line in f: training_data.append(json.loads(line))
    dataset = RouterDataset(training_data)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    model = ComplexityPredictorNet().to(device)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    for epoch in range(epochs):
        model.train()
        total_loss, correct_preds, total_samples = 0, 0, 0
        for batch in dataloader:
            features, labels = batch['features'].to(device), batch['label'].to(device)
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            preds = torch.sigmoid(outputs) > 0.5
            correct_preds += (preds == labels.bool()).sum().item()
            total_samples += labels.size(0)
        avg_loss = total_loss / len(dataloader)
        accuracy = correct_preds / total_samples
        print(f"Epoch {epoch + 1}/{epochs} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2%}")
    model_save_path = "router_model.pth"
    torch.save(model.state_dict(), model_save_path)
    print(f"\nâœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹å·²ä¿å­˜è‡³ {model_save_path}")


# ========================= ä¸»å‡½æ•°å’Œå·¥å…·å‡½æ•° =========================
def get_secure_token():
    """å®‰å…¨è·å–token"""
    hf_token = os.getenv('HUGGINGFACE_TOKEN')
    if hf_token: print("âœ… ä»ç¯å¢ƒå˜é‡è·å–token"); return hf_token
    print("ğŸ”‘ è¯·è¾“å…¥ä½ çš„HuggingFace Token:")
    return getpass.getpass("Token: ")


def test_model_access(hf_token):
    """æµ‹è¯•tokenæ˜¯å¦å¯ä»¥è®¿é—®æŒ‡å®šçš„æ¨¡å‹"""
    try:
        login(token=hf_token);
        print("âœ… HuggingFace token valid")
        print("ğŸ”„ Testing Llama-3.2-3B access...");
        AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-3B");
        print("âœ… Llama-3.2-3B access successful")
        print("ğŸ”„ Testing Llama-3.1-8B access...");
        AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct");
        print("âœ… Llama-3.1-8B access successful")
        return True
    except Exception as e:
        print(f"âŒ Model access test failed: {e}"); return False


def main():
    """ä¸»å‡½æ•° - æ”¯æŒè®­ç»ƒå’Œè¯„ä¼°æ¨¡å¼"""
    print("ğŸ”¬ GSM8Kæ™ºèƒ½è·¯ç”±å‡†ç¡®ç‡è¯„ä¼°ç³»ç»Ÿ\n" + "=" * 50)
    hf_token = get_secure_token()
    if not hf_token: print("âŒ æ— æ³•è·å–token"); return None

    # æ¨¡å¼é€‰æ‹©
    mode = input("è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼ (train/eval): ").strip().lower()

    try:
        if mode == 'train':
            evaluator = GSM8KAccuracyEvaluator(hf_token=hf_token, max_samples=2000)
            generate_router_training_data(evaluator)
            train_router()
        elif mode == 'eval':
            evaluator = GSM8KAccuracyEvaluator(hf_token=hf_token, max_samples=1000)
            evaluator.run_gsm8k_evaluation(n_samples=200, simple_ratio=0.5)
        else:
            print("æ— æ•ˆçš„æ¨¡å¼ã€‚è¯·è¾“å…¥ 'train' æˆ– 'eval'ã€‚")
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        hf_token = None


if __name__ == "__main__":
    main()