"""
ä¼˜åŒ–åçš„é˜¶æ®µ1å®éªŒï¼šç»“åˆå®Œæ•´ç»Ÿè®¡åˆ†æå’Œè·¯ç”±å†³ç­–
"""
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Dict, List

# æ³¨æ„åŠ›ç†µåˆ†æå™¨
class OptimizedAttentionEntropyAnalyzer:
    def __init__(self, entropy_threshold=2.0):
        self.entropy_threshold = entropy_threshold # ç†µçš„é˜ˆå€¼ï¼Œç”¨äºå½’ä¸€åŒ–
        self.complexity_history = []              # å­˜å‚¨å†å²è®°å½•

#entropy_threshold=2.0 çš„å«ä¹‰ï¼šè¿™æ˜¯ä¸€ä¸ªç»éªŒå€¼ï¼Œç”¨æ¥å°†ç†µå€¼è½¬æ¢ä¸º0-1çš„å¤æ‚åº¦åˆ†æ•° ç†µå€¼é€šå¸¸åœ¨0-4ä¹‹é—´ï¼Œ2.0ä½œä¸ºä¸­ç‚¹æ¯”è¾ƒåˆç† ï¼ˆåç»­ç–‘é—®ï¼‰

    def calculate_attention_entropy(self, attention_weights):
        """
        è®¡ç®—æ³¨æ„åŠ›ç†µ - æ”¯æŒå¤šç§è®¡ç®—æ–¹å¼
        è¾“å…¥: attention_weights [num_heads, seq_len] æˆ– [seq_len, seq_len]
        """
        # æ”¯æŒä¸¤ç§è¾“å…¥æ ¼å¼ï¼š
        # æ ¼å¼1: [num_heads, seq_len, seq_len] - å®Œæ•´æ³¨æ„åŠ›çŸ©é˜µ
        # æ ¼å¼2: [num_heads, seq_len] - å•ä¸ªtokençš„æ³¨æ„åŠ›åˆ†å¸ƒ

        if len(attention_weights.shape) == 3:  # [num_heads, seq_len, seq_len]
            # å¯¹äºå®Œæ•´attentionçŸ©é˜µï¼Œè®¡ç®—æ¯ä¸ªqueryçš„ç†µ
            entropies = []
            for head in attention_weights: # éå†æ¯ä¸ªæ³¨æ„åŠ›å¤´
                head_entropies = []
                for i in range(head.shape[0]):
                    attn_dist = head[i] + 1e-9  # é¿å…log(0) # é¿å…æ•°å­¦é”™è¯¯ï¼šlog(0) = -âˆ , 1e-9 æ˜¯ä¸€ä¸ªæå°çš„æ•°ï¼Œä¸ä¼šå½±å“ç»“æœä½†èƒ½é¿å…æ•°å€¼é—®é¢˜
                    # ç†µå…¬å¼ï¼šH = -Î£(p * log(p))
                    entropy = -torch.sum(attn_dist * torch.log(attn_dist))
                    head_entropies.append(entropy.item())
                entropies.append(np.mean(head_entropies)) # è¯¥å¤´çš„å¹³å‡ç†µ
            return entropies

        else:  # [num_heads, seq_len] - å•ä¸ªtokençš„æ³¨æ„åŠ›åˆ†å¸ƒ
            entropies = []
            for head_attn in attention_weights:
                head_attn = head_attn + 1e-9
                entropy = -torch.sum(head_attn * torch.log(head_attn))
                entropies.append(entropy.item())
            return entropies

    def predict_complexity(self, attention_weights):
        """åŸºäºæ³¨æ„åŠ›ç†µé¢„æµ‹å¤æ‚åº¦"""
        entropies = self.calculate_attention_entropy(attention_weights)
        avg_entropy = np.mean(entropies)   # å¹³å‡ç†µ
        max_entropy = np.max(entropies)    # æœ€å¤§ç†µ
        entropy_std = np.std(entropies)    # ç†µçš„æ ‡å‡†å·®

        # å¤šç»´åº¦å¤æ‚åº¦è¯„ä¼°
        complexity_score = min(avg_entropy / self.entropy_threshold, 1.0)

        return {
            'complexity_score': complexity_score,  # ä¸»è¦æŒ‡æ ‡
            'avg_entropy': avg_entropy,
            'max_entropy': max_entropy,
            'entropy_std': entropy_std,
            'head_entropies': entropies,
            'is_complex': complexity_score > 0.5  # äºŒåˆ†ç±»ç»“æœ
        }

    def should_route_to_cloud(self, attention_weights, threshold=0.5):
        """è·¯ç”±å†³ç­–"""
        result = self.predict_complexity(attention_weights)
        return result['complexity_score'] > threshold, result

'''
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”å®éªŒæ¡†æ¶â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
'''

class ComprehensiveBaselineExperiment:
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    def __init__(self, model_name="microsoft/DialoGPT-small"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True) # å…³é”®ï¼šè¾“å‡ºæ³¨æ„åŠ›æƒé‡
        self.analyzer = OptimizedAttentionEntropyAnalyzer()

        # è®¾ç½®pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # æ‰©å±•çš„æµ‹è¯•æ•°æ®é›†
        self.task_dataset = {
            "simple": [
                "What is 2+2?",
                "The capital of France is",
                "My name is",
                "What color is the sky?",
                "How many days in a week?",
                "What is water made of?",
                "The sun rises in the",
                "1+1 equals",
                "Cats are",
                "The alphabet starts with"
            ],
            "medium": [
                "Why do objects fall down?",
                "How does a bicycle work?",
                "What causes rain?",
                "Why is the ocean salty?",
                "How do plants make food?",
                "What makes ice float?",
                "Why do we have seasons?",
                "How do computers work?",
                "What is electricity?",
                "Why do we dream?"
            ],
            "complex": [
                "Explain the relationship between quantum mechanics and general relativity",
                "Analyze the economic impact of artificial intelligence on employment",
                "What would happen if gravity suddenly became twice as strong?",
                "Discuss the ethical implications of genetic engineering",
                "How might climate change affect global food security?",
                "Evaluate the societal effects of social media on democracy",
                "If I have 3 apples and give away 1.5 apples, then buy 2.7 more apples, what's the philosophical meaning?",
                "Compare the advantages and disadvantages of different renewable energy sources",
                "How do cultural differences affect international business negotiations?",
                "What are the long-term consequences of space exploration for humanity?"
            ]
        }

    def extract_attention_features(self, text, method="last_token"):
        """æå–æ³¨æ„åŠ›ç‰¹å¾"""
        inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)

        with torch.no_grad():
            outputs = self.model(**inputs, output_attentions=True)

        # è·å–æœ€åä¸€å±‚çš„attention
        last_attention = outputs.attentions[-1][0]  # [num_heads, seq_len, seq_len]

        if method == "last_token":
            # åˆ†ææœ€åä¸€ä¸ªæœ‰æ•ˆtokençš„attention pattern
            # å¯¹äºç”Ÿæˆå¼æ¨¡å‹ï¼Œæœ€åä¸€ä¸ªtokenï¼š
            # 1. åŒ…å«äº†å¯¹æ•´ä¸ªåºåˆ—çš„ç†è§£
            # 2. æ˜¯æ¨¡å‹è¿›è¡Œä¸‹ä¸€æ­¥é¢„æµ‹çš„åŸºç¡€
            # 3. æœ€èƒ½åæ˜ ä»»åŠ¡çš„æ•´ä½“å¤æ‚åº¦
            seq_len = inputs['attention_mask'].sum().item()
            last_token_attn = last_attention[:, seq_len - 1, :]  # [num_heads, seq_len]
            return last_token_attn

        elif method == "average":
            # å¹³å‡æ‰€æœ‰tokençš„attention
            return last_attention

        else:
            raise ValueError(f"Unknown method: {method}")

    def run_single_task(self, text, complexity_label):
        """è¿è¡Œå•ä¸ªä»»åŠ¡"""
        attention_weights = self.extract_attention_features(text)
        result = self.analyzer.predict_complexity(attention_weights)

        # æ·»åŠ çœŸå®æ ‡ç­¾
        result['true_complexity'] = complexity_label
        result['task'] = text

        return result

    def run_comprehensive_experiment(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        print("ğŸ§ª è¿è¡Œä¼˜åŒ–åçš„é˜¶æ®µ1å®éªŒ...")

        all_results = []

        # è¿è¡Œæ‰€æœ‰ä»»åŠ¡
        for complexity, tasks in self.task_dataset.items():
            print(f"\nğŸ“Š å¤„ç†{complexity}ä»»åŠ¡...")

            for task in tasks:
                result = self.run_single_task(task, complexity)
                all_results.append(result)
                print(f"'{task[:50]}...' -> å¤æ‚åº¦={result['complexity_score']:.3f}")

        # è½¬æ¢ä¸ºDataFrameä¾¿äºåˆ†æ
        df = pd.DataFrame(all_results)

        return self.analyze_results(df)

    def analyze_results(self, df):
        """å®Œæ•´çš„ç»“æœåˆ†æ"""
        print("\n" + "=" * 60)
        print("ğŸ“ˆ å®éªŒç»“æœåˆ†æ")
        print("=" * 60)

        # 1. æè¿°æ€§ç»Ÿè®¡
        summary = df.groupby('true_complexity')['complexity_score'].agg([
            'count', 'mean', 'std', 'min', 'max'
        ]).round(3)
        print("\n1. æè¿°æ€§ç»Ÿè®¡:")
        print(summary)

        # 2. å¯è§†åŒ–
        self.plot_results(df)

        # 3. ç»Ÿè®¡æ£€éªŒ
        self.statistical_tests(df)

        # 4. ç›¸å…³æ€§åˆ†æ
        self.correlation_analysis(df)

        # 5. è·¯ç”±å†³ç­–åˆ†æ
        self.routing_analysis(df)

        return df

    def plot_results(self, df):
        """ç»“æœå¯è§†åŒ–"""
        plt.figure(figsize=(15, 5))

        # ç®±çº¿å›¾
        plt.subplot(1, 3, 1)
        df.boxplot(column='complexity_score', by='true_complexity', ax=plt.gca())
        plt.title('å¤æ‚åº¦åˆ†æ•°åˆ†å¸ƒ')
        plt.ylabel('å¤æ‚åº¦åˆ†æ•°')

        # å¹³å‡ç†µå¯¹æ¯”
        plt.subplot(1, 3, 2)
        df.boxplot(column='avg_entropy', by='true_complexity', ax=plt.gca())
        plt.title('å¹³å‡æ³¨æ„åŠ›ç†µåˆ†å¸ƒ')
        plt.ylabel('å¹³å‡ç†µ')

        # æ•£ç‚¹å›¾
        plt.subplot(1, 3, 3)
        complexity_mapping = {'simple': 1, 'medium': 2, 'complex': 3}
        df['complexity_numeric'] = df['true_complexity'].map(complexity_mapping)
        plt.scatter(df['complexity_numeric'], df['complexity_score'], alpha=0.6)
        plt.xlabel('çœŸå®å¤æ‚åº¦')
        plt.ylabel('é¢„æµ‹å¤æ‚åº¦åˆ†æ•°')
        plt.title('çœŸå® vs é¢„æµ‹å¤æ‚åº¦')

        plt.tight_layout()
        plt.show()

    def statistical_tests(self, df):
        """ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
        print("\n2. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ:")

        simple_scores = df[df['true_complexity'] == 'simple']['complexity_score']
        medium_scores = df[df['true_complexity'] == 'medium']['complexity_score']
        complex_scores = df[df['true_complexity'] == 'complex']['complexity_score']

        # ANOVAæ£€éªŒ
        f_stat, p_value = stats.f_oneway(simple_scores, medium_scores, complex_scores)
        print(f"ANOVA Fç»Ÿè®¡é‡: {f_stat:.4f}, på€¼: {p_value:.4f}")

        if p_value < 0.05:
            print("âœ… ç»„é—´å·®å¼‚æ˜¾è‘— (p < 0.05)")
        else:
            print("âŒ ç»„é—´å·®å¼‚ä¸æ˜¾è‘— (p >= 0.05)")

        # ä¸¤ä¸¤æ¯”è¾ƒ
        from scipy.stats import ttest_ind
        t1, p1 = ttest_ind(simple_scores, complex_scores)
        print(f"ç®€å• vs å¤æ‚ä»»åŠ¡ tæ£€éªŒ: t={t1:.3f}, p={p1:.4f}")

    def correlation_analysis(self, df):
        """ç›¸å…³æ€§åˆ†æ"""
        print("\n3. ç›¸å…³æ€§åˆ†æ:")

        complexity_mapping = {'simple': 1, 'medium': 2, 'complex': 3}
        df['complexity_numeric'] = df['true_complexity'].map(complexity_mapping)

        correlation = df['complexity_score'].corr(df['complexity_numeric'])
        print(f"å¤æ‚åº¦åˆ†æ•°ä¸çœŸå®å¤æ‚åº¦çš„ç›¸å…³ç³»æ•°: {correlation:.4f}")

        if correlation > 0.5:
            print("âœ… å¼ºæ­£ç›¸å…³ - å‡è®¾éªŒè¯æˆåŠŸ!")
        elif correlation > 0.3:
            print("âš ï¸ ä¸­ç­‰ç›¸å…³ - æœ‰ä¸€å®šæ•ˆæœä½†éœ€æ”¹è¿›")
        else:
            print("âŒ å¼±ç›¸å…³ - éœ€è¦é‡æ–°è€ƒè™‘æ–¹æ³•")

    def routing_analysis(self, df):
        """è·¯ç”±å†³ç­–åˆ†æ"""
        print("\n4. è·¯ç”±å†³ç­–åˆ†æ:")

        # è®¡ç®—æ¯ä¸ªå¤æ‚åº¦çº§åˆ«çš„è·¯ç”±å†³ç­–
        routing_stats = df.groupby('true_complexity')['is_complex'].agg([
            'count', 'sum', lambda x: (x.sum() / len(x) * 100)
        ]).round(1)
        routing_stats.columns = ['æ€»æ•°', 'è·¯ç”±åˆ°äº‘ç«¯', 'è·¯ç”±æ¯”ä¾‹(%)']
        print(routing_stats)

        # ç†æƒ³æƒ…å†µï¼šsimpleéƒ½ä¸è·¯ç”±ï¼Œcomplexéƒ½è·¯ç”±
        simple_correct = (df[df['true_complexity'] == 'simple']['is_complex'] == False).sum()
        complex_correct = (df[df['true_complexity'] == 'complex']['is_complex'] == True).sum()

        simple_total = len(df[df['true_complexity'] == 'simple'])
        complex_total = len(df[df['true_complexity'] == 'complex'])

        print(f"\nè·¯ç”±å‡†ç¡®æ€§:")
        print(f"ç®€å•ä»»åŠ¡æ­£ç¡®è·¯ç”±ç‡: {simple_correct / simple_total * 100:.1f}%")
        print(f"å¤æ‚ä»»åŠ¡æ­£ç¡®è·¯ç”±ç‡: {complex_correct / complex_total * 100:.1f}%")

    def save_results(self, df, filename="stage1_results.csv"):
        """ä¿å­˜ç»“æœ"""
        df.to_csv(filename, index=False)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° {filename}")


if __name__ == "__main__":
    # è¿è¡Œå®éªŒ
    experiment = ComprehensiveBaselineExperiment()
    results_df = experiment.run_comprehensive_experiment()

    # ä¿å­˜ç»“æœ
    experiment.save_results(results_df)

    print("\nğŸ‰ é˜¶æ®µ1å®éªŒå®Œæˆ!")