"""
å®Œæ•´çš„GSM8Kæ™ºèƒ½è·¯ç”±å‡†ç¡®ç‡è¯„ä¼°ç³»ç»Ÿ
æ•´åˆçœŸå®GSM8Kæ•°æ®é›†ã€æ³¨æ„åŠ›æœºåˆ¶å¤æ‚åº¦åˆ†æå™¨å’Œæ¨¡å‹æ¥å£
æ ¸å¿ƒç›®æ ‡ï¼šéªŒè¯SLM/LLMçš„çœŸå®å‡†ç¡®ç‡ï¼Œç¡®ä¿è·¯ç”±å†³ç­–çš„å¯é æ€§
"""

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login
import re
import json
import os
import warnings
import getpass
from typing import Dict, List, Tuple, Optional
import random

warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# GPUè®¾ç½®
if torch.cuda.is_available():
    device = torch.device('cuda')
    print(f"ğŸš€ Using GPU: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device('cpu')
    print("ğŸ’» Using CPU")


# ========================= åŸºç¡€é…ç½®ç±» =========================
class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»"""

    def __init__(self, name: str, model_path: str, cost_per_token: float, avg_latency_ms: int):
        self.name = name
        self.model_path = model_path
        self.cost_per_token = cost_per_token
        self.avg_latency_ms = avg_latency_ms


class ModelInterface:
    """æ¨¡å‹æ¥å£åŸºç±»"""

    def __init__(self, config: ModelConfig):
        self.config = config
        self.model = None
        self.tokenizer = None

    def predict(self, question: str) -> str:
        raise NotImplementedError


# ========================= GSM8Kæ•°æ®å¤„ç†å™¨ =========================
class FixedGSM8KProcessor:
    """ä¿®å¤ç‰ˆGSM8Kæ•°æ®å¤„ç†å™¨"""

    def __init__(self, data_path="gsm8k_data/train.jsonl", max_samples=1000):
        print(f"ğŸ“š Loading GSM8K dataset...")
        self.data_path = data_path
        self.max_samples = max_samples
        self.samples = []

        # å°è¯•å¤šç§åŠ è½½æ–¹å¼
        if self._load_from_datasets():
            print(f"âœ… Loaded {len(self.samples)} samples from datasets library")
        elif self._load_from_local():
            print(f"âœ… Loaded {len(self.samples)} samples from local file")
        else:
            print("ğŸ”„ Using enhanced fallback data...")
            self.samples = self._create_balanced_fallback()

    def _load_from_datasets(self):
        """ä»datasetsåº“åŠ è½½GSM8K"""
        try:
            from datasets import load_dataset
            print("ğŸ”„ Loading from HuggingFace datasets...")

            # ä½¿ç”¨GSM8Kçš„testé›†ï¼Œå› ä¸ºå®ƒæœ‰æ ‡å‡†ç­”æ¡ˆ
            dataset = load_dataset("gsm8k", "main")
            test_data = dataset['test']

            for i in range(min(self.max_samples, len(test_data))):
                self.samples.append({
                    'question': test_data[i]['question'],
                    'answer': test_data[i]['answer']
                })
            return len(self.samples) > 0
        except Exception as e:
            print(f"âš ï¸ Failed to load from datasets: {e}")
            return False

    def _load_from_local(self):
        """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½"""
        try:
            if not os.path.exists(self.data_path):
                return False

            with open(self.data_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if line.strip():
                        try:
                            data = json.loads(line)
                            self.samples.append({
                                'question': data['question'],
                                'answer': data['answer']
                            })
                            if len(self.samples) >= self.max_samples:
                                break
                        except:
                            continue
            return len(self.samples) > 0
        except:
            return False

    def _create_balanced_fallback(self):
        """åˆ›å»ºå¹³è¡¡çš„åå¤‡æ•°æ®é›†"""
        fallback_data = [
            # ç®€å•é—®é¢˜
            {
                'question': "Janet has 3 apples. She eats 1 apple. How many apples does she have left?",
                'answer': "Janet starts with 3 apples.\nShe eats 1 apple.\n3 - 1 = 2\n#### 2"
            },
            {
                'question': "Tom bought 5 books for $2 each. How much did he spend in total?",
                'answer': "Tom bought 5 books.\nEach book costs $2.\n5 Ã— 2 = 10\n#### 10"
            },
            {
                'question': "There are 8 students in a class. If 3 students are absent, how many are present?",
                'answer': "Total students: 8\nAbsent students: 3\nPresent students: 8 - 3 = 5\n#### 5"
            },
            # ä¸­ç­‰éš¾åº¦é—®é¢˜
            {
                'question': "Sarah has twice as many stickers as Tom. Tom has 12 stickers. How many stickers do they have together?",
                'answer': "Tom has 12 stickers.\nSarah has twice as many as Tom: 2 Ã— 12 = 24 stickers.\nTogether they have: 12 + 24 = 36 stickers.\n#### 36"
            },
            {
                'question': "A store sells apples for $3 per kg. If John buys 2.5 kg and pays with a $10 bill, how much change does he get?",
                'answer': "Cost per kg: $3\nAmount bought: 2.5 kg\nTotal cost: 3 Ã— 2.5 = $7.50\nPaid: $10\nChange: 10 - 7.50 = $2.50\n#### 2.5"
            },
            # å¤æ‚é—®é¢˜
            {
                'question': "A company has 120 employees. 25% work in sales, 30% in engineering, and the rest in administration. If the sales team gets a 15% increase and engineering gets a 10% increase, how many total employees will there be after the increases?",
                'answer': "Initial employees: 120\nSales: 25% of 120 = 0.25 Ã— 120 = 30 employees\nEngineering: 30% of 120 = 0.30 Ã— 120 = 36 employees\nAdministration: 120 - 30 - 36 = 54 employees\n\nAfter increases:\nSales increase: 30 Ã— 0.15 = 4.5 â‰ˆ 5 new employees\nEngineering increase: 36 Ã— 0.10 = 3.6 â‰ˆ 4 new employees\n\nTotal after increases: 120 + 5 + 4 = 129 employees\n#### 129"
            }
        ]
        print(f"âœ… Created {len(fallback_data)} fallback samples")
        return fallback_data

    def extract_answer(self, answer_text: str) -> str:
        """ä»GSM8Kçš„ç­”æ¡ˆæ–‡æœ¬ä¸­æå–æ•°å€¼"""
        # GSM8Kæ ‡å‡†æ ¼å¼: "#### 42"
        match = re.search(r'####\s*([+-]?\d+(?:\.\d+)?)', answer_text)
        if match:
            return match.group(1)

        # å¤‡ç”¨æ–¹æ¡ˆï¼šæå–æœ€åä¸€ä¸ªæ•°å­—
        numbers = re.findall(r'([+-]?\d+(?:\.\d+)?)', answer_text)
        if numbers:
            return numbers[-1]

        return "No answer found"

    def count_solution_steps(self, answer: str) -> int:
        """å¤šç»´åº¦æ­¥éª¤è¯†åˆ«ç»¼åˆåˆ¤æ–­æ¨ç†å¤æ‚åº¦"""
        # æ–¹æ³•1: è®¡ç®—è¡Œæ•°ï¼ˆæ¯è¡Œä¸€ä¸ªé€»è¾‘æ­¥éª¤ï¼‰
        lines = [line.strip() for line in answer.split('\n') if line.strip()]
        meaningful_lines = [line for line in lines if not line.startswith('####')]

        # æ–¹æ³•2: æ•°å­¦è¿ç®—è®¡æ•°
        math_operations = len(re.findall(r'\d+\s*[+\-Ã—Ã·*/]\s*\d+', answer))

        # æ–¹æ³•3: ç­‰å·è®¡æ•°
        equals_count = answer.count('=')

        # æ–¹æ³•4: æ­¥éª¤æ ‡å¿—è¯
        step_words = len(re.findall(r'\b(then|next|so|therefore|thus|after|now|finally)\b', answer.lower()))

        # ç»¼åˆåˆ¤æ–­
        steps = max(len(meaningful_lines) - 1, math_operations, equals_count, step_words, 1)
        return min(steps, 12)

    def classify_difficulty(self, steps: int) -> str:
        """ä¿®å¤çš„éš¾åº¦åˆ†çº§ - é€‚åº”çœŸå®GSM8Kåˆ†å¸ƒ"""
        if steps <= 4:
            return "simple"  # æ”¾å®½simpleæ ‡å‡†
        elif steps <= 8:
            return "medium"  # è°ƒæ•´mediumèŒƒå›´
        else:
            return "complex"  # 8+æ­¥ä¸ºcomplex

    def get_balanced_sample(self, n_total: int = 200, simple_ratio: float = 0.5) -> Tuple[List, List]:
        """è·å–å¹³è¡¡çš„æ ·æœ¬æ•°æ®ï¼ŒæŒ‰æ­¥éª¤æ•°åˆ†ç±»"""
        print(f"ğŸ¯ å‡†å¤‡é‡‡æ · {n_total} é“é¢˜ç›® (ç®€å•é¢˜æ¯”ä¾‹: {simple_ratio:.1%})")

        simple_problems = []
        complex_problems = []

        print("ğŸ“‹ æ­£åœ¨åˆ†æé—®é¢˜å¤æ‚åº¦...")
        for i, item in enumerate(self.samples):
            if i % 100 == 0 and i > 0:
                print(f"   å¤„ç†è¿›åº¦: {i}/{len(self.samples)}")

            question = item['question']
            answer_text = item['answer']
            answer = self.extract_answer(answer_text)
            steps = self.count_solution_steps(answer_text)
            difficulty = self.classify_difficulty(steps)

            problem_data = {
                'question': question,
                'answer': answer,
                'original_answer': answer_text,
                'steps': steps,
                'difficulty': difficulty
            }

            if difficulty == "simple":
                simple_problems.append(problem_data)
            else:  # medium å’Œ complex éƒ½å½’ä¸ºå¤æ‚é—®é¢˜ï¼Œç®€åŒ–ä¸ºäºŒåˆ†ç±»
                complex_problems.append(problem_data)

        print(f"âœ… åˆ†ç±»å®Œæˆ: {len(simple_problems)} ç®€å•é¢˜, {len(complex_problems)} å¤æ‚é¢˜")

        # è®¡ç®—é‡‡æ ·æ•°é‡
        n_simple = int(n_total * simple_ratio)
        n_complex = n_total - n_simple

        # æ£€æŸ¥æ•°æ®å……è¶³æ€§
        if len(simple_problems) < n_simple:
            print(f"âš ï¸ ç®€å•é¢˜ä¸è¶³: éœ€è¦{n_simple}, å®é™…{len(simple_problems)}")
            n_simple = len(simple_problems)

        if len(complex_problems) < n_complex:
            print(f"âš ï¸ å¤æ‚é¢˜ä¸è¶³: éœ€è¦{n_complex}, å®é™…{len(complex_problems)}")
            n_complex = len(complex_problems)

        # éšæœºé‡‡æ ·
        if n_simple > 0:
            sampled_simple = random.sample(simple_problems, n_simple)
        else:
            sampled_simple = []

        if n_complex > 0:
            sampled_complex = random.sample(complex_problems, n_complex)
        else:
            sampled_complex = []

        print(f"ğŸ² æœ€ç»ˆé‡‡æ ·: {len(sampled_simple)} ç®€å•é¢˜, {len(sampled_complex)} å¤æ‚é¢˜")

        return sampled_simple, sampled_complex


# ========================= å¤æ‚åº¦è·¯ç”±å™¨ï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰ =========================
class RobustAttentionAnalyzer:
    """ç¨³å¥çš„æ³¨æ„åŠ›åˆ†æå™¨ - ä½œä¸ºå¤æ‚åº¦è·¯ç”±å™¨ä½¿ç”¨"""

    def __init__(self, device, threshold=0.160):
        self.feature_weights = {
            'entropy': 0.5,
            'variance': 0.35,
            'concentration': 0.15
        }
        self.threshold = threshold
        self.device = device
        print(f"ğŸ¯ Attention Analyzer initialized - Threshold: {self.threshold}")

    def route(self, question: str, model, tokenizer) -> str:
        """è·¯ç”±å†³ç­–ï¼šè¿”å›'SLM'æˆ–'LLM'"""
        try:
            features = self.extract_core_features(question, model, tokenizer)
            prediction = self.predict_complexity(features)

            if prediction['is_complex']:
                return "LLM"
            else:
                return "SLM"
        except Exception as e:
            print(f"âš ï¸ Route decision failed: {e}, defaulting to LLM")
            return "LLM"  # å‡ºé”™æ—¶é»˜è®¤ç”¨å¤§æ¨¡å‹

    def extract_core_features(self, text: str, model, tokenizer) -> dict:
        """ç¨³å¥çš„ç‰¹å¾æå–"""
        # è·å–æ¨¡å‹è®¾å¤‡
        model_device = next(model.parameters()).device

        # ç¼–ç è¾“å…¥
        inputs = tokenizer(text, return_tensors="pt", max_length=200, truncation=True, padding=True)

        # ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
        inputs = {k: v.to(model_device) for k, v in inputs.items()}

        # è·å–æ³¨æ„åŠ›
        with torch.no_grad():
            outputs = model(**inputs, output_attentions=True)

        attentions = outputs.attentions[-1][0]
        seq_len = inputs['attention_mask'].sum().item()
        attentions = attentions.cpu()

        # è®¡ç®—ç‰¹å¾
        entropy_features = self._compute_entropy(attentions, seq_len)
        variance_features = self._compute_variance(attentions, seq_len)
        concentration_features = self._compute_concentration(attentions, seq_len)

        return {**entropy_features, **variance_features, **concentration_features}

    def _compute_entropy(self, attentions, seq_len):
        """è®¡ç®—ç†µç‰¹å¾"""
        all_entropies = []

        for head in range(attentions.shape[0]):
            for pos in range(seq_len):
                attn_dist = attentions[head, pos, :seq_len] + 1e-9
                entropy = -torch.sum(attn_dist * torch.log(attn_dist)).item()
                all_entropies.append(entropy)

        return {
            'avg_entropy': np.mean(all_entropies),
            'entropy_std': np.std(all_entropies),
            'max_entropy': np.max(all_entropies)
        }

    def _compute_variance(self, attentions, seq_len):
        """è®¡ç®—æ–¹å·®ç‰¹å¾"""
        variances = []

        for head in range(attentions.shape[0]):
            for pos in range(seq_len):
                attn_weights = attentions[head, pos, :seq_len]
                variance = torch.var(attn_weights).item()
                variances.append(variance)

        return {
            'avg_variance': np.mean(variances),
            'variance_std': np.std(variances),
            'max_variance': np.max(variances)
        }

    def _compute_concentration(self, attentions, seq_len):
        """è®¡ç®—é›†ä¸­åº¦ç‰¹å¾"""
        max_attentions = []

        for head in range(attentions.shape[0]):
            for pos in range(seq_len):
                attn_weights = attentions[head, pos, :seq_len]
                max_attn = torch.max(attn_weights).item()
                max_attentions.append(max_attn)

        return {
            'avg_max_attention': np.mean(max_attentions),
            'concentration_std': np.std(max_attentions)
        }

    def predict_complexity(self, features: dict) -> dict:
        """é¢„æµ‹å¤æ‚åº¦"""
        # æ ‡å‡†åŒ–ç‰¹å¾
        entropy_score = self._normalize(features['avg_entropy'], 0.8, 3.5)
        variance_score = self._normalize(features['avg_variance'], 0.0, 0.35)
        concentration_score = self._normalize(features['avg_max_attention'], 0.1, 0.9)

        # ç»¼åˆè¯„åˆ†
        complexity_score = (
                self.feature_weights['entropy'] * entropy_score +
                self.feature_weights['variance'] * variance_score +
                self.feature_weights['concentration'] * (1 - concentration_score)
        )

        complexity_score = np.clip(complexity_score, 0, 1)

        return {
            'complexity_score': complexity_score,
            'is_complex': complexity_score > self.threshold,
            'entropy_score': entropy_score,
            'variance_score': variance_score,
            'concentration_score': concentration_score,
            'raw_features': features
        }

    def _normalize(self, value, min_val, max_val):
        """æ ‡å‡†åŒ–ç‰¹å¾"""
        if max_val <= min_val:
            return 0.0
        normalized = (value - min_val) / (max_val - min_val)
        return np.clip(normalized, 0, 1)


# ========================= SLMæ¥å£ =========================
class SLMInterface(ModelInterface):
    """å°æ¨¡å‹æ¥å£"""

    def load_model(self):
        """åŠ è½½å°æ¨¡å‹"""
        print(f"ğŸ”„ Loading SLM: {self.config.name}")

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_path,
                trust_remote_code=True
            )

            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_path,
                torch_dtype=torch.float16,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True,
                output_attentions=True  # éœ€è¦æ³¨æ„åŠ›ç”¨äºè·¯ç”±å™¨
            )

            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            print(f"âœ… SLM loaded successfully")

        except Exception as e:
            print(f"âŒ Failed to load SLM: {e}")
            raise

    def predict(self, question: str) -> str:
        """SLMé¢„æµ‹"""
        if self.model is None:
            self.load_model()

        prompt = f"Question: {question}\nAnswer: Let me solve this step by step.\n"

        inputs = self.tokenizer.encode(prompt, return_tensors="pt", max_length=1024, truncation=True)
        if torch.cuda.is_available():
            inputs = inputs.to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_new_tokens=200,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id
            )

        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = full_response[len(prompt):].strip()
        return answer


# ========================= LLMæ¥å£ =========================
class EnhancedLLMInterface(ModelInterface):
    """å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥å£"""

    def __init__(self, config: ModelConfig, hf_token: str = None):
        super().__init__(config)
        self.hf_token = hf_token
        self.max_memory_per_gpu = "35GB"

    def setup_authentication(self):
        """è®¾ç½®HuggingFaceè®¤è¯"""
        if self.hf_token:
            login(token=self.hf_token)
        elif os.getenv('HUGGINGFACE_TOKEN'):
            login(token=os.getenv('HUGGINGFACE_TOKEN'))
        else:
            print("âš ï¸ No HuggingFace token provided. Trying without authentication...")

    def load_model(self):
        """åŠ è½½LLMæ¨¡å‹"""
        print(f"ğŸ”„ Loading LLM: {self.config.name}")
        self.setup_authentication()

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_path,
                trust_remote_code=True
            )

            model_kwargs = {
                "torch_dtype": torch.float16,
                "trust_remote_code": True,
                "low_cpu_mem_usage": True,
            }

            if "70B" in self.config.model_path:
                model_kwargs.update({
                    "device_map": "auto",
                    "max_memory": {0: self.max_memory_per_gpu},
                    "load_in_8bit": True,
                })
            elif "34B" in self.config.model_path or "30B" in self.config.model_path:
                model_kwargs.update({
                    "device_map": "auto",
                    "max_memory": {0: self.max_memory_per_gpu},
                })
            else:
                model_kwargs.update({
                    "device_map": "auto" if torch.cuda.is_available() else None,
                })

            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_path,
                **model_kwargs
            )

            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            print(f"âœ… LLM loaded successfully on {next(self.model.parameters()).device}")

        except Exception as e:
            print(f"âŒ Failed to load LLM: {e}")
            print("ğŸ’¡ Suggestions:")
            print("   1. Check if you have HuggingFace access to the model")
            print("   2. Verify your GPU memory is sufficient")
            print("   3. Try a smaller model variant")
            raise

    def predict(self, question: str) -> str:
        """LLMé¢„æµ‹ç­”æ¡ˆ"""
        if self.model is None:
            self.load_model()

        prompt = self._build_math_prompt(question)
        inputs = self.tokenizer.encode(prompt, return_tensors="pt", max_length=2048, truncation=True)
        inputs = inputs.to(next(self.model.parameters()).device)

        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_new_tokens=300,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1
            )

        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = full_response[len(prompt):].strip()
        return answer

    def _build_math_prompt(self, question: str) -> str:
        """æ„å»ºæ•°å­¦é—®é¢˜çš„æç¤º"""
        prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful math tutor. Solve the following math problem step by step and provide the final numerical answer at the end.

<|eot_id|><|start_header_id|>user<|end_header_id|>

{question}

<|eot_id|><|start_header_id|>assistant<|end_header_id|>

I'll solve this step by step.

"""
        return prompt


# ========================= å‡†ç¡®ç‡éªŒè¯å™¨ =========================
class AccuracyValidator:
    """å‡†ç¡®ç‡éªŒè¯å™¨"""

    @staticmethod
    def extract_final_answer(response: str) -> str:
        """ä»æ¨¡å‹å›ç­”ä¸­æå–æœ€ç»ˆæ•°å€¼ç­”æ¡ˆ"""
        patterns = [
            r'[Tt]he answer is\s*([+-]?\d+(?:\.\d+)?)',
            r'[Ff]inal answer:\s*([+-]?\d+(?:\.\d+)?)',
            r'[Aa]nswer:\s*([+-]?\d+(?:\.\d+)?)',
            r'=\s*([+-]?\d+(?:\.\d+)?)\s*$',
            r'####\s*([+-]?\d+(?:\.\d+)?)',
            r'([+-]?\d+(?:\.\d+)?)\s*$',
        ]

        for pattern in patterns:
            match = re.search(pattern, response)
            if match:
                return match.group(1).strip()

        numbers = re.findall(r'([+-]?\d+(?:\.\d+)?)', response)
        if numbers:
            return numbers[-1]

        return "No answer found"

    @staticmethod
    def is_correct(predicted: str, ground_truth: str, tolerance: float = 0.01) -> bool:
        """åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦æ­£ç¡®"""
        try:
            pred_num = float(predicted)
            true_num = float(ground_truth)

            if abs(pred_num - true_num) <= tolerance:
                return True

            if true_num != 0:
                relative_error = abs(pred_num - true_num) / abs(true_num)
                return relative_error <= 0.01

            return False
        except (ValueError, TypeError):
            return str(predicted).strip().lower() == str(ground_truth).strip().lower()


# ========================= ä¸»è¯„ä¼°å™¨ =========================
class GSM8KAccuracyEvaluator:
    """åŸºäºGSM8Kçš„å‡†ç¡®ç‡è¯„ä¼°å™¨"""

    def __init__(self, hf_token=None, max_samples=1000):
        self.hf_token = hf_token
        self.validator = AccuracyValidator()
        self.data_processor = FixedGSM8KProcessor(max_samples=max_samples)

        # é…ç½®æ¨¡å‹
        self.slm_config = ModelConfig(
            name="Llama-3.2-3B",
            model_path="meta-llama/Llama-3.2-3B",
            cost_per_token=0.001,
            avg_latency_ms=100
        )

        self.llm_config = ModelConfig(
            name="Llama-3.1-8B-Instruct",
            model_path="meta-llama/Llama-3.1-8B-Instruct",
            cost_per_token=0.003,
            avg_latency_ms=800
        )

        # åˆå§‹åŒ–æ¨¡å‹æ¥å£
        self.slm = SLMInterface(self.slm_config)
        self.llm = EnhancedLLMInterface(self.llm_config, hf_token)

        # åˆå§‹åŒ–è·¯ç”±å™¨ - ç­‰SLMåŠ è½½åå†åˆå§‹åŒ–
        self.router = None

        print(f"ğŸ¯ SLM: {self.slm_config.name}")
        print(f"ğŸ¯ LLM: {self.llm_config.name}")
        print(f"ğŸ“Š æˆæœ¬æ¯”ä¾‹: 1:{self.llm_config.cost_per_token / self.slm_config.cost_per_token:.1f}")

    def _ensure_slm_loaded(self):
        """ç¡®ä¿SLMå·²åŠ è½½ï¼ˆè·¯ç”±å™¨éœ€è¦ç”¨åˆ°ï¼‰"""
        if self.slm.model is None:
            self.slm.load_model()

        if self.router is None:
            self.router = RobustAttentionAnalyzer(device, threshold=0.16)
            print("âœ… æ³¨æ„åŠ›è·¯ç”±å™¨å·²åˆå§‹åŒ–")

    def evaluate_model_on_problems(self, model_interface, problems: List[Dict],
                                   model_name: str, max_problems: Optional[int] = None) -> Dict:
        """åœ¨æŒ‡å®šé—®é¢˜é›†åˆä¸Šè¯„ä¼°æ¨¡å‹"""
        print(f"\nğŸ” è¯„ä¼° {model_name}...")

        if max_problems and len(problems) > max_problems:
            problems = random.sample(problems, max_problems)
            print(f"   éšæœºé€‰æ‹© {max_problems} é“é¢˜è¿›è¡Œæµ‹è¯•")

        correct_count = 0
        total_count = len(problems)
        detailed_results = []
        error_cases = []

        for i, problem in enumerate(problems):
            question = problem['question']
            ground_truth = problem['answer']

            print(f"   å¤„ç†é—®é¢˜ {i + 1}/{total_count}...")

            try:
                # è·å–æ¨¡å‹é¢„æµ‹
                response = model_interface.predict(question)
                predicted_answer = self.validator.extract_final_answer(response)

                # éªŒè¯å‡†ç¡®æ€§
                is_correct = self.validator.is_correct(predicted_answer, ground_truth)

                if is_correct:
                    correct_count += 1
                else:
                    # è®°å½•é”™è¯¯æ¡ˆä¾‹
                    error_cases.append({
                        'question': question[:100] + "..." if len(question) > 100 else question,
                        'ground_truth': ground_truth,
                        'predicted': predicted_answer,
                        'full_response': response[:200] + "..." if len(response) > 200 else response
                    })

                detailed_results.append({
                    'question': question,
                    'ground_truth': ground_truth,
                    'model_response': response,
                    'extracted_answer': predicted_answer,
                    'is_correct': is_correct
                })

                # å®æ—¶æ˜¾ç¤ºé”™è¯¯ï¼ˆä½†ä¸è¦å¤ªé¢‘ç¹ï¼‰
                if not is_correct and len(error_cases) <= 3:
                    print(f"   âŒ é”™è¯¯: é¢„æµ‹={predicted_answer}, æ­£ç¡®={ground_truth}")

            except Exception as e:
                print(f"   âš ï¸ å¤„ç†é”™è¯¯: {e}")
                detailed_results.append({
                    'question': question,
                    'ground_truth': ground_truth,
                    'model_response': f"ERROR: {e}",
                    'extracted_answer': "ERROR",
                    'is_correct': False
                })

        accuracy = correct_count / total_count if total_count > 0 else 0

        result = {
            'model_name': model_name,
            'accuracy': accuracy,
            'correct_count': correct_count,
            'total_count': total_count,
            'error_rate': 1 - accuracy,
            'error_cases': error_cases[:10],
            'detailed_results': detailed_results
        }

        print(f"âœ… {model_name} å‡†ç¡®ç‡: {accuracy:.2%} ({correct_count}/{total_count})")
        if len(error_cases) > 0:
            print(f"   é”™è¯¯æ¡ˆä¾‹æ•°: {len(error_cases)}")

        return result

    def evaluate_routing_accuracy(self, simple_problems: List, complex_problems: List) -> Dict:
        """è¯„ä¼°è·¯ç”±å‡†ç¡®æ€§"""
        print(f"\nğŸ§­ è¯„ä¼°è·¯ç”±å‡†ç¡®æ€§...")

        # ç¡®ä¿SLMå·²åŠ è½½ï¼ˆè·¯ç”±å™¨éœ€è¦ä½¿ç”¨ï¼‰
        self._ensure_slm_loaded()

        correct_routes = 0
        total_routes = 0
        routing_details = []

        # æµ‹è¯•ç®€å•é—®é¢˜çš„è·¯ç”±ï¼ˆåº”è¯¥è·¯ç”±åˆ°SLMï¼‰
        test_simple = simple_problems[:min(50, len(simple_problems))]
        for problem in test_simple:
            question = problem['question']
            try:
                predicted_route = self.router.route(question, self.slm.model, self.slm.tokenizer)
                expected_route = "SLM"

                is_correct = (predicted_route == expected_route)
                if is_correct:
                    correct_routes += 1

                routing_details.append({
                    'question': question[:100] + "..." if len(question) > 100 else question,
                    'true_complexity': 'simple',
                    'expected': expected_route,
                    'predicted': predicted_route,
                    'correct': is_correct
                })
                total_routes += 1
            except Exception as e:
                print(f"   âš ï¸ è·¯ç”±é”™è¯¯: {e}")
                continue

        # æµ‹è¯•å¤æ‚é—®é¢˜çš„è·¯ç”±ï¼ˆåº”è¯¥è·¯ç”±åˆ°LLMï¼‰
        test_complex = complex_problems[:min(50, len(complex_problems))]
        for problem in test_complex:
            question = problem['question']
            try:
                predicted_route = self.router.route(question, self.slm.model, self.slm.tokenizer)
                expected_route = "LLM"

                is_correct = (predicted_route == expected_route)
                if is_correct:
                    correct_routes += 1

                routing_details.append({
                    'question': question[:100] + "..." if len(question) > 100 else question,
                    'true_complexity': 'complex',
                    'expected': expected_route,
                    'predicted': predicted_route,
                    'correct': is_correct
                })
                total_routes += 1
            except Exception as e:
                print(f"   âš ï¸ è·¯ç”±é”™è¯¯: {e}")
                continue

        routing_accuracy = correct_routes / total_routes if total_routes > 0 else 0

        print(f"âœ… è·¯ç”±å‡†ç¡®ç‡: {routing_accuracy:.2%} ({correct_routes}/{total_routes})")

        return {
            'routing_accuracy': routing_accuracy,
            'correct_routes': correct_routes,
            'total_routes': total_routes,
            'routing_details': routing_details
        }

    def run_gsm8k_evaluation(self, n_samples: int = 200, simple_ratio: float = 0.5):
        """è¿è¡ŒGSM8Kè¯„ä¼°"""
        print("ğŸš€ å¼€å§‹åŸºäºGSM8Kçš„å‡†ç¡®ç‡è¯„ä¼°")
        print("=" * 60)

        # 1. åŠ è½½å’Œé‡‡æ ·æ•°æ®
        print(f"ğŸ“Š å‡†å¤‡GSM8Kæ•°æ® (æ ·æœ¬æ•°: {n_samples})...")
        try:
            simple_problems, complex_problems = self.data_processor.get_balanced_sample(
                n_total=n_samples, simple_ratio=simple_ratio
            )
        except Exception as e:
            print(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            return None

        if len(simple_problems) == 0 and len(complex_problems) == 0:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®")
            return None

        # 2. è¯„ä¼°SLMåœ¨ç®€å•é—®é¢˜ä¸Šçš„è¡¨ç°
        if len(simple_problems) > 0:
            slm_simple_results = self.evaluate_model_on_problems(
                self.slm, simple_problems, "SLM on Simple Problems"
            )
        else:
            slm_simple_results = {
                'model_name': 'SLM on Simple Problems',
                'accuracy': 0, 'correct_count': 0, 'total_count': 0,
                'error_cases': [], 'detailed_results': []
            }

        # 3. è¯„ä¼°LLMåœ¨å¤æ‚é—®é¢˜ä¸Šçš„è¡¨ç°
        if len(complex_problems) > 0:
            llm_complex_results = self.evaluate_model_on_problems(
                self.llm, complex_problems, "LLM on Complex Problems"
            )
        else:
            llm_complex_results = {
                'model_name': 'LLM on Complex Problems',
                'accuracy': 0, 'correct_count': 0, 'total_count': 0,
                'error_cases': [], 'detailed_results': []
            }

        # 4. äº¤å‰éªŒè¯ï¼šSLMåœ¨å¤æ‚é—®é¢˜ä¸Šçš„è¡¨ç°ï¼ˆéªŒè¯å…¶å±€é™æ€§ï¼‰
        if len(complex_problems) > 0:
            slm_complex_results = self.evaluate_model_on_problems(
                self.slm, complex_problems[:min(20, len(complex_problems))],
                "SLM on Complex Problems (éªŒè¯å±€é™æ€§)"
            )
        else:
            slm_complex_results = {
                'model_name': 'SLM on Complex Problems',
                'accuracy': 0, 'correct_count': 0, 'total_count': 0,
                'error_cases': [], 'detailed_results': []
            }

        # 5. äº¤å‰éªŒè¯ï¼šLLMåœ¨ç®€å•é—®é¢˜ä¸Šçš„è¡¨ç°ï¼ˆéªŒè¯è¿‡åº¦é…ç½®ï¼‰
        if len(simple_problems) > 0:
            llm_simple_results = self.evaluate_model_on_problems(
                self.llm, simple_problems[:min(20, len(simple_problems))],
                "LLM on Simple Problems (éªŒè¯è¿‡åº¦é…ç½®)"
            )
        else:
            llm_simple_results = {
                'model_name': 'LLM on Simple Problems',
                'accuracy': 0, 'correct_count': 0, 'total_count': 0,
                'error_cases': [], 'detailed_results': []
            }

        # 6. è¯„ä¼°è·¯ç”±å‡†ç¡®æ€§
        if len(simple_problems) > 0 or len(complex_problems) > 0:
            routing_results = self.evaluate_routing_accuracy(simple_problems, complex_problems)
        else:
            routing_results = {
                'routing_accuracy': 0, 'correct_routes': 0, 'total_routes': 0,
                'routing_details': []
            }

        # 7. è®¡ç®—æ™ºèƒ½è·¯ç”±ç³»ç»Ÿæ€§èƒ½
        smart_routing_results = self._calculate_smart_routing_performance(
            slm_simple_results, llm_complex_results, routing_results, n_samples
        )

        # 8. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        final_report = self._generate_final_report(
            slm_simple_results, llm_complex_results, slm_complex_results,
            llm_simple_results, routing_results, smart_routing_results, n_samples
        )

        return final_report

    def _calculate_smart_routing_performance(self, slm_results, llm_results, routing_results, n_samples):
        """è®¡ç®—æ™ºèƒ½è·¯ç”±ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½"""
        routing_acc = routing_results['routing_accuracy']
        slm_acc = slm_results['accuracy']
        llm_acc = llm_results['accuracy']

        # ä¼°ç®—æ™ºèƒ½è·¯ç”±çš„å‡†ç¡®ç‡
        if slm_results['total_count'] > 0 and llm_results['total_count'] > 0:
            expected_accuracy = (slm_acc + llm_acc) / 2
        elif slm_results['total_count'] > 0:
            expected_accuracy = slm_acc
        elif llm_results['total_count'] > 0:
            expected_accuracy = llm_acc
        else:
            expected_accuracy = 0.5

        # è€ƒè™‘è·¯ç”±é”™è¯¯çš„å½±å“
        estimated_accuracy = expected_accuracy * routing_acc + (1 - routing_acc) * 0.5

        # æˆæœ¬è®¡ç®—
        simple_ratio = slm_results['total_count'] / max(1, slm_results['total_count'] + llm_results['total_count'])

        smart_routing_cost = (simple_ratio * self.slm_config.cost_per_token +
                              (1 - simple_ratio) * self.llm_config.cost_per_token)
        pure_llm_cost = self.llm_config.cost_per_token
        pure_slm_cost = self.slm_config.cost_per_token

        cost_savings_vs_llm = (pure_llm_cost - smart_routing_cost) / pure_llm_cost
        cost_increase_vs_slm = (smart_routing_cost - pure_slm_cost) / pure_slm_cost

        return {
            'estimated_accuracy': estimated_accuracy,
            'cost_per_problem': smart_routing_cost,
            'cost_savings_vs_llm': cost_savings_vs_llm,
            'cost_increase_vs_slm': cost_increase_vs_slm,
            'baseline_accuracy': expected_accuracy
        }

    def _generate_final_report(self, slm_simple, llm_complex, slm_complex,
                               llm_simple, routing, smart_routing, n_samples):
        """ç”Ÿæˆæœ€ç»ˆè¯„ä¼°æŠ¥å‘Š"""

        print("\n" + "=" * 70)
        print("ğŸ“‹ GSM8Kæ•°æ®é›†æ™ºèƒ½è·¯ç”±å‡†ç¡®ç‡è¯„ä¼°æŠ¥å‘Š")
        print("=" * 70)
        print(f"ğŸ“Š è¯„ä¼°è§„æ¨¡: {n_samples} é“GSM8KçœŸé¢˜")

        print(f"\nğŸ¯ æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡:")
        print(
            f"â”œâ”€â”€ SLMåœ¨ç®€å•é—®é¢˜å‡†ç¡®ç‡: {slm_simple['accuracy']:.2%} ({slm_simple['correct_count']}/{slm_simple['total_count']})")
        print(
            f"â”œâ”€â”€ LLMåœ¨å¤æ‚é—®é¢˜å‡†ç¡®ç‡: {llm_complex['accuracy']:.2%} ({llm_complex['correct_count']}/{llm_complex['total_count']})")
        print(
            f"â”œâ”€â”€ è·¯ç”±åˆ¤æ–­å‡†ç¡®ç‡: {routing['routing_accuracy']:.2%} ({routing['correct_routes']}/{routing['total_routes']})")
        print(f"â””â”€â”€ æ™ºèƒ½è·¯ç”±é¢„ä¼°å‡†ç¡®ç‡: {smart_routing['estimated_accuracy']:.2%}")

        print(f"\nğŸ“Š äº¤å‰éªŒè¯ç»“æœ:")
        print(f"â”œâ”€â”€ SLMåœ¨å¤æ‚é—®é¢˜å‡†ç¡®ç‡: {slm_complex['accuracy']:.2%} (è¯æ˜å±€é™æ€§)")
        print(f"â”œâ”€â”€ LLMåœ¨ç®€å•é—®é¢˜å‡†ç¡®ç‡: {llm_simple['accuracy']:.2%} (è¯æ˜è¿‡åº¦é…ç½®)")

        if slm_simple['total_count'] > 0 and llm_simple['total_count'] > 0:
            capability_gap = llm_simple['accuracy'] - slm_simple['accuracy']
            print(f"â””â”€â”€ èƒ½åŠ›å·®è·: {capability_gap:.2%}")

        print(f"\nğŸ’° æˆæœ¬æ•ˆç›Šåˆ†æ:")
        print(f"â”œâ”€â”€ çº¯SLMæˆæœ¬: ${self.slm_config.cost_per_token:.4f}/é—®é¢˜")
        print(f"â”œâ”€â”€ çº¯LLMæˆæœ¬: ${self.llm_config.cost_per_token:.4f}/é—®é¢˜")
        print(f"â”œâ”€â”€ æ™ºèƒ½è·¯ç”±æˆæœ¬: ${smart_routing['cost_per_problem']:.4f}/é—®é¢˜")
        print(f"â”œâ”€â”€ vs LLMèŠ‚çœ: {smart_routing['cost_savings_vs_llm']:.1%}")
        print(f"â””â”€â”€ vs SLMå¢åŠ : {smart_routing['cost_increase_vs_slm']:.1%}")

        # å…³é”®åˆ¤æ–­
        slm_reliable = slm_simple['accuracy'] >= 0.80
        routing_reliable = routing['routing_accuracy'] >= 0.85
        cost_effective = smart_routing['cost_savings_vs_llm'] >= 0.30

        print(f"\nğŸ’¡ å…³é”®åˆ¤æ–­:")
        print(
            f"â”œâ”€â”€ SLMå¯é æ€§: {'âœ… å¯é ' if slm_reliable else 'âŒ ä¸å¯é '} ({slm_simple['accuracy']:.1%} {'â‰¥' if slm_reliable else '<'} 80%)")
        print(
            f"â”œâ”€â”€ è·¯ç”±å¯é æ€§: {'âœ… å¯é ' if routing_reliable else 'âŒ ä¸å¯é '} ({routing['routing_accuracy']:.1%} {'â‰¥' if routing_reliable else '<'} 85%)")
        print(
            f"â””â”€â”€ æˆæœ¬æ•ˆç›Š: {'âœ… æ˜¾è‘—' if cost_effective else 'âŒ æœ‰é™'} ({smart_routing['cost_savings_vs_llm']:.1%} {'â‰¥' if cost_effective else '<'} 30%)")

        # æœ€ç»ˆå»ºè®®
        if slm_reliable and routing_reliable and cost_effective:
            recommendation = "âœ… å¼ºçƒˆæ¨èä½¿ç”¨æ™ºèƒ½è·¯ç”±ç³»ç»Ÿ"
            reason = "SLMå¯é ã€è·¯ç”±å‡†ç¡®ã€æˆæœ¬æ•ˆç›Šæ˜¾è‘—"
        elif not slm_reliable:
            recommendation = "âŒ ä¸æ¨èä½¿ç”¨ - SLMä¸å¯é "
            reason = f"SLMåœ¨ç®€å•é—®é¢˜ä¸Šå‡†ç¡®ç‡ä»…{slm_simple['accuracy']:.1%}ï¼Œå­˜åœ¨è¿‡åº¦è‡ªä¿¡é£é™©"
        elif not routing_reliable:
            recommendation = "âŒ ä¸æ¨èä½¿ç”¨ - è·¯ç”±ä¸å‡†ç¡®"
            reason = f"è·¯ç”±åˆ¤æ–­å‡†ç¡®ç‡ä»…{routing['routing_accuracy']:.1%}ï¼Œä¼šå¯¼è‡´é”™è¯¯åˆ†é…"
        else:
            recommendation = "âš ï¸ è°¨æ…è€ƒè™‘ - æˆæœ¬æ•ˆç›Šæœ‰é™"
            reason = f"è™½ç„¶ç³»ç»Ÿå¯é ï¼Œä½†æˆæœ¬èŠ‚çœä»…{smart_routing['cost_savings_vs_llm']:.1%}"

        print(f"\nğŸ¯ æœ€ç»ˆå»ºè®®:")
        print(f"â””â”€â”€ {recommendation}")
        print(f"    ç†ç”±: {reason}")

        # æ˜¾ç¤ºå…³é”®é”™è¯¯æ¡ˆä¾‹
        if len(slm_simple['error_cases']) > 0:
            print(f"\nâŒ SLMé”™è¯¯æ¡ˆä¾‹åˆ†æ (å‰3ä¸ª):")
            for i, case in enumerate(slm_simple['error_cases'][:3]):
                print(f"   {i + 1}. é—®é¢˜: {case['question']}")
                print(f"      é¢„æµ‹: {case['predicted']}, æ­£ç¡®: {case['ground_truth']}")

        # ä¿å­˜ç»“æœ
        timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
        filename = f"gsm8k_routing_evaluation_{timestamp}.csv"

        # åˆ›å»ºè¯¦ç»†ç»“æœæ•°æ®æ¡†
        all_results = []
        for result in slm_simple['detailed_results']:
            all_results.append({
                'model': 'SLM',
                'problem_type': 'simple',
                **result
            })
        for result in llm_complex['detailed_results']:
            all_results.append({
                'model': 'LLM',
                'problem_type': 'complex',
                **result
            })

        if all_results:
            results_df = pd.DataFrame(all_results)
            results_df.to_csv(filename, index=False)
            print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {filename}")

        return {
            'dataset_info': {'name': 'GSM8K', 'sample_size': n_samples},
            'slm_simple_performance': slm_simple,
            'llm_complex_performance': llm_complex,
            'slm_complex_performance': slm_complex,
            'llm_simple_performance': llm_simple,
            'routing_performance': routing,
            'smart_routing_performance': smart_routing,
            'evaluation_summary': {
                'slm_reliable': slm_reliable,
                'routing_reliable': routing_reliable,
                'cost_effective': cost_effective,
                'recommendation': recommendation,
                'reason': reason
            }
        }


# ========================= ä¸»å‡½æ•°å’Œå·¥å…·å‡½æ•° =========================
def get_secure_token():
    """å®‰å…¨è·å–token"""
    hf_token = os.getenv('HUGGINGFACE_TOKEN')
    if hf_token:
        print("âœ… ä»ç¯å¢ƒå˜é‡è·å–token")
        return hf_token

    print("ğŸ”‘ è¯·è¾“å…¥ä½ çš„HuggingFace Token:")
    return getpass.getpass("Token: ")


def test_model_access(hf_token):
    """æµ‹è¯•tokenæ˜¯å¦å¯ä»¥è®¿é—®æŒ‡å®šçš„æ¨¡å‹"""
    try:
        login(token=hf_token)
        print("âœ… HuggingFace token valid")

        # æµ‹è¯•Llama-3.2-3Bè®¿é—®
        print("ğŸ”„ Testing Llama-3.2-3B access...")
        llama32_tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-3B")
        print("âœ… Llama-3.2-3B access successful")

        # æµ‹è¯•Llama-3.1-8Bè®¿é—®
        print("ğŸ”„ Testing Llama-3.1-8B access...")
        llama_tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")
        print("âœ… Llama-3.1-8B access successful")

        return True

    except Exception as e:
        print(f"âŒ Model access test failed: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ GSM8Kæ™ºèƒ½è·¯ç”±å‡†ç¡®ç‡è¯„ä¼°ç³»ç»Ÿ")
    print("ğŸ¯ æ ¸å¿ƒç›®æ ‡: éªŒè¯SLM/LLMçš„çœŸå®å¯é æ€§")
    print("=" * 50)

    # è·å–token
    hf_token = get_secure_token()
    if not hf_token:
        print("âŒ æ— æ³•è·å–token")
        return None

    try:
        # å¯é€‰ï¼šæµ‹è¯•æ¨¡å‹è®¿é—®
        print("\nğŸ” æµ‹è¯•æ¨¡å‹è®¿é—®æƒé™...")
        if not test_model_access(hf_token):
            print("âš ï¸ æ¨¡å‹è®¿é—®æµ‹è¯•å¤±è´¥ï¼Œä½†å°†ç»§ç»­å°è¯•å®éªŒ...")

        # åˆ›å»ºè¯„ä¼°å™¨
        print("\nğŸš€ åˆå§‹åŒ–è¯„ä¼°å™¨...")
        evaluator = GSM8KAccuracyEvaluator(hf_token=hf_token, max_samples=1000)

        # è¿è¡Œå‡†ç¡®ç‡è¯„ä¼° - ä»å°æ ·æœ¬å¼€å§‹
        print("\nğŸ§ª å¼€å§‹å‡†ç¡®ç‡éªŒè¯å®éªŒ...")
        print("ğŸ“‹ å®éªŒå‚æ•°:")
        print("   â€¢ æ ·æœ¬æ•°é‡: 200é“é¢˜")
        print("   â€¢ ç®€å•é¢˜æ¯”ä¾‹: 50%")
        print("   â€¢ è¯„ä¼°é‡ç‚¹: SLMå¯é æ€§éªŒè¯")

        results = evaluator.run_gsm8k_evaluation(n_samples=200, simple_ratio=0.5)

        if results:
            # è¾“å‡ºæœ€ç»ˆè¯„ä¼°ç»“è®º
            print("\nğŸ¯ æœ€ç»ˆè¯„ä¼°ç»“è®º:")
            summary = results['evaluation_summary']
            print(f"å»ºè®®: {summary['recommendation']}")
            print(f"ç†ç”±: {summary['reason']}")

            # å¦‚æœç»“æœè‰¯å¥½ï¼Œæç¤ºå¯ä»¥æ‰©å¤§è§„æ¨¡
            if summary['slm_reliable'] and summary['routing_reliable']:
                print("\nğŸš€ ç³»ç»Ÿè¡¨ç°è‰¯å¥½ï¼å»ºè®®:")
                print("   1. å¢åŠ æ ·æœ¬æ•°é‡åˆ°500-1000è¿›è¡Œæ›´å…¨é¢éªŒè¯")
                print("   2. è°ƒæ•´è·¯ç”±é˜ˆå€¼ä»¥ä¼˜åŒ–æ€§èƒ½")
                print("   3. è€ƒè™‘åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²")

        return results

    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None
    finally:
        hf_token = None  # æ¸…ç†token


if __name__ == "__main__":
    results = main()