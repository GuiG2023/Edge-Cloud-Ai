# common_utils.py

import torch
import numpy as np
import pandas as pd
from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM
import re
import json
import os
import warnings
from typing import Dict, List, Tuple, Optional
import random
from torch import nn

# GPUè®¾ç½®
if torch.cuda.is_available():
    device = torch.device('cuda')
    print(f"ğŸš€ Using GPU: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device('cpu')
    print("ğŸ’» Using CPU")

# ========================= åŸºç¡€é…ç½®ç±» =========================
class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»"""
    def __init__(self, name: str, model_path: str, cost_per_token: float, avg_latency_ms: int):
        self.name = name
        self.model_path = model_path
        self.cost_per_token = cost_per_token
        self.avg_latency_ms = avg_latency_ms

class ModelInterface:
    """æ¨¡å‹æ¥å£åŸºç±»"""
    def __init__(self, config: ModelConfig):
        self.config = config
        self.model = None
        self.tokenizer = None

    def predict(self, question: str) -> str:
        raise NotImplementedError

# ========================= GSM8Kæ•°æ®å¤„ç†å™¨ =========================
class FixedGSM8KProcessor:
    """ä¿®å¤ç‰ˆGSM8Kæ•°æ®å¤„ç†å™¨"""
    def __init__(self, data_path="gsm8k_data/train.jsonl", max_samples=1000):
        print(f"ğŸ“š Loading GSM8K dataset...")
        self.data_path = data_path
        self.max_samples = max_samples
        self.samples = []
        if self._load_from_datasets():
            print(f"âœ… Loaded {len(self.samples)} samples from datasets library")
        elif self._load_from_local():
            print(f"âœ… Loaded {len(self.samples)} samples from local file")
        else:
            print("âŒ Critical Error: Could not load GSM8K data from any source.")

    def _load_from_datasets(self):
        try:
            from datasets import load_dataset
            print("ğŸ”„ Loading from HuggingFace datasets...")
            dataset = load_dataset("gsm8k", "main")
            # ä½¿ç”¨è®­ç»ƒé›†ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œæµ‹è¯•é›†ç”¨äºæœ€ç»ˆè¯„ä¼°
            data_split = dataset['train']
            for i in range(min(self.max_samples, len(data_split))):
                self.samples.append({'question': data_split[i]['question'], 'answer': data_split[i]['answer']})
            return len(self.samples) > 0
        except Exception as e:
            print(f"âš ï¸ Failed to load from datasets: {e}"); return False

    def _load_from_local(self):
        try:
            if not os.path.exists(self.data_path): return False
            with open(self.data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        try:
                            self.samples.append(json.loads(line))
                            if len(self.samples) >= self.max_samples: break
                        except: continue
            return len(self.samples) > 0
        except: return False

    def extract_answer(self, answer_text: str) -> str:
        match = re.search(r'####\s*([+-]?[\d,]+(?:\.\d+)?)', answer_text)
        if match: return match.group(1).replace(',', '')
        numbers = re.findall(r'([+-]?[\d,]+(?:\.\d+)?)', answer_text)
        return numbers[-1].replace(',', '') if numbers else "No answer found"

    def count_solution_steps(self, answer: str) -> int:
        lines = [line.strip() for line in answer.split('\n') if line.strip()]
        meaningful_lines = [line for line in lines if not line.startswith('####')]
        math_operations = len(re.findall(r'\d+\s*[+\-Ã—Ã·*/]\s*\d+', answer))
        equals_count = answer.count('=')
        return max(len(meaningful_lines) - 1, math_operations, equals_count, 1)

    def classify_difficulty(self, steps: int) -> str:
        if steps <= 4: return "simple"
        elif steps <= 8: return "medium"
        else: return "complex"

    def get_balanced_sample(self, n_total: int = 200, simple_ratio: float = 0.5) -> Tuple[List, List]:
        print(f"ğŸ¯ å‡†å¤‡é‡‡æ · {n_total} é“é¢˜ç›® (ç®€å•é¢˜æ¯”ä¾‹: {simple_ratio:.1%})")
        simple_problems, complex_problems = [], []
        print("ğŸ“‹ æ­£åœ¨åˆ†æé—®é¢˜å¤æ‚åº¦...")
        for i, item in enumerate(self.samples):
            steps = self.count_solution_steps(item['answer'])
            problem_data = {'question': item['question'], 'answer': self.extract_answer(item['answer']), 'difficulty': self.classify_difficulty(steps)}
            if problem_data['difficulty'] == "simple": simple_problems.append(problem_data)
            else: complex_problems.append(problem_data)
        print(f"âœ… åˆ†ç±»å®Œæˆ: {len(simple_problems)} ç®€å•é¢˜, {len(complex_problems)} å¤æ‚é¢˜")
        n_simple = int(n_total * simple_ratio)
        n_complex = n_total - n_simple
        sampled_simple = random.sample(simple_problems, min(n_simple, len(simple_problems)))
        sampled_complex = random.sample(complex_problems, min(n_complex, len(complex_problems)))
        print(f"ğŸ² æœ€ç»ˆé‡‡æ ·: {len(sampled_simple)} ç®€å•é¢˜, {len(sampled_complex)} å¤æ‚é¢˜")
        return sampled_simple, sampled_complex

# ========================= å¯è®­ç»ƒçš„å¤æ‚åº¦é¢„æµ‹ç½‘ç»œ =========================
class ComplexityPredictorNet(nn.Module):
    """ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œï¼Œç”¨äºä»æ³¨æ„åŠ›ç‰¹å¾ä¸­å­¦ä¹ å¹¶é¢„æµ‹ä»»åŠ¡å¤æ‚åº¦ã€‚"""
    def __init__(self, input_features: int = 8):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_features, 64), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(64, 32), nn.ReLU(),
            nn.Linear(32, 1)
        )
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)

# ========================= å¯å­¦ä¹ çš„æ™ºèƒ½è·¯ç”±å™¨ =========================
class LearnedAttentionRouter:
    """ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œæ¥ä»£æ›¿å›ºå®šè§„åˆ™ï¼Œè¿›è¡Œè·¯ç”±å†³ç­–ã€‚"""
    def __init__(self, model_path: str, device, threshold=0.5):
        self.device = device
        self.threshold = threshold
        self.model_path = model_path
        print(f"ğŸ§  Initializing LearnedAttentionRouter...")
        self.predictor_net = ComplexityPredictorNet(input_features=8).to(self.device)
        if os.path.exists(self.model_path):
            print(f"   Loading learned predictor from: {self.model_path}")
            self.predictor_net.load_state_dict(torch.load(self.model_path, map_location=self.device))
            self.predictor_net.eval()
            print("   âœ… Learned predictor loaded successfully.")
        else:
            print(f"   âš ï¸ Model file {self.model_path} not found! Router will be untrained.")
            print("   ğŸ’¡ Run 'train_router.py' first to create this file.")

    def route(self, question: str, slm_model, slm_tokenizer) -> Tuple[str, float]:
        try:
            features = self.extract_core_features(question, slm_model, slm_tokenizer)
            prediction = self.predict_complexity(features)
            route_decision = "LLM" if prediction['is_complex'] else "SLM"
            return route_decision, prediction['complexity_score']
        except Exception as e:
            print(f"âš ï¸ Route decision failed: {e}, defaulting to LLM")
            return "LLM", 1.0

    def predict_complexity(self, features: dict) -> dict:
        feature_vector = torch.tensor([
            features['avg_entropy'], features['entropy_std'], features['max_entropy'],
            features['avg_variance'], features['variance_std'], features['max_variance'],
            features['avg_max_attention'], features['concentration_std']
        ], dtype=torch.float32).to(self.device)
        with torch.no_grad():
            logit = self.predictor_net(feature_vector.unsqueeze(0))
            probability = torch.sigmoid(logit).item()
        return {'complexity_score': probability, 'is_complex': probability > self.threshold}

    def extract_core_features(self, text: str, model, tokenizer) -> dict:
        model_device = next(model.parameters()).device
        inputs = tokenizer(text, return_tensors="pt", max_length=200, truncation=True, padding=True)
        inputs = {k: v.to(model_device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs, output_attentions=True)
        attentions = outputs.attentions[-1][0].cpu()
        seq_len = inputs['attention_mask'].sum().item()
        entropy_features = self._compute_entropy(attentions, seq_len)
        variance_features = self._compute_variance(attentions, seq_len)
        concentration_features = self._compute_concentration(attentions, seq_len)
        return {**entropy_features, **variance_features, **concentration_features}

    def _compute_entropy(self, attentions, seq_len):
        """
        ã€ã€ã€æ•°å€¼ç¨³å®šç‰ˆã€‘ã€‘ã€‘çš„ç†µè®¡ç®—å‡½æ•°
        """
        all_entropies = []
        for head in range(attentions.shape[0]):
            for pos in range(seq_len):
                # é€‰å–å½“å‰ä½ç½®æœ‰æ•ˆçš„æ³¨æ„åŠ›åˆ†å¸ƒ
                attn_dist = attentions[head, pos, :seq_len]

                # --- æ ¸å¿ƒä¿®å¤å¼€å§‹ ---
                # 1. ä¸ºé˜²æ­¢é™¤ä»¥0ï¼Œåœ¨åˆ†æ¯ä¸Šå¢åŠ ä¸€ä¸ªæå°å€¼epsilon
                #    è¿™ä¸€æ­¥ç¡®ä¿attn_distæ˜¯ä¸€ä¸ªåˆæ³•çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆæ€»å’Œä¸º1ï¼‰
                attn_dist_normalized = attn_dist / (torch.sum(attn_dist) + 1e-9)

                # 2. åœ¨å–å¯¹æ•°å‰ï¼Œä¹Ÿå¢åŠ ä¸€ä¸ªæå°å€¼epsilonï¼Œé˜²æ­¢log(0)å¯¼è‡´NaN
                entropy = -torch.sum(attn_dist_normalized * torch.log(attn_dist_normalized + 1e-9)).item()
                # --- æ ¸å¿ƒä¿®å¤ç»“æŸ ---

                all_entropies.append(entropy)

        # ä½œä¸ºæœ€åçš„ä¿é™©ï¼Œæ£€æŸ¥æœ€ç»ˆåˆ—è¡¨ä¸­æ˜¯å¦ä»æœ‰NaNå€¼
        valid_entropies = [e for e in all_entropies if not np.isnan(e)]
        if not valid_entropies:
            # å¦‚æœå› æœªçŸ¥åŸå› æ‰€æœ‰è®¡ç®—éƒ½å¤±è´¥äº†ï¼Œè¿”å›é»˜è®¤çš„0å€¼
            return {'avg_entropy': 0.0, 'entropy_std': 0.0, 'max_entropy': 0.0}

        return {
            'avg_entropy': np.mean(valid_entropies),
            'entropy_std': np.std(valid_entropies),
            'max_entropy': np.max(valid_entropies)
        }
    def _compute_variance(self, attentions, seq_len):
        variances = [torch.var(attentions[h, p, :seq_len]).item() for h in range(attentions.shape[0]) for p in range(seq_len)]
        return {'avg_variance': np.mean(variances), 'variance_std': np.std(variances), 'max_variance': np.max(variances)}
    def _compute_concentration(self, attentions, seq_len):
        max_attentions = [torch.max(attentions[h, p, :seq_len]).item() for h in range(attentions.shape[0]) for p in range(seq_len)]
        return {'avg_max_attention': np.mean(max_attentions), 'concentration_std': np.std(max_attentions)}

# ========================= SLM/LLMæ¥å£ å’Œ å‡†ç¡®ç‡éªŒè¯å™¨ =========================
class SLMInterface(ModelInterface):
    def load_model(self):
        print(f"ğŸ”„ Loading SLM: {self.config.name}")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(self.config.model_path, torch_dtype=torch.float16, device_map="auto" if torch.cuda.is_available() else None, trust_remote_code=True, output_attentions=True,attn_implementation="eager" )
            if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token
            print("âœ… SLM loaded successfully")
        except Exception as e: print(f"âŒ Failed to load SLM: {e}"); raise

    def predict(self, question: str) -> str:
        if self.model is None:
            self.load_model()

        # --- ã€ã€ã€UPGRADED PROMPT FOR SLMã€‘ã€‘ã€‘---
        # This prompt guides the model to reason step-by-step and format the final answer.
        prompt = f"""Solve the following math problem. Think step by step and then write the final answer in the format #### <answer>.

    Question: {question}
    Answer: Let's think step by step.
    """
        # --- END NEW PROMPT ---

        inputs = self.tokenizer.encode(prompt, return_tensors="pt", max_length=1024, truncation=True)
        if torch.cuda.is_available():
            inputs = inputs.to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_new_tokens=200,
                num_return_sequences=1,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id
            )

        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Adjust the slicing logic to match the new prompt
        assistant_response_start = "Answer: Let's think step by step."
        start_index = full_response.rfind(assistant_response_start)  # Use rfind for robustness
        if start_index != -1:
            answer = full_response[start_index + len(assistant_response_start):].strip()
        else:  # Fallback if the prompt isn't found in the output
            answer = full_response

        return answer


# ==========================================================
# ===== åœ¨ common_utils.py ä¸­ï¼Œç”¨è¿™ä¸ªç‰ˆæœ¬æ›¿æ¢æ—§çš„ =====
# ==========================================================

class EnhancedLLMInterface(ModelInterface):
    def __init__(self, config: ModelConfig, hf_token: str = None):
        super().__init__(config)
        self.hf_token = hf_token

    def setup_authentication(self):
        # ... (æ­¤æ–¹æ³•ä¿æŒä¸å˜)
        if self.hf_token:
            login(token=self.hf_token)
        elif os.getenv('HUGGINGFACE_TOKEN'):
            print("Found token in environment.")
        else:
            print("âš ï¸ No HuggingFace token provided.")

    def load_model(self):
        # ... (æ­¤æ–¹æ³•ä¿æŒä¸å˜)
        print(f"ğŸ”„ Loading LLM: {self.config.name}");
        self.setup_authentication()
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(self.config.model_path, torch_dtype=torch.bfloat16,
                                                              device_map="auto", trust_remote_code=True)
            if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token
            print(f"âœ… LLM loaded successfully on {next(self.model.parameters()).device}")
        except Exception as e:
            print(f"âŒ Failed to load LLM: {e}"); raise

    def predict(self, question: str) -> str:
        """
        ã€ã€ã€å·²æ›´æ–°ä¸ºåŒ…å«ä¼˜åŒ–ç‰ˆPromptçš„ç‰ˆæœ¬ã€‘ã€‘ã€‘
        """
        if self.model is None:
            self.load_model()

        # --- ä½¿ç”¨æ–°çš„ã€æ›´ä¼˜åŒ–çš„â€œæ€ç»´é“¾â€æç¤º ---
        prompt = f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>
Solve the following math problem step-by-step. At the end, provide the final numerical answer inside <|answer|> and <|end-of-answer|>.

Question: {question}
<|eot_id|><|start_header_id|>assistant<|end_header_id|>
Let's think step by step.
"""
        # --- æ–°æç¤ºç»“æŸ ---

        inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(next(self.model.parameters()).device)
        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_new_tokens=300,
                num_return_sequences=1,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id
            )

        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # è°ƒæ•´ç­”æ¡ˆåˆ‡åˆ†é€»è¾‘ä»¥åŒ¹é…æ–°æç¤º
        assistant_response_start = "Let's think step by step."
        start_index = full_response.rfind(assistant_response_start)  # ä½¿ç”¨rfindç¡®ä¿ä»æœ€åçš„assistantéƒ¨åˆ†å¼€å§‹
        if start_index != -1:
            answer = full_response[start_index + len(assistant_response_start):].strip()
        else:
            # å¦‚æœæ‰¾ä¸åˆ°æç¤ºï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
            answer = re.sub(r"(?s).*\<\|start_header_id\|\>assistant\<\|end_header_id\|\>\n", "", full_response,
                            1).strip()

        return answer


class AccuracyValidator:
    @staticmethod
    def extract_final_answer(response: str) -> str:
        """
        ã€ã€ã€å‡çº§ç‰ˆç­”æ¡ˆæå–å™¨ã€‘ã€‘ã€‘
        1. ä¼˜å…ˆåŒ¹é…æˆ‘ä»¬è‡ªå®šä¹‰çš„ã€æœ€å¯é çš„ç­”æ¡ˆæ ‡ç­¾ã€‚
        2. å…¶æ¬¡åŒ¹é…GSM8Kçš„æ ‡å‡†æ ¼å¼ã€‚
        3. å¢åŠ æ›´å¤šå¸¸è§å¥å¼çš„åŒ¹é…ã€‚
        4. å°†æå–æœ€åä¸€ä¸ªæ•°å­—ä½œä¸ºæœ€ç»ˆçš„å¤‡ç”¨æ–¹æ¡ˆã€‚
        5. è‡ªåŠ¨å¤„ç†æ•°å­—ä¸­çš„é€—å·ã€‚
        """
        # ä¼˜å…ˆåŒ¹é…æˆ‘ä»¬ä¸ºLLMè®¾è®¡çš„<|answer|>æ ‡ç­¾
        match = re.search(r'<\|answer\|>(.*?)<\|end-of-answer\|>', response, re.DOTALL)
        if match:
            return match.group(1).strip().replace(',', '')

        # å…¶æ¬¡åŒ¹é…GSM8Kçš„æ ‡å‡†####æ ¼å¼
        match = re.search(r'####\s*([+-]?[\d,]+(?:\.\d+)?)', response)
        if match:
            return match.group(1).replace(',', '')

        # åŒ¹é… "the answer is X" ç­‰å¸¸è§å¥å¼
        patterns = [
            r'[Tt]he final answer is.*?([+-]?[\d,]+(?:\.\d+)?)',
            r'[Tt]he answer is.*?([+-]?[\d,]+(?:\.\d+)?)',
            r'is therefore.*?([+-]?[\d,]+(?:\.\d+)?)'
        ]
        for pattern in patterns:
            match = re.search(pattern, response)
            if match:
                return match.group(1).strip().replace(',', '')

        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œæå–å›ç­”ä¸­çš„æœ€åä¸€ä¸ªæ•°å­—ä½œä¸ºå¤‡ç”¨
        numbers = re.findall(r'([+-]?[\d,]+(?:\.\d+)?)', response)
        if numbers:
            return numbers[-1].replace(',', '')

        return "No answer found"

# --- ã€ã€ã€è¯·ç¡®ä¿è¿™ä¸ªæ–¹æ³•å­˜åœ¨ä¸”ä½äºç±»å®šä¹‰å†…éƒ¨ã€‘ã€‘ã€‘ ---
    @staticmethod
    def is_correct(predicted: str, ground_truth: str, tolerance: float = 1e-9) -> bool:
        """åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦æ­£ç¡®"""
        try:
            pred_num = float(predicted)
            true_num = float(ground_truth)
            return abs(pred_num - true_num) <= tolerance
        except (ValueError, TypeError):
            # å¦‚æœæ— æ³•è½¬æ¢ä¸ºæ•°å­—ï¼Œåˆ™è¿›è¡Œå­—ç¬¦ä¸²æ¯”è¾ƒ
            return str(predicted).strip().lower() == str(ground_truth).strip().lower()
# ========================= ä¸»è¯„ä¼°å™¨ =========================
class GSM8KAccuracyEvaluator:
    def __init__(self, hf_token=None, max_samples=1000, project_path="."):
        self.project_path = project_path
        self.hf_token = hf_token
        self.validator = AccuracyValidator()
        self.data_processor = FixedGSM8KProcessor(max_samples=max_samples)
        self.slm_config = ModelConfig(name="Llama-3.2-3B", model_path="meta-llama/Llama-3.2-3B", cost_per_token=0.001, avg_latency_ms=100)
        self.llm_config = ModelConfig(name="Llama-3.1-8B-Instruct", model_path="meta-llama/Llama-3.1-8B-Instruct", cost_per_token=0.003, avg_latency_ms=800)
        self.slm = SLMInterface(self.slm_config)
        self.llm = EnhancedLLMInterface(self.llm_config, hf_token)
        self.router = None

    def _ensure_slm_loaded(self):
        if self.slm.model is None: self.slm.load_model()
        if self.router is None:
            router_model_path = os.path.join(self.project_path, "router_model.pth")
            self.router = LearnedAttentionRouter(model_path=router_model_path, device=device, threshold=0.5)

    def evaluate_model_on_problems(self, model_interface, problems, model_name, max_problems=None):
        print(f"\nğŸ” è¯„ä¼° {model_name}...")
        if max_problems and len(problems) > max_problems:
            problems = random.sample(problems, max_problems)
        correct_count, total_count, detailed_results, error_cases = 0, len(problems), [], []
        for i, problem in enumerate(problems):
            print(f"   å¤„ç†é—®é¢˜ {i + 1}/{total_count}...", end="\r")
            try:
                response = model_interface.predict(problem['question'])
                predicted_answer = self.validator.extract_final_answer(response)
                is_correct = self.validator.is_correct(predicted_answer, problem['answer'])
                if is_correct: correct_count += 1
                else: error_cases.append({'q': problem['question'][:100], 'gt': problem['answer'], 'pred': predicted_answer})
                detailed_results.append({'is_correct': is_correct})
            except Exception as e:
                print(f"   âš ï¸ å¤„ç†é”™è¯¯: {e}"); detailed_results.append({'is_correct': False})
        print()
        accuracy = correct_count / total_count if total_count > 0 else 0
        print(f"âœ… {model_name} å‡†ç¡®ç‡: {accuracy:.2%} ({correct_count}/{total_count})")
        return {'accuracy': accuracy, 'correct_count': correct_count, 'total_count': total_count, 'detailed_results': detailed_results}

    def evaluate_routing_accuracy(self, simple_problems, complex_problems):
        print("\nğŸ§­ è¯„ä¼°è·¯ç”±å‡†ç¡®æ€§...")
        self._ensure_slm_loaded()
        correct_routes, total_routes, routing_details = 0, 0, []
        for p_type, problems, expected in [("simple", simple_problems, "SLM"), ("complex", complex_problems, "LLM")]:
            for problem in problems:
                route, _ = self.router.route(problem['question'], self.slm.model, self.slm.tokenizer)
                if route == expected: correct_routes += 1
                total_routes += 1
        accuracy = correct_routes / total_routes if total_routes > 0 else 0
        print(f"âœ… è·¯ç”±å‡†ç¡®ç‡: {accuracy:.2%} ({correct_routes}/{total_routes})")
        return {'routing_accuracy': accuracy, 'correct_routes': correct_routes, 'total_routes': total_routes}

    def run_gsm8k_evaluation(self, n_samples=200, simple_ratio=0.5):
        print("="*60 + "\nğŸš€ å¼€å§‹è¯„ä¼°\n" + "="*60)
        simple, complex = self.data_processor.get_balanced_sample(n_total=n_samples, simple_ratio=simple_ratio)
        slm_simple = self.evaluate_model_on_problems(self.slm, simple, "SLM on Simple")
        llm_complex = self.evaluate_model_on_problems(self.llm, complex, "LLM on Complex")
        slm_complex = self.evaluate_model_on_problems(self.slm, complex, "SLM on Complex (X-Eval)", max_problems=50)
        llm_simple = self.evaluate_model_on_problems(self.llm, simple, "LLM on Simple (X-Eval)", max_problems=50)
        routing = self.evaluate_routing_accuracy(simple, complex)
        smart_routing = self._calculate_smart_routing_performance(slm_simple, llm_complex, routing)
        self._generate_final_report(slm_simple, llm_complex, slm_complex, llm_simple, routing, smart_routing, n_samples)

    def _calculate_smart_routing_performance(self, slm_res, llm_res, rout_res):
        rout_acc, slm_acc, llm_acc = rout_res['routing_accuracy'], slm_res['accuracy'], llm_res['accuracy']
        simple_ratio = slm_res['total_count'] / max(1, slm_res['total_count'] + llm_res['total_count'])
        est_acc = ((slm_acc * simple_ratio) + (llm_acc * (1 - simple_ratio))) * rout_acc + ((slm_res.get('accuracy_on_complex', 0) * (1-simple_ratio)) + (llm_res.get('accuracy_on_simple', 1) * simple_ratio)) * (1-rout_acc)
        smart_cost = simple_ratio * self.slm_config.cost_per_token + (1-simple_ratio) * self.llm_config.cost_per_token
        return {'estimated_accuracy': est_acc, 'cost_per_problem': smart_cost, 'cost_savings_vs_llm': (self.llm_config.cost_per_token - smart_cost) / self.llm_config.cost_per_token}

    def _generate_final_report(self, s_s, l_c, s_c, l_s, r, s_r, n):
        print("\n" + "="*70 + "\nğŸ“‹ è¯„ä¼°æŠ¥å‘Š\n" + "="*70)
        print(f"ğŸ“Š è¯„ä¼°è§„æ¨¡: {n} é“é¢˜")
        print(f"\nğŸ¯ æ ¸å¿ƒæ€§èƒ½:\nâ”œâ”€â”€ SLM on Simple: {s_s['accuracy']:.2%}\nâ”œâ”€â”€ LLM on Complex: {l_c['accuracy']:.2%}\nâ”œâ”€â”€ Router Accuracy: {r['routing_accuracy']:.2%}\nâ””â”€â”€ Smart System Est. Accuracy: {s_r['estimated_accuracy']:.2%}")
        print(f"\nğŸ“Š äº¤å‰éªŒè¯:\nâ”œâ”€â”€ SLM on Complex: {s_c['accuracy']:.2%}\nâ”œâ”€â”€ LLM on Simple: {l_s['accuracy']:.2%}")
        print(f"\nğŸ’° æˆæœ¬æ•ˆç›Š:\nâ”œâ”€â”€ SLM Cost: ${self.slm_config.cost_per_token:.4f}\nâ”œâ”€â”€ LLM Cost: ${self.llm_config.cost_per_token:.4f}\nâ”œâ”€â”€ Smart Route Cost: ${s_r['cost_per_problem']:.4f}\nâ”œâ”€â”€ Savings vs LLM: {s_r['cost_savings_vs_llm']:.1%}")
        slm_ok, rout_ok, cost_ok = s_s['accuracy'] >= 0.8, r['routing_accuracy'] >= 0.85, s_r['cost_savings_vs_llm'] >= 0.3
        print(f"\nğŸ’¡ å…³é”®åˆ¤æ–­:\nâ”œâ”€â”€ SLM Reliability: {'âœ…' if slm_ok else 'âŒ'}\nâ”œâ”€â”€ Router Reliability: {'âœ…' if rout_ok else 'âŒ'}\nâ””â”€â”€ Cost-Benefit: {'âœ…' if cost_ok else 'âŒ'}")
        if slm_ok and rout_ok and cost_ok: rec = "âœ… å¼ºçƒˆæ¨è"
        elif not slm_ok: rec = "âŒ ä¸æ¨è - SLMä¸å¯é "
        elif not rout_ok: rec = "âŒ ä¸æ¨è - è·¯ç”±ä¸å‡†ç¡®"
        else: rec = "âš ï¸ è°¨æ…è€ƒè™‘ - æˆæœ¬æ•ˆç›Šæœ‰é™"
        print(f"\nğŸ¯ æœ€ç»ˆå»ºè®®: {rec}")