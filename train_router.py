import os
import json

import numpy as np
import torch
from torch import nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from common_utils import GSM8KAccuracyEvaluator, device, LearnedAttentionRouter, AccuracyValidator, \
    ComplexityPredictorNet
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pickle  # ç”¨äºä¿å­˜æ ‡å‡†åŒ–å¤„ç†å™¨



def generate_router_training_data(evaluator, output_file):
    """
    ç”Ÿæˆç”¨äºè®­ç»ƒè·¯ç”±å™¨çš„æ•°æ®é›†ã€‚
    ã€ã€ã€å¥å£®ç‰ˆï¼šæ”¯æŒå®æ—¶ä¿å­˜ã€æ–­ç‚¹ç»­ä¼ å’Œå®æ—¶è®¡æ•°ã€‘ã€‘ã€‘
    """
    import time

    print("\n" + "=" * 50 + "\nğŸ§  Generating router training data (Resumable Mode)...\n" + "=" * 50)

    processed_samples = set()
    processed_count = 0
    # --- ã€ã€ã€æ–°å¢ã€‘ã€‘ã€‘åˆå§‹åŒ–ç®€å•/å¤æ‚è®¡æ•°å™¨ ---
    simple_label_count = 0
    complex_label_count = 0
    # ------------------------------------

    if os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    if 'question' in data:
                        processed_samples.add(data['question'])
                        # --- ã€ã€ã€æ–°å¢ã€‘ã€‘ã€‘ä»å·²æœ‰æ–‡ä»¶ä¸­æ¢å¤è®¡æ•° ---
                        if data.get('label') == 0.0:
                            simple_label_count += 1
                        else:
                            complex_label_count += 1
                        # ------------------------------------
                except:
                    continue
        processed_count = len(processed_samples)
        print(f"ğŸ”„ Found existing data file with {processed_count} samples. Resuming...")
        print(f"   Initial counts: Simple Labels = {simple_label_count}, Complex Labels = {complex_label_count}")

    evaluator._ensure_slm_loaded()
    slm_interface = evaluator.slm
    all_problems = evaluator.data_processor.samples
    total_to_process = len(all_problems)
    print(f"ğŸ“Š Total problems to process: {total_to_process}. Already processed: {processed_count}.")

    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´

    with open(output_file, 'a', encoding='utf-8') as f:
        temp_feature_extractor = LearnedAttentionRouter("dummy_path.pth", device)
        validator = AccuracyValidator()

        for i, problem in enumerate(all_problems):
            if problem['question'] in processed_samples:
                continue

            current_progress = processed_count + 1

            try:
                # --- ã€ã€ã€æ–°çš„æ ‡ç­¾é€»è¾‘ã€‘ã€‘ã€‘---
                steps = evaluator.data_processor.count_solution_steps(problem['answer'])
                label = 1.0 if steps > 6 else 0.0  # ä½¿ç”¨æ­¥éª¤æ•°ä½œä¸ºå®¢è§‚æ ‡ç­¾
                # # --- ã€ã€ã€debugæ‘„åƒå¤´1å·ï¼šåœ¨è°ƒç”¨å‰æ‰“å°å‚æ•°ã€‘ã€‘ã€‘---
                # print("\n--- [CALLER SIDE] Preparing to call extract_core_features ---")
                # print(f"   - Arg 1 (question): type={type(problem['question'])}")
                # print(f"   - Arg 2 (model): type={type(slm_interface.model)}")
                # print(f"   - Arg 3 (tokenizer): type={type(slm_interface.tokenizer)}")
                # print(f"   - Arg 4 (slm_interface): type={type(slm_interface)}")
                # print("-----------------------------------------------------------------")
                # # --- æ‰“å°ç»“æŸ ---
                # --- ã€ã€ã€æ–°çš„ç‰¹å¾æå–è°ƒç”¨ã€‘ã€‘ã€‘---
                features = temp_feature_extractor.extract_core_features(
                    problem['question'],
                    slm_interface.model,
                    slm_interface.tokenizer,
                    slm_interface
                )
                # --- ã€ã€ã€æ ¸å¿ƒä¿®æ”¹ï¼šä¸å†ä¾èµ–SLMçš„å¯¹é”™ã€‘ã€‘ã€‘---

                # 1. ç›´æ¥ä»é—®é¢˜çš„æ ‡å‡†ç­”æ¡ˆä¸­è®¡ç®—è§£é¢˜æ­¥éª¤æ•°
                # problem['answer'] æ­¤æ—¶è¿˜æ˜¯åŸå§‹çš„ã€åŒ…å«è§£é¢˜æ­¥éª¤çš„ç­”æ¡ˆæ–‡æœ¬
                # steps = evaluator.data_processor.count_solution_steps(problem['answer'])

                # 2. æ ¹æ®æ­¥éª¤æ•°ï¼Œè®¾å®šä¸€ä¸ªæ¸…æ™°ã€å®¢è§‚çš„â€œå¤æ‚â€æ ‡ç­¾
                # è¿™é‡Œçš„é˜ˆå€¼â€œ4â€æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ï¼Œæ‚¨å¯ä»¥åç»­è¿›è¡Œæ•æ„Ÿæ€§åˆ†æ
                # label = 1.0 if steps > 6 else 0.0

                # ----------------------------------------------------

                # ç‰¹å¾æå–éƒ¨åˆ†ä¿æŒä¸å˜ï¼Œä¾ç„¶éœ€è¦SLMçš„â€œæ€è€ƒè¿‡ç¨‹â€
                # features = temp_feature_extractor.extract_core_features(
                #     problem['question'], slm_interface.model, slm_interface.tokenizer
                # )

                # å¦‚æœç‰¹å¾æå–å¤±è´¥ï¼Œåˆ™è·³è¿‡è¯¥æ ·æœ¬
                if not features:
                    print(f"\n   âš ï¸ Skipped problem #{i} due to feature extraction failure.")
                    continue

                # åç»­çš„è®¡æ•°ã€ä¿å­˜é€»è¾‘ä¿æŒä¸å˜
                if label == 0.0:
                    simple_label_count += 1
                else:
                    complex_label_count += 1

                sample_to_save = {
                    "question": problem['question'],
                    "features": features,
                    "label": label
                }

                f.write(json.dumps(sample_to_save) + '\n')
                f.flush()
                processed_count += 1
                processed_samples.add(problem['question'])
                # slm_response = slm_interface.predict(problem['question'])
                # slm_answer = validator.extract_final_answer(slm_response)
                # gt_answer = evaluator.data_processor.extract_answer(problem['answer'])
                # is_slm_correct = validator.is_correct(slm_answer, gt_answer)
                #
                # label = 1.0 if not is_slm_correct else 0.0
                #
                # # --- ã€ã€ã€æ–°å¢ã€‘ã€‘ã€‘æ›´æ–°è®¡æ•°å™¨ ---
                # if label == 0.0:
                #     simple_label_count += 1
                # else:
                #     complex_label_count += 1
                # # --------------------------------
                #
                # features = temp_feature_extractor.extract_core_features(
                #     problem['question'], slm_interface.model, slm_interface.tokenizer
                # )
                #
                # sample_to_save = {"question": problem['question'], "features": features, "label": label}
                #
                # f.write(json.dumps(sample_to_save) + '\n')
                # f.flush()
                # processed_count += 1
                # processed_samples.add(problem['question'])

                # --- ã€ã€ã€æœ€ç»ˆç‰ˆï¼šå¢åŠ è¯¦ç»†è¿›åº¦æŠ¥å‘Šã€‘ã€‘ã€‘---
                # æ¯å¤„ç†20ä¸ªæ ·æœ¬ï¼Œæˆ–è€…åœ¨ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªæ—¶ï¼Œæ‰“å°ä¸€æ¬¡æ¸…æ™°çš„è¿›åº¦
                if current_progress % 20 == 0 or current_progress == 1 or current_progress == total_to_process:
                    # --- 1. ã€æ–°çš„é€Ÿåº¦è®¡ç®—é€»è¾‘ã€‘ ---
                    # è®¡ç®—ä»è„šæœ¬å¼€å§‹åˆ°ç°åœ¨ï¼Œæ–°å¤„ç†äº†å¤šå°‘æ ·æœ¬
                    newly_processed_count = processed_count - (simple_label_count + complex_label_count)

                    elapsed_time = time.time() - start_time
                    samples_per_second = newly_processed_count / elapsed_time if elapsed_time > 0 else 0

                    print(f"\n--- Progress Update ---")
                    print(f"   Processed: {current_progress}/{total_to_process}")

                    # --- 2. ã€æ–°çš„æ ‡ç­¾å«ä¹‰è¯´æ˜ã€‘ ---
                    print(
                        f"   Label Counts: Simple (Steps < 6) = {simple_label_count}, Complex (Steps >= 6) = {complex_label_count}")

                    print(f"   Speed: {samples_per_second:.2f} samples/sec")
                    print(f"-----------------------")
                # --- è¿›åº¦æŠ¥å‘Šç»“æŸ ---


            except Exception as e:
                print(f"\n   âš ï¸ Skipped problem #{i} ('{problem['question'][:30]}...') due to error: {e}")
                continue

    print(f"\nâœ… Training data generation complete! Total {processed_count} samples saved to {output_file}")
    print(f"   Final Label Distribution: Simple = {simple_label_count}, Complex = {complex_label_count}")


# åœ¨ train_router.py ä¸­

class RouterDataset(Dataset):
    def __init__(self, data_path, feature_subset: list):  # <--- ã€æ ¸å¿ƒä¿®æ­£ã€‘åœ¨è¿™é‡Œæ¥æ”¶ feature_subset
        self.samples = []
        self.feature_keys = feature_subset if feature_subset else [
            'entropy_mean',
            'entropy_std',
            'entropy_max'
        ]

        print(f"--- Dataset Initialized using {len(self.feature_keys)} features: {self.feature_keys} ---")

        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    sample = json.loads(line)
                    # ä¸¥æ ¼æŒ‰ç…§ä¼ å…¥çš„feature_keysé¡ºåºå’Œæ•°é‡æ„å»ºç‰¹å¾å‘é‡
                    feature_vector = [sample['features'].get(key, 0.0) for key in self.feature_keys]
                    self.samples.append({
                        "features": torch.tensor(feature_vector, dtype=torch.float32),
                        "label": torch.tensor([sample['label']], dtype=torch.float32)
                    })
                except (json.JSONDecodeError, KeyError) as e:
                    print(f"Warning: Skipping a malformed line in dataset. Error: {e}")
                    continue

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]


# ========================================================================
# ===== åœ¨ train_router.py ä¸­ï¼Œä½¿ç”¨è¿™ä¸ªã€æœ€ç»ˆç¨³å¥ç‰ˆã€‘çš„è®­ç»ƒå‡½æ•° =====
# ========================================================================

def train_router(training_data_path, model_save_path, feature_subset, epochs=20, lr=1e-4, batch_size=32):
    # å¯¼å…¥å®Œæˆè¿™ä¸ªå‡½æ•°æ‰€éœ€çš„å…¨éƒ¨åº“
    from common_utils import ComplexityPredictorNet
    import torch.optim as optim
    from torch.utils.data import TensorDataset, DataLoader
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    import pickle
    import numpy as np
    import os

    print(f"\nğŸš€ Training the smart router with {len(feature_subset)} features...")

    # 1. åŠ è½½æ•°æ® (é€»è¾‘ä¸å˜)
    dataset = RouterDataset(training_data_path, feature_subset=feature_subset)
    if len(dataset) == 0:
        print("âŒ Error: Dataset is empty. Cannot start training.")
        return

    all_features = np.array([s['features'].numpy() for s in dataset])
    all_labels = np.array([s['label'].numpy() for s in dataset])

    # æ£€æŸ¥åŸå§‹ç‰¹å¾ä¸­æ˜¯å¦æœ‰nan/infå€¼
    if not np.all(np.isfinite(all_features)):
        print("âš ï¸ Warning: NaN or infinity found in raw features. Replacing with 0.")
        all_features = np.nan_to_num(all_features)

    # 2. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (80/20)ï¼Œç”¨äºå®¢è§‚è¯„ä¼°æ¨¡å‹å­¦ä¹ æ•ˆæœ
    X_train, X_val, y_train, y_val = train_test_split(
        all_features, all_labels, test_size=0.2, random_state=42, stratify=all_labels
    )
    print(f"--- Data split: {len(X_train)} for training, {len(X_val)} for validation ---")

    # --- ã€ã€ã€æ ¸å¿ƒä¿®å¤ 1ï¼šç‰¹å¾æ ‡å‡†åŒ–ã€‘ã€‘ã€‘---
    # åˆ›å»ºä¸€ä¸ªæ ‡å‡†åŒ–å¤„ç†å™¨ï¼Œå¹¶ç”¨ã€è®­ç»ƒé›†ã€‘çš„æ•°æ®è¿›è¡Œæ‹Ÿåˆ
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    # ç”¨åŒä¸€ä¸ªscaleræ¥è½¬æ¢éªŒè¯é›†
    X_val_scaled = scaler.transform(X_val)

    # ä¿å­˜è¿™ä¸ªscalerï¼Œä»¥ä¾¿æœªæ¥åœ¨è¯„ä¼°æ—¶ä½¿ç”¨
    scaler_path = os.path.join(os.path.dirname(model_save_path), "router_scaler.pkl")
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"âœ… Feature scaler saved to {scaler_path}")
    # --- ç‰¹å¾æ ‡å‡†åŒ–ç»“æŸ ---

    # 4. åˆ›å»ºPyTorchçš„Datasetå’ŒDataLoader
    train_tensor_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32),
                                         torch.tensor(y_train, dtype=torch.float32))
    val_tensor_dataset = TensorDataset(torch.tensor(X_val_scaled, dtype=torch.float32),
                                       torch.tensor(y_val, dtype=torch.float32))
    train_dataloader = DataLoader(train_tensor_dataset, batch_size=batch_size, shuffle=True)
    val_dataloader = DataLoader(val_tensor_dataset, batch_size=batch_size)

    # 5. æ¨¡å‹ã€æŸå¤±å’Œä¼˜åŒ–å™¨ (é€»è¾‘ä¸å˜)
    model = ComplexityPredictorNet(input_features=len(feature_subset)).to(device)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # 6. è®­ç»ƒå¾ªç¯ (å¢åŠ äº†ç¨³å®šæ€§ä¿æŠ¤)
    print("--- Starting training loop ---")
    for epoch in range(epochs):
        model.train()
        for features, labels in train_dataloader:
            features, labels = features.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, labels)

            # --- ã€ã€ã€æ ¸å¿ƒä¿®å¤ 2ï¼šæ£€æŸ¥å¹¶è·³è¿‡NaN Lossã€‘ã€‘ã€‘---
            if torch.isnan(loss):
                print(f"Epoch {epoch + 1}: Detected NaN loss. Skipping this batch.")
                continue

            loss.backward()

            # --- ã€ã€ã€æ ¸å¿ƒä¿®å¤ 3ï¼šæ¢¯åº¦è£å‰ªã€‘ã€‘ã€‘---
            # å¼ºåˆ¶å°†è¿‡å¤§çš„æ¢¯åº¦â€œæ‹‰å›â€åˆ°ä¸€ä¸ªåˆç†çš„èŒƒå›´å†…ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

        # æ¯ä¸ªepochåè¿›è¡ŒéªŒè¯ (é€»è¾‘ä¸å˜)
        model.eval()
        val_correct, val_total = 0, 0
        with torch.no_grad():
            for features, labels in val_dataloader:
                features, labels = features.to(device), labels.to(device)
                outputs = model(features)
                preds = torch.sigmoid(outputs) > 0.5
                val_correct += (preds == labels.bool()).sum().item()
                val_total += labels.size(0)
        print(f"Epoch {epoch + 1:02d}/{epochs} | Val Acc: {val_correct / val_total if val_total > 0 else 0:.2%}")

    torch.save(model.state_dict(), model_save_path)
    print(f"\nâœ… Training complete! Model saved to {model_save_path}")


# ========================================================================
# ===== åœ¨ train_router.py åº•éƒ¨ï¼Œä½¿ç”¨è¿™ä¸ªã€æœ€ç»ˆç®€åŒ–ç‰ˆã€‘çš„ä¸»æµç¨‹ =====
# ========================================================================

if __name__ == "__main__":
    # --- 1. ä»ç¯å¢ƒå˜é‡ä¸­è¯»å– Colab ä¼ é€’è¿‡æ¥çš„ä¿¡æ¯ ---
    PROJECT_PATH = os.getenv('PROJECT_PATH_GDRIVE', '.')
    hf_token = os.getenv('HUGGINGFACE_TOKEN')

    # --- 2. å®šä¹‰æ–‡ä»¶è·¯å¾„ ---
    # å»ºè®®ä½¿ç”¨æ–°æ–‡ä»¶åï¼Œæ¸…æ™°åœ°è¡¨æ˜è¿™æ˜¯åŸºäºåŠ¨æ€ç‰¹å¾çš„
    training_file = os.path.join(PROJECT_PATH, "router_training_data_dynamic.jsonl")
    model_file = os.path.join(PROJECT_PATH, "router_model_dynamic.pth")

    # --- 3. æ‰§è¡Œæ•°æ®ç”Ÿæˆ ---
    # åˆå§‹åŒ–è¯„ä¼°å™¨å®ä¾‹ï¼Œç”¨äºæ•°æ®ç”Ÿæˆ
    # max_samples å†³å®šäº†æ‚¨è¦ç”Ÿæˆå¤šå°‘è®­ç»ƒæ•°æ®ï¼Œ2000æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹
    evaluator_for_data_gen = GSM8KAccuracyEvaluator(hf_token=hf_token, max_samples=200, project_path=PROJECT_PATH)

    # è°ƒç”¨æ•°æ®ç”Ÿæˆå‡½æ•°ã€‚
    # å®ƒä¼šä½¿ç”¨æˆ‘ä»¬æœ€æ–°çš„â€œåŠ¨æ€ç‰¹å¾æå–â€å’Œâ€œæ­¥éª¤æ•°æ ‡ç­¾â€é€»è¾‘ã€‚
    # å¦‚æœæ•°æ®å·²å­˜åœ¨ï¼Œè¿™æ­¥ä¼šå› â€œæ–­ç‚¹ç»­ä¼ â€è€Œå¾ˆå¿«å®Œæˆã€‚
    generate_router_training_data(evaluator_for_data_gen, output_file=training_file)

    # --- 4. æ‰§è¡Œæ¨¡å‹è®­ç»ƒ ---
    # åªæœ‰åœ¨æ•°æ®æ–‡ä»¶ç¡®è®¤å­˜åœ¨åï¼Œæ‰ç»§ç»­è¿›è¡Œ
    if os.path.exists(training_file) and os.path.getsize(training_file) > 0:

        # å®šä¹‰æˆ‘ä»¬æ–°çš„4ä¸ªåŠ¨æ€ç‰¹å¾
        dynamic_features = ['entropy_mean', 'entropy_std', 'entropy_max', 'entropy_trend']

        # ç›´æ¥è°ƒç”¨è®­ç»ƒå‡½æ•°ï¼Œä½¿ç”¨å…¨éƒ¨çš„åŠ¨æ€ç‰¹å¾
        train_router(training_data_path=training_file,
                     model_save_path=model_file,
                     feature_subset=dynamic_features)
    else:
        print(f"âŒ å…³é”®é”™è¯¯ï¼šæ•°æ®æ–‡ä»¶ '{training_file}' æœªæ‰¾åˆ°æˆ–ä¸ºç©ºï¼Œè®­ç»ƒæ— æ³•ç»§ç»­ã€‚")

    print("\nâœ… è®­ç»ƒæµç¨‹ç»“æŸï¼")
