import os
import json
import torch
from torch import nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from common_utils import GSM8KAccuracyEvaluator, device, LearnedAttentionRouter, AccuracyValidator, \
    ComplexityPredictorNet


# ========================================================================
# ===== åœ¨ train_router.py æ–‡ä»¶ä¸­ï¼Œä½¿ç”¨è¿™ä¸ªæ–°ç‰ˆæœ¬çš„å‡½æ•°æ¥æ›¿æ¢æ—§çš„ =====
# ========================================================================

# ========================================================================
# ===== åœ¨ train_router.py æ–‡ä»¶ä¸­ï¼Œä½¿ç”¨è¿™ä¸ªã€å¸¦å®æ—¶è®¡æ•°ã€‘çš„ç‰ˆæœ¬ =====
# ========================================================================

def generate_router_training_data(evaluator, output_file):
    """
    ç”Ÿæˆç”¨äºè®­ç»ƒè·¯ç”±å™¨çš„æ•°æ®é›†ã€‚
    ã€ã€ã€å¥å£®ç‰ˆï¼šæ”¯æŒå®æ—¶ä¿å­˜ã€æ–­ç‚¹ç»­ä¼ å’Œå®æ—¶è®¡æ•°ã€‘ã€‘ã€‘
    """
    import time

    print("\n" + "=" * 50 + "\nğŸ§  Generating router training data (Resumable Mode)...\n" + "=" * 50)

    processed_samples = set()
    processed_count = 0
    # --- ã€ã€ã€æ–°å¢ã€‘ã€‘ã€‘åˆå§‹åŒ–ç®€å•/å¤æ‚è®¡æ•°å™¨ ---
    simple_label_count = 0
    complex_label_count = 0
    # ------------------------------------

    if os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    if 'question' in data:
                        processed_samples.add(data['question'])
                        # --- ã€ã€ã€æ–°å¢ã€‘ã€‘ã€‘ä»å·²æœ‰æ–‡ä»¶ä¸­æ¢å¤è®¡æ•° ---
                        if data.get('label') == 0.0:
                            simple_label_count += 1
                        else:
                            complex_label_count += 1
                        # ------------------------------------
                except:
                    continue
        processed_count = len(processed_samples)
        print(f"ğŸ”„ Found existing data file with {processed_count} samples. Resuming...")
        print(f"   Initial counts: Simple Labels = {simple_label_count}, Complex Labels = {complex_label_count}")

    evaluator._ensure_slm_loaded()
    slm_interface = evaluator.slm
    all_problems = evaluator.data_processor.samples
    total_to_process = len(all_problems)
    print(f"ğŸ“Š Total problems to process: {total_to_process}. Already processed: {processed_count}.")

    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´

    with open(output_file, 'a', encoding='utf-8') as f:
        temp_feature_extractor = LearnedAttentionRouter("dummy_path.pth", device)
        validator = AccuracyValidator()

        for i, problem in enumerate(all_problems):
            if problem['question'] in processed_samples:
                continue

            current_progress = processed_count + 1

            try:
                # --- ã€ã€ã€æ–°çš„æ ‡ç­¾é€»è¾‘ã€‘ã€‘ã€‘---
                steps = evaluator.data_processor.count_solution_steps(problem['answer'])
                label = 1.0 if steps > 6 else 0.0  # ä½¿ç”¨æ­¥éª¤æ•°ä½œä¸ºå®¢è§‚æ ‡ç­¾
                # --- ã€ã€ã€æ‘„åƒå¤´1å·ï¼šåœ¨è°ƒç”¨å‰æ‰“å°å‚æ•°ã€‘ã€‘ã€‘---
                print("\n--- [CALLER SIDE] Preparing to call extract_core_features ---")
                print(f"   - Arg 1 (question): type={type(problem['question'])}")
                print(f"   - Arg 2 (model): type={type(slm_interface.model)}")
                print(f"   - Arg 3 (tokenizer): type={type(slm_interface.tokenizer)}")
                print(f"   - Arg 4 (slm_interface): type={type(slm_interface)}")
                print("-----------------------------------------------------------------")
                # --- æ‰“å°ç»“æŸ ---
                # --- ã€ã€ã€æ–°çš„ç‰¹å¾æå–è°ƒç”¨ã€‘ã€‘ã€‘---
                features = temp_feature_extractor.extract_core_features(
                    problem['question'],
                    slm_interface.model,
                    slm_interface.tokenizer,
                    slm_interface
                )
                # --- ã€ã€ã€æ ¸å¿ƒä¿®æ”¹ï¼šä¸å†ä¾èµ–SLMçš„å¯¹é”™ã€‘ã€‘ã€‘---

                # 1. ç›´æ¥ä»é—®é¢˜çš„æ ‡å‡†ç­”æ¡ˆä¸­è®¡ç®—è§£é¢˜æ­¥éª¤æ•°
                # problem['answer'] æ­¤æ—¶è¿˜æ˜¯åŸå§‹çš„ã€åŒ…å«è§£é¢˜æ­¥éª¤çš„ç­”æ¡ˆæ–‡æœ¬
                # steps = evaluator.data_processor.count_solution_steps(problem['answer'])

                # 2. æ ¹æ®æ­¥éª¤æ•°ï¼Œè®¾å®šä¸€ä¸ªæ¸…æ™°ã€å®¢è§‚çš„â€œå¤æ‚â€æ ‡ç­¾
                # è¿™é‡Œçš„é˜ˆå€¼â€œ4â€æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ï¼Œæ‚¨å¯ä»¥åç»­è¿›è¡Œæ•æ„Ÿæ€§åˆ†æ
                # label = 1.0 if steps > 6 else 0.0

                # ----------------------------------------------------

                # ç‰¹å¾æå–éƒ¨åˆ†ä¿æŒä¸å˜ï¼Œä¾ç„¶éœ€è¦SLMçš„â€œæ€è€ƒè¿‡ç¨‹â€
                # features = temp_feature_extractor.extract_core_features(
                #     problem['question'], slm_interface.model, slm_interface.tokenizer
                # )

                # å¦‚æœç‰¹å¾æå–å¤±è´¥ï¼Œåˆ™è·³è¿‡è¯¥æ ·æœ¬
                if not features:
                    print(f"\n   âš ï¸ Skipped problem #{i} due to feature extraction failure.")
                    continue

                # åç»­çš„è®¡æ•°ã€ä¿å­˜é€»è¾‘ä¿æŒä¸å˜
                if label == 0.0:
                    simple_label_count += 1
                else:
                    complex_label_count += 1

                sample_to_save = {
                    "question": problem['question'],
                    "features": features,
                    "label": label
                }

                f.write(json.dumps(sample_to_save) + '\n')
                f.flush()
                processed_count += 1
                processed_samples.add(problem['question'])
                # slm_response = slm_interface.predict(problem['question'])
                # slm_answer = validator.extract_final_answer(slm_response)
                # gt_answer = evaluator.data_processor.extract_answer(problem['answer'])
                # is_slm_correct = validator.is_correct(slm_answer, gt_answer)
                #
                # label = 1.0 if not is_slm_correct else 0.0
                #
                # # --- ã€ã€ã€æ–°å¢ã€‘ã€‘ã€‘æ›´æ–°è®¡æ•°å™¨ ---
                # if label == 0.0:
                #     simple_label_count += 1
                # else:
                #     complex_label_count += 1
                # # --------------------------------
                #
                # features = temp_feature_extractor.extract_core_features(
                #     problem['question'], slm_interface.model, slm_interface.tokenizer
                # )
                #
                # sample_to_save = {"question": problem['question'], "features": features, "label": label}
                #
                # f.write(json.dumps(sample_to_save) + '\n')
                # f.flush()
                # processed_count += 1
                # processed_samples.add(problem['question'])

                # --- ã€ã€ã€æœ€ç»ˆç‰ˆï¼šå¢åŠ è¯¦ç»†è¿›åº¦æŠ¥å‘Šã€‘ã€‘ã€‘---
                # æ¯å¤„ç†20ä¸ªæ ·æœ¬ï¼Œæˆ–è€…åœ¨ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªæ—¶ï¼Œæ‰“å°ä¸€æ¬¡æ¸…æ™°çš„è¿›åº¦
                if current_progress % 20 == 0 or current_progress == 1 or current_progress == total_to_process:
                    # --- 1. ã€æ–°çš„é€Ÿåº¦è®¡ç®—é€»è¾‘ã€‘ ---
                    # è®¡ç®—ä»è„šæœ¬å¼€å§‹åˆ°ç°åœ¨ï¼Œæ–°å¤„ç†äº†å¤šå°‘æ ·æœ¬
                    newly_processed_count = processed_count - (simple_label_count + complex_label_count)

                    elapsed_time = time.time() - start_time
                    samples_per_second = newly_processed_count / elapsed_time if elapsed_time > 0 else 0

                    print(f"\n--- Progress Update ---")
                    print(f"   Processed: {current_progress}/{total_to_process}")

                    # --- 2. ã€æ–°çš„æ ‡ç­¾å«ä¹‰è¯´æ˜ã€‘ ---
                    print(
                        f"   Label Counts: Simple (Steps < 6) = {simple_label_count}, Complex (Steps >= 6) = {complex_label_count}")

                    print(f"   Speed: {samples_per_second:.2f} samples/sec")
                    print(f"-----------------------")
                # --- è¿›åº¦æŠ¥å‘Šç»“æŸ ---


            except Exception as e:
                print(f"\n   âš ï¸ Skipped problem #{i} ('{problem['question'][:30]}...') due to error: {e}")
                continue

    print(f"\nâœ… Training data generation complete! Total {processed_count} samples saved to {output_file}")
    print(f"   Final Label Distribution: Simple = {simple_label_count}, Complex = {complex_label_count}")


# åœ¨ train_router.py ä¸­
class RouterDataset(Dataset):
    def __init__(self, data_path, feature_subset=None):
        self.samples = []
        self.feature_keys = feature_subset if feature_subset else ['entropy_mean', 'entropy_std', 'entropy_max',
                                                                   'entropy_trend']
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                sample = json.loads(line)
                feature_vector = [sample['features'].get(key, 0.0) for key in self.feature_keys]
                self.samples.append({
                    "features": torch.tensor(feature_vector, dtype=torch.float32),
                    "label": torch.tensor([sample['label']], dtype=torch.float32)
                })

    def __len__(self): return len(self.samples)

    def __getitem__(self, idx): return self.samples[idx]


# ==============================================================
# ===== ä¿®æ”¹ç‚¹ 2: train_router ç°åœ¨ä¹Ÿæ¥æ”¶ feature_subset =====
# ==============================================================
def train_router(training_data_path, model_save_path, feature_subset: list, epochs=20, lr=1e-4, batch_size=32):
    print("\n" + "=" * 50 + f"\nğŸš€ Training the smart router with {len(feature_subset)} features...\n" + "=" * 50)

    # 1. ä½¿ç”¨ä¼ å…¥çš„ç‰¹å¾å­é›†æ¥åˆ›å»ºæ•°æ®é›†
    dataset = RouterDataset(training_data_path, feature_subset=feature_subset)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # 2. ã€å…³é”®ã€‘æ ¹æ®ç‰¹å¾å­é›†çš„æ•°é‡ï¼ŒåŠ¨æ€åˆ›å»ºæ¨¡å‹
    num_features = len(feature_subset) if feature_subset else 4
    model = ComplexityPredictorNet(input_features=num_features).to(device)

    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # 3. è®­ç»ƒå¾ªç¯ (ä¿æŒä¸å˜)
    for epoch in range(epochs):
        model.train()
        total_loss, correct_preds, total_samples = 0, 0, 0
        for batch in dataloader:
            features, labels = batch['features'].to(device), batch['label'].to(device)
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            preds = torch.sigmoid(outputs) > 0.5
            correct_preds += (preds == labels.bool()).sum().item()
            total_samples += labels.size(0)

        avg_loss = total_loss / len(dataloader)
        accuracy = correct_preds / total_samples
        print(f"Epoch {epoch + 1:02d}/{epochs} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2%}")

    torch.save(model.state_dict(), model_save_path)
    print(f"\nâœ… Training complete! Model saved to {model_save_path}")


# ========================================================================
# ===== åœ¨ train_router.py åº•éƒ¨ï¼Œä½¿ç”¨è¿™ä¸ªã€æœ€ç»ˆç®€åŒ–ç‰ˆã€‘çš„ä¸»æµç¨‹ =====
# ========================================================================

if __name__ == "__main__":
    # --- 1. ä»ç¯å¢ƒå˜é‡ä¸­è¯»å– Colab ä¼ é€’è¿‡æ¥çš„ä¿¡æ¯ ---
    PROJECT_PATH = os.getenv('PROJECT_PATH_GDRIVE', '.')
    hf_token = os.getenv('HUGGINGFACE_TOKEN')

    # --- 2. å®šä¹‰æ–‡ä»¶è·¯å¾„ ---
    # å»ºè®®ä½¿ç”¨æ–°æ–‡ä»¶åï¼Œæ¸…æ™°åœ°è¡¨æ˜è¿™æ˜¯åŸºäºåŠ¨æ€ç‰¹å¾çš„
    training_file = os.path.join(PROJECT_PATH, "router_training_data_dynamic.jsonl")
    model_file = os.path.join(PROJECT_PATH, "router_model_dynamic.pth")

    # --- 3. æ‰§è¡Œæ•°æ®ç”Ÿæˆ ---
    # åˆå§‹åŒ–è¯„ä¼°å™¨å®ä¾‹ï¼Œç”¨äºæ•°æ®ç”Ÿæˆ
    # max_samples å†³å®šäº†æ‚¨è¦ç”Ÿæˆå¤šå°‘è®­ç»ƒæ•°æ®ï¼Œ2000æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹
    evaluator_for_data_gen = GSM8KAccuracyEvaluator(hf_token=hf_token, max_samples=2000, project_path=PROJECT_PATH)

    # è°ƒç”¨æ•°æ®ç”Ÿæˆå‡½æ•°ã€‚
    # å®ƒä¼šä½¿ç”¨æˆ‘ä»¬æœ€æ–°çš„â€œåŠ¨æ€ç‰¹å¾æå–â€å’Œâ€œæ­¥éª¤æ•°æ ‡ç­¾â€é€»è¾‘ã€‚
    # å¦‚æœæ•°æ®å·²å­˜åœ¨ï¼Œè¿™æ­¥ä¼šå› â€œæ–­ç‚¹ç»­ä¼ â€è€Œå¾ˆå¿«å®Œæˆã€‚
    generate_router_training_data(evaluator_for_data_gen, output_file=training_file)

    # --- 4. æ‰§è¡Œæ¨¡å‹è®­ç»ƒ ---
    # åªæœ‰åœ¨æ•°æ®æ–‡ä»¶ç¡®è®¤å­˜åœ¨åï¼Œæ‰ç»§ç»­è¿›è¡Œ
    if os.path.exists(training_file) and os.path.getsize(training_file) > 0:

        # å®šä¹‰æˆ‘ä»¬æ–°çš„4ä¸ªåŠ¨æ€ç‰¹å¾
        dynamic_features = ['entropy_mean', 'entropy_std', 'entropy_max', 'entropy_trend']

        # ç›´æ¥è°ƒç”¨è®­ç»ƒå‡½æ•°ï¼Œä½¿ç”¨å…¨éƒ¨çš„åŠ¨æ€ç‰¹å¾
        train_router(training_data_path=training_file,
                     model_save_path=model_file,
                     feature_subset=dynamic_features)
    else:
        print(f"âŒ å…³é”®é”™è¯¯ï¼šæ•°æ®æ–‡ä»¶ '{training_file}' æœªæ‰¾åˆ°æˆ–ä¸ºç©ºï¼Œè®­ç»ƒæ— æ³•ç»§ç»­ã€‚")

    print("\nâœ… è®­ç»ƒæµç¨‹ç»“æŸï¼")
