import os
import json
import torch
from torch import nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from common_utils import GSM8KAccuracyEvaluator, device, LearnedAttentionRouter, AccuracyValidator, \
    ComplexityPredictorNet


# ========================================================================
# ===== åœ¨ train_router.py æ–‡ä»¶ä¸­ï¼Œä½¿ç”¨è¿™ä¸ªæ–°ç‰ˆæœ¬çš„å‡½æ•°æ¥æ›¿æ¢æ—§çš„ =====
# ========================================================================

# ========================================================================
# ===== åœ¨ train_router.py æ–‡ä»¶ä¸­ï¼Œä½¿ç”¨è¿™ä¸ªã€å¸¦å®æ—¶è®¡æ•°ã€‘çš„ç‰ˆæœ¬ =====
# ========================================================================

def generate_router_training_data(evaluator, output_file):
    """
    ç”Ÿæˆç”¨äºè®­ç»ƒè·¯ç”±å™¨çš„æ•°æ®é›†ã€‚
    ã€ã€ã€å¥å£®ç‰ˆï¼šæ”¯æŒå®æ—¶ä¿å­˜ã€æ–­ç‚¹ç»­ä¼ å’Œå®æ—¶è®¡æ•°ã€‘ã€‘ã€‘
    """
    import time

    print("\n" + "=" * 50 + "\nğŸ§  Generating router training data (Resumable Mode)...\n" + "=" * 50)

    processed_samples = set()
    processed_count = 0
    # --- ã€ã€ã€æ–°å¢ã€‘ã€‘ã€‘åˆå§‹åŒ–ç®€å•/å¤æ‚è®¡æ•°å™¨ ---
    simple_label_count = 0
    complex_label_count = 0
    # ------------------------------------

    if os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    if 'question' in data:
                        processed_samples.add(data['question'])
                        # --- ã€ã€ã€æ–°å¢ã€‘ã€‘ã€‘ä»å·²æœ‰æ–‡ä»¶ä¸­æ¢å¤è®¡æ•° ---
                        if data.get('label') == 0.0:
                            simple_label_count += 1
                        else:
                            complex_label_count += 1
                        # ------------------------------------
                except:
                    continue
        processed_count = len(processed_samples)
        print(f"ğŸ”„ Found existing data file with {processed_count} samples. Resuming...")
        print(f"   Initial counts: Simple Labels = {simple_label_count}, Complex Labels = {complex_label_count}")

    evaluator._ensure_slm_loaded()
    slm_interface = evaluator.slm
    all_problems = evaluator.data_processor.samples
    total_to_process = len(all_problems)
    print(f"ğŸ“Š Total problems to process: {total_to_process}. Already processed: {processed_count}.")

    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´

    with open(output_file, 'a', encoding='utf-8') as f:
        temp_feature_extractor = LearnedAttentionRouter("dummy_path.pth", device)
        validator = AccuracyValidator()

        for i, problem in enumerate(all_problems):
            if problem['question'] in processed_samples:
                continue

            current_progress = processed_count + 1

            try:
                slm_response = slm_interface.predict(problem['question'])
                slm_answer = validator.extract_final_answer(slm_response)
                gt_answer = evaluator.data_processor.extract_answer(problem['answer'])
                is_slm_correct = validator.is_correct(slm_answer, gt_answer)

                label = 1.0 if not is_slm_correct else 0.0

                # --- ã€ã€ã€æ–°å¢ã€‘ã€‘ã€‘æ›´æ–°è®¡æ•°å™¨ ---
                if label == 0.0:
                    simple_label_count += 1
                else:
                    complex_label_count += 1
                # --------------------------------

                features = temp_feature_extractor.extract_core_features(
                    problem['question'], slm_interface.model, slm_interface.tokenizer
                )

                sample_to_save = {"question": problem['question'], "features": features, "label": label}

                f.write(json.dumps(sample_to_save) + '\n')
                f.flush()
                processed_count += 1
                processed_samples.add(problem['question'])

                # --- ã€ã€ã€æ ¸å¿ƒä¿®æ”¹ï¼šå¢åŠ è¯¦ç»†è¿›åº¦æŠ¥å‘Šã€‘ã€‘ã€‘---
                # æ¯å¤„ç†20ä¸ªæ ·æœ¬ï¼Œæˆ–è€…åœ¨ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªæ—¶ï¼Œæ‰“å°ä¸€æ¬¡æ¸…æ™°çš„è¿›åº¦
                if current_progress % 20 == 0 or current_progress == 1 or current_progress == total_to_process:
                    elapsed_time = time.time() - start_time
                    samples_per_second = (
                                                     current_progress - processed_count + simple_label_count + complex_label_count) / elapsed_time if elapsed_time > 0 else 0
                    print(f"\n--- Progress Update ---")
                    print(f"   Processed: {current_progress}/{total_to_process}")
                    print(
                        f"   Label Counts: Simple (Correct) = {simple_label_count}, Complex (Incorrect) = {complex_label_count}")
                    print(f"   Speed: {samples_per_second:.2f} samples/sec")
                    print(f"-----------------------")
                # --- è¿›åº¦æŠ¥å‘Šç»“æŸ ---


            except Exception as e:
                print(f"\n   âš ï¸ Skipped problem #{i} ('{problem['question'][:30]}...') due to error: {e}")
                continue

    print(f"\nâœ… Training data generation complete! Total {processed_count} samples saved to {output_file}")
    print(f"   Final Label Distribution: Simple = {simple_label_count}, Complex = {complex_label_count}")
# åœ¨ train_router.py ä¸­
class RouterDataset(Dataset):
    def __init__(self, data_path):
        self.samples = []
        self.feature_keys = [
            'mid_avg_entropy', 'mid_entropy_std', 'mid_max_entropy', 'mid_avg_variance',
            'mid_variance_std', 'mid_max_variance', 'mid_avg_max_attention', 'mid_concentration_std',
            'last_avg_entropy', 'last_entropy_std', 'last_max_entropy', 'last_avg_variance',
            'last_variance_std', 'last_max_variance', 'last_avg_max_attention', 'last_concentration_std',
            'entropy_diff', 'variance_diff'
        ]
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                sample = json.loads(line)
                feature_vector = [sample['features'].get(key, 0.0) for key in self.feature_keys]
                self.samples.append({
                    "features": torch.tensor(feature_vector, dtype=torch.float32),
                    "label": torch.tensor([sample['label']], dtype=torch.float32)
                })

    def __len__(self): return len(self.samples)

    def __getitem__(self, idx): return self.samples[idx]


def train_router(training_data_path, model_save_path, epochs=20, lr=1e-4, batch_size=32):
    from common_utils import ComplexityPredictorNet  # Local import
    print("\n" + "=" * 50 + "\nğŸš€ Training the smart router...\n" + "=" * 50)
    dataset = RouterDataset(training_data_path)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    model = ComplexityPredictorNet().to(device)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        model.train()
        total_loss, correct_preds, total_samples = 0, 0, 0
        for batch in dataloader:
            features, labels = batch['features'].to(device), batch['label'].to(device)
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            preds = torch.sigmoid(outputs) > 0.5
            correct_preds += (preds == labels.bool()).sum().item()
            total_samples += labels.size(0)
        avg_loss = total_loss / len(dataloader)
        accuracy = correct_preds / total_samples
        print(f"Epoch {epoch + 1:02d}/{epochs} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2%}")

    torch.save(model.state_dict(), model_save_path)
    print(f"\nâœ… Training complete! Model saved to {model_save_path}")


# train_router.py (æœ€ç»ˆä¿®æ­£ç‰ˆ)

# ==========================================================
# ===== ä¿®æ”¹ç‚¹ 1: RouterDataset ç°åœ¨æ¥æ”¶ feature_subset =====
# ==========================================================
class RouterDataset(Dataset):
    def __init__(self, data_path, feature_subset: list):
        self.samples = []
        self.feature_keys = feature_subset  # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ç‰¹å¾åˆ—è¡¨

        print(f"--- Dataset Initialized using {len(self.feature_keys)} features: {self.feature_keys} ---")

        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    sample = json.loads(line)
                    # ä¸¥æ ¼æŒ‰ç…§ä¼ å…¥çš„feature_keysé¡ºåºå’Œæ•°é‡æ„å»ºç‰¹å¾å‘é‡
                    feature_vector = [sample['features'].get(key, 0.0) for key in self.feature_keys]
                    self.samples.append({
                        "features": torch.tensor(feature_vector, dtype=torch.float32),
                        "label": torch.tensor([sample['label']], dtype=torch.float32)
                    })
                except (json.JSONDecodeError, KeyError) as e:
                    print(f"Warning: Skipping a malformed line in dataset. Error: {e}")
                    continue

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]


# ==============================================================
# ===== ä¿®æ”¹ç‚¹ 2: train_router ç°åœ¨ä¹Ÿæ¥æ”¶ feature_subset =====
# ==============================================================
def train_router(training_data_path, model_save_path, feature_subset: list, epochs=20, lr=1e-4, batch_size=32):
    print("\n" + "=" * 50 + f"\nğŸš€ Training the smart router with {len(feature_subset)} features...\n" + "=" * 50)

    # 1. ä½¿ç”¨ä¼ å…¥çš„ç‰¹å¾å­é›†æ¥åˆ›å»ºæ•°æ®é›†
    dataset = RouterDataset(training_data_path, feature_subset=feature_subset)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # 2. ã€å…³é”®ã€‘æ ¹æ®ç‰¹å¾å­é›†çš„æ•°é‡ï¼ŒåŠ¨æ€åˆ›å»ºæ¨¡å‹
    num_features = len(feature_subset)
    model = ComplexityPredictorNet(input_features=num_features).to(device)

    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # 3. è®­ç»ƒå¾ªç¯ (ä¿æŒä¸å˜)
    for epoch in range(epochs):
        model.train()
        total_loss, correct_preds, total_samples = 0, 0, 0
        for batch in dataloader:
            features, labels = batch['features'].to(device), batch['label'].to(device)
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            preds = torch.sigmoid(outputs) > 0.5
            correct_preds += (preds == labels.bool()).sum().item()
            total_samples += labels.size(0)

        avg_loss = total_loss / len(dataloader)
        accuracy = correct_preds / total_samples
        print(f"Epoch {epoch + 1:02d}/{epochs} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2%}")

    torch.save(model.state_dict(), model_save_path)
    print(f"\nâœ… Training complete! Model saved to {model_save_path}")


if __name__ == "__main__":
    # ä»ç¯å¢ƒå˜é‡ä¸­è¯»å– Colab ä¼ é€’è¿‡æ¥çš„ä¿¡æ¯
    PROJECT_PATH = os.getenv('PROJECT_PATH_GDRIVE', '.')
    hf_token = os.getenv('HUGGINGFACE_TOKEN')

    training_file = os.path.join(PROJECT_PATH, "router_training_data_rich_features.jsonl")

    # é¦–å…ˆï¼Œç¡®ä¿å¯Œç‰¹å¾æ•°æ®é›†å­˜åœ¨
    if not os.path.exists(training_file):
        print(f"âŒ é”™è¯¯: å¯Œç‰¹å¾æ•°æ®æ–‡ä»¶ '{training_file}' æœªæ‰¾åˆ°!")
        print("   è¯·å…ˆè¿è¡Œå®Œæ•´çš„æ•°æ®ç”Ÿæˆæµç¨‹æ¥åˆ›å»ºè¿™ä¸ªæ–‡ä»¶ã€‚")
        # æ‚¨å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ è°ƒç”¨ generate_router_training_data çš„é€»è¾‘
    else:
        # æ‚¨çš„ç‰¹å¾é‡è¦æ€§æ’å (ä»é«˜åˆ°ä½)
        all_18_features_ranked = [
            'last_avg_max_attention', 'last_max_entropy', 'last_concentration_std', 'variance_diff',
            'mid_max_entropy', 'last_entropy_std', 'mid_entropy_std', 'mid_variance_std',
            'mid_concentration_std', 'last_avg_variance', 'last_avg_entropy', 'mid_avg_max_attention',
            'mid_avg_entropy', 'mid_max_variance', 'last_max_variance', 'entropy_diff',
            'last_variance_std', 'mid_avg_variance'
        ]

        # --- ã€ã€ã€å®éªŒé€‰æ‹©åŒºã€‘ã€‘ã€‘---
        # ä¸€æ¬¡åªå–æ¶ˆæ³¨é‡Šä¸€ä¸ªå®éªŒï¼Œè¿è¡Œå®Œæˆåï¼Œå†æ³¨é‡Šæ‰ï¼Œæ¢ä¸‹ä¸€ä¸ªã€‚

        # --- å®éªŒ1ï¼šåªä½¿ç”¨æœ€é‡è¦çš„å‰5ä¸ªç‰¹å¾ ---
        print("\n--- æ­£åœ¨è¿è¡Œå®éªŒ1ï¼šTop 5 ç‰¹å¾ ---")
        selected_features = all_18_features_ranked[:5]
        model_save_path = os.path.join(PROJECT_PATH, "router_model_top5.pth")
        # -----------------------------------

        # # --- å®éªŒ2ï¼šåªä½¿ç”¨æœ€é‡è¦çš„å‰10ä¸ªç‰¹å¾ ---
        # print("\n--- æ­£åœ¨è¿è¡Œå®éªŒ2ï¼šTop 10 ç‰¹å¾ ---")
        # selected_features = all_18_features_ranked[:10]
        # model_save_path = os.path.join(PROJECT_PATH, "router_model_top10.pth")
        # # ------------------------------------

        # # --- å®éªŒ3ï¼šä½¿ç”¨å…¨éƒ¨18ä¸ªç‰¹å¾ (ä½œä¸ºå¯¹æ¯”åŸºå‡†) ---
        # print("\n--- æ­£åœ¨è¿è¡Œå®éªŒ3ï¼šå…¨éƒ¨ 18 ä¸ªç‰¹å¾ ---")
        # selected_features = all_18_features_ranked
        # model_save_path = os.path.join(PROJECT_PATH, "router_model_all.pth")
        # # ------------------------------------

        # --- æ‰§è¡Œé€‰å®šçš„å®éªŒ ---
        train_router(training_data_path=training_file,
                     model_save_path=model_save_path,
                     feature_subset=selected_features)  # å°†é€‰å®šçš„ç‰¹å¾åˆ—è¡¨ä¼ è¿›å»

    print("\nâœ… è®­ç»ƒæµç¨‹ç»“æŸï¼")